The information categorization method was proposed by Peng et al. [25] , it is the modification based on the method proposed by Yang et al. [20, 21] . Based on this method, we consider the stock time series and choose the appropriate way to analyze the stock markets. Now we will briefly review the modified method. Consider a financial market time series, {x 1 , x 2 , . . . , x N }, where x i is the closing price in day i. We can classify each pair of successive closing prices into one of the two states that represents a decrease in x, or an increase in x. These two states are mapped to the symbols 0 and 1, respectively To define the measurement of similarity between two symbolic sequences, we carry out the following procedures. First, we map m + 1 successive interval to a binary sequence of length m, called an m-bit word. Each m-bit word, w k , therefore, represents a unique pattern of fluctuations in a given time series. By shifting one data point at a time, the algorithm produces a collection of m-bit words over the whole time series. Therefore, it is plausible that the occurrence of these m-bit words reflects the underlying dynamics of the original time series. Different types of dynamics thus produce different distributions of these m-bit words. Then we count the occurrences of different words, and then sort them in descending order by frequency of occurrence. The most frequently occurring word is ranked number 1, and so on. The resulting rank-frequency distribution, therefore, represents the statistical hierarchy of symbolic words of the original time series. For example, the first rank word corresponds to one type of fluctuation which is the most frequent pattern in the time series. In contrast, the last rank word defines the most unlikely pattern in the time series. Note that for any m-bit word, its rank order can be different in these two sequences. Therefore, we can plot the rank number of each m-bit word in the first time series against that of the second time series (see Fig. 1 ). If two time series are similar in their rank order of the words, the scattered points will be located near the diagonal line. Therefore, the average deviation of these scattered points away from the diagonal line is a measure of the distance between these two time series. Greater distance indicates less similarity and vice versa. In addition, we incorporate the likelihood of each word in the following definition of a weighted distance, D m , between two symbolic sequences, S 1 and S 2 . Here p 1 (w k ) and R 1 (w k ) represent probability and rank of a specific word, w k , in time series S 1 . Similarly, p 2 (w k ) and R 2 (w k ) stand for probability and rank of the same m-bit word in time series S 2 . The absolute difference of ranks is multiplied by the normalized probabilities as a weighted sum by using Shannon entropy [4] as the weighting factor. Finally, the sum is divided by the value 2 m âˆ’ 1 to keep the value in the same range of [0, 1]. The normalization factor Z is given by 