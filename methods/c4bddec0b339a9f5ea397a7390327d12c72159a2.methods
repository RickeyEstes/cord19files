AM is used to make decisions in the face of uncertainty that would otherwise impede consensus. AM involves a sequence of steps (Table 1) , including the statement of an objective (usually encapsulated in a reward [or cost] function), of possible management options, and of any uncertainties that hinder effective decision making (usually formulated as alternative state dynamic models). All possible model and action combinations are then evaluated in terms of their ability to achieve the stated objective. If all models agree about the best management action, despite disagreeing about the underlying uncertainty, then no further analysis is needed, and the decision can be made. However, if there is disagreement among models about the best action to take, it is possible to quantify how much learning about the ''correct'' model can be expected to improve outcomes. If the value of learning is sufficiently high, then an initial action can be chosen (on the basis of the highest expected benefit [or lowest expected cost] in light of model uncertainty), but AM plans for this action to be changed should information gained during early interventions reduce our uncertainty about the best model. The value of AM in selecting an intervention can be evaluated using the expected value of perfect information (EVPI), which estimates the value to the decision maker of resolving one or more uncertainties prior to the implementation of specific decisions. EVPI was originally developed in economics [30] , and has since been applied in ecological contexts [30, 37, 38] and in the development and evaluation of clinical trials [39] [40] [41] to identify key sources of uncertainty that limit management success and direct the allocation of research effort to most efficiently improve management outcomes. EVPI reflects a theoretical maximum achievable benefit [42] . Though managers often passively update interventions as new information comes to light, the potential to recover the EVPI is necessarily limited by the lack of a framework for real-time learning. This explicit structured decision-making framework is integral to AM, in which learning is valued insofar as it helps to maximize the proportion of the EVPI attained through informed interventions. The EVPI calculates the objective value gained by learning before making a decision. It involves a comparison of costs (and/or benefits) assuming perfect information with costs (and/or benefits) assuming the current level of information. Understanding the value of perfect information can meaningfully quantify the value of undertaking an AM program. Formally, EVPI is the difference between the average of optimum values conditional on each model and the optimum of an average of values, where the expectation is taken over the weights associated with the alternative models: Here, C ik is the cost associated with action i under model k, p k is the weight associated with model k (subject to the constraint that P k p k~1 ), and opt i indicates the optimum (in our case, the minimum) over all candidate actions (also see Table 2 ). 