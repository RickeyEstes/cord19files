We conducted a systematic review of the published literature on the conduct and reporting of meta-analyses in observational studies. Databases searched included MEDLINE, Educational Resources Information Center, PsycLIT (http://www.wesleyan.edu /libr), and the Current Index to Statistics. In addition, we examined reference lists and contacted experts in the field. We used the 32 articles retrieved to generate the conference agenda and set topics of bias, searching and abstracting, heterogeneity, study categorization, and statistical methods. We invited experts in meta-analysis from the fields of clinical practice, trials, statistics, epidemiology, social sciences, and biomedical editing. The workshop included an overview of the quality of reporting of metaanalyses in education and the social sciences. Plenary talks were given on the topics set by the conference agenda. For each of 2 sessions, workshop participants were assigned to 1 of 5 small discussion groups, organized around the topic areas. For each group, 1 of the authors served as facilitator, and a recorder summarized points of discussion for issues to be presented to all participants. Time was provided for the 2 recorders and 2 facilitators for each topic to meet and prepare plenary presentations given to the entire group. We proposed a checklist for metaanalyses of observational studies based on the deliberation of the independent groups. Finally, we circulated the checklist for comment to all conference attendees and representatives of several constituencies who would use the checklist. Items in this checklist section are concerned with the appropriateness of any quantitative summary of the data; degree to which coding of data from the articles was specified and objective; assessment of confounding, study quality, and heterogeneity; use of statistical methods; and display of results. Empirical evidence shows that reporting of procedures for classification and coding and quality assessment is often incomplete: fewer than half of the meta-analyses reported details of classifying and coding the primary study data, and only 22% assessed quality of the primary studies. 10 We recognize that the use of quality scoring in meta-analyses of observa-tional studies is controversial, as it is for RCTs, 16, 33 because scores constructed in an ad hoc fashion may lack demonstrated validity, and results may not be associated with quality. 34 Nevertheless, some particular aspects of study quality have been shown to be associated with effect: eg, adequate concealment of allocation in randomized trials. 35 Thus, key components of design, rather than aggregate scores themselves, may be important. For example, in a study of blinding (masking) of readers participating in meta-analyses, masking essentially made no difference in the summary odds ratios across the 5 meta-analyses. 36 We recommend the reporting of quality scoring if it has been done and also recommend subgroup or sensitivity analysis rather than using quality scores as weights in the analysis. 37, 38 While some control over heterogeneity of design may be accomplished through the use of exclusion rules, we recommend using broad inclusion criteria for studies, and then performing analyses relating design features to outcome. 8 In cases when heterogeneity of outcomes is particularly problematic, a single summary measure may well be inappropriate. 39 Analyses that stratify by study feature or regression analysis with design features as predictors can be useful in assessing whether study outcomes indeed vary systematically with these features. 40 Investigating heterogeneity was a key feature of a meta-analysis of observational studies of asbestos exposure and risk of gastrointestinal cancer. 41 The authors of the meta-analysis hypothesized that studies allowing for a latent period between the initiation of exposure and any increases in risk should show, on average, appropriately higher standardized mortality ratios than studies that ignored latency. In other words, the apparent effect of exposure would be attenuated by including the latent period in the calculation of time at risk (the "denominator"), since exposurerelated deaths (the "numerator") would, by definition, not occur during that latent period (FIGURE) . In fact, the data suggested that studies allowing for latent periods found on average somewhat higher standardized mortality ratios than studies ignoring latency. This example shows that sources of bias and heterogeneity can be hypothesized prior to analysis and subsequently confirmed by the analysis. 