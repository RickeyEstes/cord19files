Our methodology can be summarized as follows. Using alerts retrieved by HealthMap we generated a dataset specially tailored to train a geo-parsing algorithm. To generate this dataset, the HealthMap gazetteer-based algorithm was first applied to this set of alerts in order to extract the words in the text referring to geographic locations. The same alerts were then run through the part-of-speech tagger algorithm provided by NEC's project SENNA (a Neural Network Architecture for Semantic Extraction) [22] , making the syntax of the text explicit. This part-of-speech tagger has a reported accuracy of 96.85% on the reference Penn Treebank dataset [23] . We then assigned to every word in the alerts a capitalization status, ie none, first character, upper case. After these 3 steps in the data generation process, each word in each alert had 4 features: the word itself, its part-of-speech tag, its capitalization status and a label indicating if the word is a geographic location or not. The last step in the data generation process consisted in replacing the lexical feature of the words with lowest frequency by a blank, as explained in the Results section. Using the data just described, we trained an artificial neural network to output a probability estimate of the label value y i for the i th word x i in an alert x, given a window (n -1 = 2 × hw) of preceding and following words. The neural network was trained by negative log-likelihood minimization using stochastic gradient descent. The neural network with an example of input (with hw = 2) is illustrated in Figure 7 . The network architecture can be decomposed as follows. First, each word in the window sequence is given as input to a unique multilayer perceptron (MLP) which has been replicated n = 2 × hw + 1 times, in a siamese network fashion [24] . This first MLP can be seen as a function ϕ mapping the extremely sparse representation x i of the words into a new representation, ϕ(x i ) ∈ R d , which has the advantage of being learned during the training. This approach was applied with success for language modeling in [25] and more recently for semantic role parsing in [22] . The outputs ϕ(x i-hw ), ..., ϕ(x i ), ..., ϕ(x i+hw ) are concatenated into a vector z ∈ R d × n which is itself given as input to a sec- ond multi-layer perceptron. This second MLP, called ψ in Figure 7 , has as output layer a softmax filtering function which allows us to consider the outputs of the neural network as probabilities. A threshold on P(y i = loc|x i-hw , ..., x i , ..., x i+hw ) allows us to decide if the input is a location or not. This threshold and the hyper-parameters of the neural network are tuned on a separate validation set. Tuning this threshold away from 0.5 compensates for the fact that some none labels are in fact locations unknown to the HealthMap gazetteer. 