In order to study the association between the variable "presence of rotavirus" and the variable "consistency of faeces", a 2 Ã— 3 contingency table was done and the chi-square test was performed. Diagnostic test results were analyzed in two ways: first, with PAGE as a gold standard and secondly, on a comparative basis with each other. In the first case, results of the commercial tests were compared with those of PAGE by using sensitivity, specificity, positive predictive value, negative predictive value and diagnostic accuracy. In the abscence of a standard, the Kappa statistic was used for comparison. The comparative measures are defined as follows: Sensitivity (true-positive rate ): is the proportion of samples in which the test is positive when PAGE is positive. Specificity (true-negative rate ): is the proportion of samples in which the test is negative when PAGE is negative. Positive predictive value: is the proportion of samples in which PAGE is positive when the test is positive. Negative predictive value: is the proportion of samples in which PAGE is negative when the test is negative. Diagnostic accuracy: was determined by dividing the number of positive and negative specimens in both the commercial kit and PAGE by the total number of specimens tested. Kappa statistic is an overall measure of agreement between two tests and is useful for measuring agreement in the absence of a standard. It compares the observed proportion of samples in which the tests agree with the proportion that would be expected to agree by chance. Kappa calculation was carried out as described by Goodman and Kruskal ( 1972 ) . For interpreting Kappa values the following benchmarks can be considered: a result between 0.0 and 0.20 indicates a slight agreement, between 0.21 and 0.40 is a fair agreement, between 0.41 and 0.60 is a moderate agreement, between 0.61 and 0.80 is a substantial agreement and between 0.81 and 1.00 is an almost perfect agreement. 


Section:statistical methodology