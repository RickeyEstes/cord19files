SVMProt is based on a statistical learning method support vector machines (SVM) (Burges, 1998) . In addition to the prediction of protein functional class (Cai et al., 2003 Han et al., 2004; Karchin et al., 2002) , SVM has also been used for a variety of protein classification problems including fold recognition (Ding and Dubchak, 2001) , analysis of solvent accessibility (Yuan et al., 2002) , prediction of secondary structures (Hua and Sun, 2001) , and protein-protein interactions (Bock and Gough, 2001) . As a method that uses sequence-derived physicochemical properties of proteins as the basis for classification, SVM may be particularly useful for functional classification of distantly related proteins and homologous proteins of different functions (Cai et al., 2003 . There are 75 protein functional classes currently covered by SVMProt. These include 46 enzyme families, 13 channel/transporter families, 4 RNA-binding protein families, DNA-binding proteins, G-protein-coupled receptors, nuclear receptors, Tyrosine receptor kinases, cell adhesion proteins, coat proteins, envelope proteins, outer membrane Human herpesvirus 6 chemokine like (Luttichau et al., 2003) No function predicted NM (continued on next page) proteins, structural proteins, and growth factors. Two broadly defined families of antigens and transmembrane proteins are also included. The majority of known types of viral proteins are included in these classes. Representative proteins of a particular functional class (positive samples) and those do not belong to this class (negative samples) are needed to train a SVMProt classifier for this class. The positive samples of a class are constructed by using all of the known distinct protein members in that class. Because of the enormous number of proteins, the size of negative samples needs to be restricted to a manageable level by using a minimum set of representative proteins. One way for choosing representative proteins is to select one or a few proteins from each protein domain family. The negative samples of a class are selected from seed proteins of the 7316 curated protein families (domain-based) in the Pfam database excluding those families that have at least one member belong to the functional class. Pfam families are constructed on the basis of sequence similarity. The purpose of using Pfam proteins is to ensure that the negative samples are evenly distributed in the protein space. Sequence similarity is not required for selecting positive samples. In this sense, SVMProt is to some extent independent of sequence similarity. The SVMProt training system for each family is optimized and tested by using separate testing sets of both positive and negative samples. While possible, all the remaining distinct proteins in each functional family (not in the training set of that family) are used as positive samples and all the remaining representative seed proteins in Pfam curated families are used to construct negative samples in a testing set. The performance of SVMProt classification is further evaluated by using independent sets of both positive and negative samples. There is no duplicate protein in each training, testing, or independent evaluation set. Data set construction can be demonstrated by an illustrative example of viral coat proteins. The key word bvirus coat proteinQ is used to search the Swissprot, which finds 3012 entries. These entries are checked to remove noncoat proteins, redundant entries, and putative proteins, which gives 848 positive samples. These positive samples cover 140 Pfam families; thus, 14 758 seed proteins of the remaining 7176 Pfam families are used as the negative samples. These positive and negative samples are further divided into 346 and 1474 training, 305 and 8370 testing, and 197 and 4914 independent evaluation sets using the procedure described above. Not all of the SVMProt classes are at the same hierarchical level. These classes are mixtures of subfamilies, families, and superfamilies. Some classes, such as antigen, need to be more clearly defined into specific subclasses. While it is desirable to define all of the classes at the same level, this is not yet possible because of insufficient data for the subhierarchies of some families and superfamilies. Effort is being made to collect sufficient data so that SVMProt classification systems can be constructed on the basis of a more evenly distributed family structures. Transferase (Wilfred et al., 2002) No function predicted NM SPLT13 (NP _ 258405) SpLtMNPV virus A noval envelope protein (Yin et al., 2003) No function predicted NM TRL10 (AAL27474) Human cytomegalovirus (HCMV) Structural envelop glycoprotein (Spaderna et al., 2002) Transmembrane ( Nonetheless, prediction on the basis of the current structures provides useful hint about the function of a protein. SVMProt is trained for protein classification in the following manner. First, every protein sequence is represented by specific feature vector assembled from encoded representations of tabulated residue properties including amino acid composition, hydrophobicity, normalized Van der Waals volume, polarity, polarizability, charge, surface tension, secondary structure, and solvent accessibility for each residue in the sequence (Cai et al., 2003) . The feature vectors of the positive and negative samples are used to train a SVMProt classifier. The trained SVMProt classifier can then be used to classify a protein into either the positive group (protein is predicted to be a member of the class) or the negative group (protein is predicted to not belong to the class). The theory of SVM has been described in the literature (Burges, 1998) . Thus, only a brief description is given here. SVM is based on the structural risk minimization (SRM) principle from statistical learning theory (Burges, 1998) . In linearly separable cases, SVM constructs a hyperplane that separates two different groups of feature vectors with a maximum margin. A feature vector is represented by x i , with physicochemical descriptors of a protein as its components. The hyperplane is constructed by finding another vector w and a parameter b that minimizes twt 2 and satisfies the following conditions: where y i is the group index, w is a vector normal to the hyperplane, |b| / twt is the perpendicular distance from the hyperplane to the origin and twt 2 is the Euclidean norm of w. After the determination of w and b, a given vector x can be classified by: In nonlinearly separable cases, SVM maps the input variable into a high dimensional feature space using a kernel function K(x i , x j ). An example of a kernel function is the Gaussian kernel that has been extensively used in different protein classification studies (Bock and Gough, 2001; Burges, 1998; Cai et al., 2002; Ding and Dubchak, 2001; Hua and Sun, 2001; Karchin et al., 2002; Yuan et al., 2002) : Linear support vector machine is applied to this feature space and then the decision function is given by: where the coefficients a i 0 and b are determined by maximizing the following Langrangian expression: under conditions: a i z 0 and X l iÀ1 a i y i ¼ 0 A positive or negative value from Eq. (3) or Eq. (5) indicates that the vector x belongs to the positive or negative group, respectively. To further reduce the complexity of parameter selection, hard margin SVM with threshold instead of soft margin SVM with threshold is used in SVMProt. Scoring of SVM classification of proteins has been estimated by a reliability index and its usefulness has been demonstrated by statistical analysis (Cai et al., 2003; Hua and Sun, 2001) . A slightly modified reliability score, R value, is used in SVMProt: where d is the distance between the position of the vector of a classified protein and the optimal separating hyperplane in the hyperspace, d N 0 indicates the sample belongs to the positive group and d b 0 the negative group. There is a statistical correlation between R value and expected classification accuracy (probability of correct classification) (Cai et al., 2003; Hua and Sun, 2001) . Thus, another quantity, P value, is introduced to indicate the expected classification accuracy. P value is derived from the statistical relationship between the R value and actual classification accuracy based on the analysis of 9932 positive and 45,999 negative samples of proteins (Cai et al., 2003) . 


Section:computational method