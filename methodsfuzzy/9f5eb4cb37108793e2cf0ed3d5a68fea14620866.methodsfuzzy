Although there has been considerable interest in applying machine learning methods to predict the effects of non-synonymous mutations, the majority of the work focused on deleterious mutations or disease associated mutations [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27] . Previous efforts outlined structure-and sequencebased criteria for designing TS mutations of globular proteins [14, 15] . They suggest first the identification of buried sites or ligand binding sites and then random mutation of one site. As these ad hoc criteria have no underlying statistical framework, it is infeasible to compare them with our TS prediction models. A recently published study presented a method to predict TS mutants but the evaluation is based on cross-validation only [28] . The method used support vector machine (SVM) with a smaller training set than ours (75 TS and 130 non-TS). It developed a similar number of structure and sequence features as ours (108 features), most of which are Rosetta relax-derived features. Further, it only considered mutations on buried residues while our method ranks mutations on all residues. To objectively compare the performance of our method with the existing one, we tested them on the independent mutation data of HIV-1 protease. However, only six out of 124 mutations were predicted to have confidence score using the existing method; the rest are either not on buried residues or have no confidence score. Performance comparison based on these six mutations (3 TS and 3 neutral) is shown in Figure 8 . The result indicates that our ''all features'' model either outperforms the existing method or achieves similar performance as the existing one. Further, based on cross-validation on identical training data, we can compare the performance of our model with other machine learning approaches. We built a SVM classifier to predict TS mutations. SVM classifiers identify a hyperplane that can best separate TS and neutral mutations in a high-dimensional space [56] . Same training data and same feature set were used for SVM as those for the ''all features'' model. The SVM classifier was trained and built with Matlab interface for the Libsvm package [57] . Different kernel functions were tested, including linear, polynomial, and radial basis function. The best combination of parameters was selected by a grid-search. Based on a ten-fold cross-validation, we obtained the best-performed SVM classifier with polynomial kernel using degree of 3, SVM parameter c of 0.004, and penalty parameter C of 0.00011. The AUC of this SVM classifier was 0.91, which is same as that of the ''all features'' model; both classifiers were evaluated via the identical cross validation procedure. This result demonstrates that our logistic regression models have similar performance as the SVM classifier. 


Section:comparison with other methods