In small data sets it is essential to save degrees of freedom (DOF). In this perspective, the adopted model -of the type semiparametric -consists of two parts: a purely nonparametric and a parametric one. While the former does not pose problems in terms of DOF, the latter clearly does. However, the sacrifice in terms of DOF is very limited as an autoregressive model of order 1 (employed in a suitable distance function, as below illustrated) has proved sufficient for the purpose. DOF-saving strategy is also the driving force of the choice not to consider as an exogenous parameter the georeferencing of Regions or to include the regional population in a regression-like scheme but to implicitly assumed these variable embedded in the dynamic of the time series in question. The bootstrap scheme adopted proved to be a real asset for the problem at hand. Given the pivotal role played it will be briefly presented. In essence, the choice of the most appropriate resampling method is far from being an easy task, especially when the identical and independent distribution iid assumption (Efron's initial bootstrap method) is violated. Under dependence structures embedded in the data, simple sampling with replacement has been proved -see, for example Carlstein et al. (1986) -to yield suboptimal results. As a matter of fact, iid-based bootstrap schmes are not designed to capture, and therefore replicate, dependence structures. This is especially true under the actual conditions (small sample sizes). In such cases, selecting the "right" resampling scheme becomes a particularly challenging task. Several ad hoc methods have been therefore Page 3 of 14 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity. is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10. 1101 /2020 proposed, many of which now freely and publicly available in the form of powerful routines working under software package such as Python R or R R . In more details, while in the classic bootstrap an ensemble Ω represents the population of reference the observed time series is drawn from, in MEB a large number of ensembles (subsets), say {ω 1 , . . . , ω N } becomes the elements belonging to Ω, each of them containing a large number of replicates {x 1 , . . . , x J }. Perhaps, the most important characteristic of the MEB algorithm is that its design guarantees the inference process to satisfy the ergodic theorem. Formally, denoting by the symbol | · | the cardinality function (counting function) of a given ensemble of time series {x t ∈ ω i ; i = 1, . . . , N }, the MEB procedure generates a set of disjoint subsets the sample mean. Furthermore, basic shape and probabilistic structure (dependency) is guaranteed to be retained ∀x * t,j ⊂ ω i ⊂ Ω. MEB resampling scheme has not negligible advantages over many of the available bootstrap methods: it does not require complicated tune up procedures (unavoidable, for example, in the case of resampling methods of the type Block Bootstrap) and it is effective under non-stationarity. MEB method relies on the entropy theory and the related concept of (un)informativeness of a system. In particular, the Maximum Entropy of a given density δ(x), is chosen so that the expectation of the Shannon Information Under mass and mean preserving constraints, this resampling scheme generates an ensemble of time series from a density function satisfying (4). Technically, MEB algorithm can be broken down, following Koutris et al. (2008) , in 8 steps. They are: 1. a sorting matrix of dimension T × 2, say S 1 , accommodates in its first column the time series of interest x t and an Index Set -i.e. I ind = {2, 3, . . . , T } -in the other one; 2. S 1 is sorted according to the numbers placed in the first column. As a result, the order statistics x (t) and the vector I ord of sorted I ind are generated and respectively placed in the first and second column; 3. compute "intermediate points", averaging over successive order statistics, i.e. c t = x (t) +x (t+1) 2 , t = 1, . . . T − 1 and define intervals I t constructed on c t and r t , using ad hoc weights obtained by solving the following set of equations: Page 4 of 14 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity. is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.14.20036103 doi: medRxiv preprint 4. from a uniform distribution in [0, 1], generate T pseudorandom numbers and define the interval R t = (t/T ; t + 1/T ] for t = 0, 1, . . . , T − 1, in which each p j falls; 5. create a matching between R t and I t according to the following equations: so that a set of T values {x j,t }, as the j th resample is obtained. Here θ is the mean of the standard exponential distribution; 6. a new T × 2 sorting matrix S 2 is defined and the T members of the set {x j,t } for the j th resample obtained in Step 5 is reordered in an increasing order of magnitude and placed in column 1. The sorted I ord values ( Step 2) are placed in column 2 of S 2 ; 7. matrix S 2 is sorted according to the second column so that the order {1, 2, . . . , T } is there restored. The jointly sorted elements of column 1 is denoted by {x S,j,t }, where S recalls the sorting step; 8. Repeat Steps 1 to 7 a large number of times. 


Section:the proposed method
Section:the resampling method