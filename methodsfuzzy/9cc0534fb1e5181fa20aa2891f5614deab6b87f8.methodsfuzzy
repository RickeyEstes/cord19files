Mathematical modeling and simulation allows for rapid assessment. Simulation is also used when the cost of collecting data is prohibitively expensive, or there are a large number of experimental conditions to test. Over the years, a vast number of approaches have been proposed looking at the problem from different perspectives. These encompass three general categories (see Fig. 1 ): (1) statistical methods for surveillance of outbreaks and identification of spatial patterns in real epidemics, (2) mathematical models within the context of dynamical systems (also called state-space models) used to forecast the evolution of a "hypothetical" or on-going epidemic spread, and (3) machine learning/ expert methods for the forecasting of the evolution of an ongoing epidemic. For all three of these categories there are again different approaches weaving a big and diverse literature. Here, we try to draw the map of these approaches and try to describe their basic underpinning concepts. One of the most important aspects in epidemics revolves around the surveillance, early detection of possible outbreaks and patterns that may help controlling a spread. One of the very first success stories in the area is the modeling of cholera epidemic that swept through London in 1854. At that time John Snow, a physician, collected spatiotemporal data and by visualizing them in a map found that there was a particular pattern around the Broad Street water pump, 21 which actually was the zero point of transmission. His analysis helped eradicate the disease. In the dawn of 20th century Greenwood an epidemiologist and statistician was the first Professor of Epidemiology and Statistics at the London School of Hygiene and Tropical Diseases establishing a rigorous mathematical connection between fields. 22 Today, global initiatives to combat epidemics require effective domestic action mechanisms and preparedness through the globe. An intensive worldwide effort led by World Health Organization and Centers for Disease Control is speeding up the developments for the establishment of a global surveillance network. New emerged pandemics such as the AIDS, the severe acute respiratory syndrome (SARS) of 2002-2003 and the H1N1 swine flu of 2009 pandemics reminds us about the importance of surveillance and prompt outbreak detection. Toward this aim, statistical methods have enhanced our potential in fighting epidemics allowing for rapid assessment of emerging situations. Obviously, the correctness of the data and the selection of the appropriate methodology are crucial for the construction of statistical models that can capture in an efficient robust way the communicable disease characteristics. To date, several statistical methods have been proposed (see also Unkel et al. [2012] 23 for a review of statistical methods for the detection of disease outbreaks). In the website of Centers for Disease Control and Prevention (CDC) (http://www.cdc.gov/) one can find a list of references in the field. Here we present Daniel Bernoulli developed a mathematical model to analyze the mortality due to smallpox in England, which at that time was one in 14 of the total mortality. Bernoulli used his model to show that inoculation against the virus would increase the life expectancy at birth by about three years. A translation in English and review of this work can be found in Sally Blower (2004), 7 while a revision of the main findings and a presentation of the criticism by D'Alembert appears in Dietz and Heesterbeek (2002) . 8 Lambert, in 1772, followed up the work of Bernoulli extending the model by incorporating age-dependent parameters. 9 Laplace has also worked on the same concept. 10 However this line of research has not been developed systematically until the benchmark paper of Ross in 1911, which actually establishes modern mathematical epidemiology. 11 In this work, Ross addressed the mechanistic a priori modeling approach using a set of equations to approximate the discrete-time dynamics of malaria through the mosquitoborne pathogen transmission (for a discussion and a review of this model see also Smith et al. [2012] 12 ). Following up the work of Ross, Kermack and McKendrick published three seminal papers which founded the deterministic compartmental epidemic modeling. [13] [14] [15] In these papers, they addressed the mass-action incident in disease transmission cycle, suggesting that the probability of infection of a susceptible (virgin from illness) is analogous to the number of its contacts with infected individuals. Hence, the rate at which susceptibles become infected is given by kSI where S and I represent population densities of susceptible and infected people, respectively. In this context, the rate at which infected individuals become recovered is given by λI, while the rate at which recovered individuals become again susceptible is given by μR; k, λ and μ are analogy constants. This mechanistic-deterministic representation holds strong analogy to the Law of Mass Action 16 introduced by Guldberg and Waage in 1864 and is called the SIR model, implying homogeneous mixing of the contacts and conservation of the total mass (population) as well as relatively low rates of interaction. Forty years after the paper of Ross, MacDonald extended Ross's model to explain in depth the transmission process of malaria and propose methods for eradicating the disease on an operational level. Due to the importance of MacDonald's contribution to the field by exploiting the use of computers, mathematical models for the dynamics and the control of mosquito-transmitted pathogens are known as Ross-MacDonald models. 12 At this point it would be remiss of us not to mention the work of Enko, [17] [18] [19] assessment have been also proposed. 27 Today, the above approach is used by the Centers for Disease Control in the US, Australia, France, and Italy for the detection of influenza outbreaks. While this approach is very popular among epidemiologists for predicting and surveillance purposes, one has to be cautious about their use as the form of the equations relies usually on ad hoc assumptions on the dependence between the dynamics of a disease and the independent factors (variables) that determine its spread. In addition, the choice of the model (linear/nonlinear), assumptions on the statistical properties (for example independence, normal distribution and fixed variance) of the unmodeled dynamics (represented by e(t)) flash a "note of caution" in their use especially for the surveillance and prediction of outbreaks of new emerging epidemics. Times series analysis based on autoregressive models such as the autoregressive integrated moving average model (ARIMA) and seasonal ARIMA (SARIMA) 30-33 as well as neural networks. 34 These models relax the hypothesis of autocorrelation of regression models as well as the hypothesis of simple autoregressive models such as AR (autoregressive) and ARMA (autoregressive moving) in which past disturbances are not modeled. In this category, ARIMA models are the most commonly used. Their general form reads: where y(t) denotes a stationary stochastic process at time t with mean value E(y(t)) = μ; z -1 is the backward shift operator defined by z -k y(t) = y(t -k) and Δ d is the differencing operator of order d defined by Δ d ≡ (1 -z -1 ) d ; A(z -1 ) is the autoregressive operator defined as ; B(z -1 ) is the moving-average operator defined by ; and discuss the most common schemes that can be classified as follows: Regression methods. 24-29 Regression models try to detect an outbreak from time-series of epidemic-free periods by monitoring a statistic of reported infected cases, say y(t). An epidemic alert is raised when a certain threshold, say k, is surpassed, defined by , (μ being the mean value of the time-series distribution) within a confidence interval (usually of 95%). A basic regression model is that proposed from Serfling which was initially constructed to monitor the deaths of influenza based on the seasonal pattern of pneumonia and influenza deaths. 24 Due to the seasonal behavior of the disease the following cyclic regression model has been addressed: θ is a linear function of time t while the coefficients are to be determined by a parameter identification technique. The cosine and sine terms are used to approximate cyclical seasonal patterns; e(t) is the noise (assumed that is Gaussian distributed with mean zero and variance σ 2 ) which is estimated from the time-series. In the original paper of Serfling, y(t) was the expected mean value of total deaths due to pneumonia and influenza in units of 4-weeks periods. The model was fitted using data from 108 US cities for a 3 year period starting in September of 1955. Using least squares estimation Serfling ended up to the following model: Other models including square terms, t 2 , to account long-term changes due to factors such as the population growth or disease where μ 0 and μ 1 are the mean values of the in-control and outof-control Poisson distributions. For an epidemic that involves time-varying characteristics, such as seasonality, the reference parameter is now time-varying itself, i.e., k ≡ k(t) . The EWMA control chart method monitors infectious disease dynamics using the following recursive statistical estimator, which in its simple form reads: forgetting" factor, a number between 0 and 1 which weights the significance of past values. Actually this factor reduces the importance of past observed information in estimating future. Again, an alarm is raised at time Other statistical process control methods such as temporal scan statistics have been also used. 46, 50, 51 Hidden Markov models (HMM) used to explain statistical correlation in time series. 52, 53 The question that the HMMs come to answer in epidemiology is the following: how can we infer about the dynamics of a particular infectious disease and forecast its outbreak when we cannot monitor/record explicitly the characteristics of the disease but we can observe some possible indicators of the disease? For example, can we forecast the evolution of an influenza epidemic by monitoring for example the number of reported cases as recorded through a surveillance network of physicians or in hospital units? 52, 54 HMM models are exploited exactly under these limitations/ constraints. Within this context, let us denote by Y(t) the stochastic process of the unobserved (hidden) state, e.g., the number of cases of the disease in the population at time t and with O(t) the stochastic process of the observable states. Formally, HMMs are Markov processes, i.e., stochastic processes which satisfy the so called Markov property (here for the sake of presentation we assume discrete in time Markov processes) defined by: along with the time-invariant transition probability between two realizations, say y i (.), y j (.): The above relations simply state that all the necessary information for predicting the distribution of Y(t) at time Y(t) with a certain probability defined by P(.) is contained within Y(t -1); y(.) denotes a realization of the stochastic process Y(.). In HMMs, the following conditional independence assumption holds: e(t) is the residual (noise) at time t representing the part of the measurement that cannot be predicted from previous measurements. For d = 0 and n a = 0 one gets the moving average model, while for d = 1, n a = n b = 0 one gets the random walk with drift. Seasonal differencing enters naturally in the above framework by considering the seasonal differencing operator where k is the length of seasonal cycle and S is the degree of seasonal differencing producing series of changes from one season to the next. The time-series is then split in two sets: one containing the times-series serving as a training set, and another one containing the remaining data serving as a test (validation) set. The Akaike Information Criterion 35 is usually applied to identify the optimal model order by compromising between the goodness-of-fit and number of parameters. The fitted model is then used for the forecasting of disease evolution. The reliability of such approaches is limited mostly by (1) the statistical uncertainty related to the estimation of the values of the unknown parameters and (2) the hypotheses related to the statistical properties of the corresponding time series. Statistical process control methods including cumulative sum (CUSUM) charts 36-41 and exponentially weighted moving average (EWMA) 42,43 -based methods. CUSUM is probably the most common used technique for the detection of disease outbreaks. This is achieved by monitoring a cumulative performance measure over time. Let us consider the number of infected cases y(t i ) as observed at different time instances t i , i = 1, 2, …, n . In its simple representation, for a single parameter process, CUSUM is defined as or in a recursive form as CUSUM (0) = 0 , i ≥ 0 where k is a reference value corresponding to the difference between to the in-control and the out-of-control mean. The process is considered to be in-control if CUSUM(i) < h with h denoting a threshold (its value is usually taken to be three times the standard deviation from the baseline/mean value of in-controlobservations). An alarm is raised at time t i if CUSUM(i) exceeds h; the process is considered to be out-of-control. The reference value k is determined by likelihood ratio based methods. [44] [45] [46] [47] [48] [49] Hence, denoting by f(θ 0 ) and f(θ 1 ) the probability function of the in-control and out-of-control processes with parameters θ 0 and θ 1 respectively, the reference value reads: The probability functions f(θ 0 ) and f(θ 1 ) and their parameters can be estimated using data from past periods. For Poisson distributions the above relation reads: demographic variables (such as age, gender, social status, spatial characteristics) on the survival rates, i.e., occurrence rates of events such as death or infection in the population. 85, 86  


Section:mathematical modeling methodologies in epidemiology
Section:statistical-based methods for epidemic surveillance