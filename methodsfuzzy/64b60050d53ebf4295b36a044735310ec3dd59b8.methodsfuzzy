We will now describe some of the recent developments in the calculation of molecular descriptors.  The effect of binary representations of fingerprints has been known for some time, such as combinatorial preferences [54] and size effects [55] (depending on the similarity coefficient used). More recently, another aspect of the binary representation of features in a fingerprint has been analyzed [56] . Integer or real-valued representations of feature vectors were calculated for 12 activity classes and employed CATS2D and CATS3D autocorrelation descriptors as well as Ghose and Crippen fragment descriptors. Afterward, retrospective virtual screening calculations were performed for both the original (quantitative) representations and the binary (presence/absence) fingerprints. Surprisingly, in only 2 out of the 12 cases did significantly different numbers of actives get retrieved (defined as more than 20% difference). In addition, the retrieved actives showed, depending on the activity class, very different overlap, between 0% and 90%, indicating some orthogonality of the same descriptor, differing by its representation (integer/real-values vs. binary format). Exploiting the 'molecular similarity principle' by not only looking for neighbors of an active compound and assuming they are active (as is usually done in virtual screening) but also using this knowledge further to improve the model, has recently been exploited in a method called 'Turbo Similarity Searching' [57] . By feeding back information about the nearest neighbors of an active compound into the model generation step, an increased number of active compounds can be retrieved in a subsequent step. This is analogous to the re-use of hot air in turbo chargers in cars, where the output (hot gas, nearest neighbor in this case) is fed back into the loop to improve performance. A number of publications have appeared recently focusing on the validation of QSAR models. A wealth of parameters exist here, such as training/ test/validation set splits, the dimensionality of descriptors used in relation to the number of degrees of freedom of a model, or the way selection of features is performed. While it has been recognized for some time that a larger number of descriptors increases the likelihood of chance correlations [58] , more recently a discussion of the validity of statistical significance tests, such as the F test, has appeared [59] which puts the number of features considered into relation to the significance of a model. This study cautions in agreement with earlier work that one needs to be very careful when judging the statistical significance of correlation models if feature selection is applied -and that statistically 'significant' models can hardly be 'avoided' if too large a variable pool is chosen to select features in the first place. Since datasets are generally limited in size, a suitable split into training and test set(s) is crucial in order to achieve sufficient training examples on the one hand, and as high as possible a predictivity of the model on the other. Often, leave-one-out cross-validation has been used to judge model performance -where the compound 'left out' was supposed to be a novel compound found for which property predictions had to be made. Unfortunately this is, according to recent studies, not a suitable validation method [60, 61] . In the case of leave-one-out cross-validation, where features are selected from a wider range, the tendency exists in every case to select those features which perform best on a particular compound -thus decreasing generalizability of the model. Results were summarized in a simple statement: 'Beware of q 2 !', where specifically the cross-validated correlation coefficient of a leave-one-out cross-validation is alluded to. In addition, general guidelines for developing robust QSAR models were developed, namely a high cross-validated correlation coefficient and a regression, which shows slope close to 1 and no significant bias. Using theoretical considerations as well as empirical evaluations the question of leave-one-out vs. separate test sets was recently considered in detail [62] . Performing repeated cross-validations of both types on a large QSAR dataset, the conclusion was drawn that in the case of smaller datasets, separate test sets are wasteful, but in case of larger datasets (at least large three-digit numbers of data points) it is recommended. This partly contradicts the above conclusion, that separate test sets should always be used. The discrepancy was explained by the fact that in the earlier work only small separate test sets were used (containing 10 compounds), which was not able to provide a sufficiently reliable performance measure. The finding that cross-validation often overestimates model performance was corroborated in a recent related study [63] , in particular, in cases where strong model selection such as variable selection is applied. The main influence on quality overestimation was found to be a (small) dataset size; other factors are the size of the variable pool considered, the objectto-variable-ratio, the variable selection method, and the correlation structure of the underlying data matrix. While in case of conventional stepwise variable selection overconfidence is commonly encountered, as a remedy LASSO (least absolute shrinking and selection operator) selection is proposed, as well as the utilization of ensemble averaging. Both techniques give more reliable estimates of the quality of the developed model. Given that the latter was shown to improve performance in many cases on its own the generation of reliable performance measures is an additional advantage of ensemble techniques. Overfitting is a problem which describes good model performance on a training set but much worse performance on subsequent data, and thus, mediocre generalizability of the model (the model is not robust). A recent discussion of this problem, with many accessible examples, gives similar guidelines to those above, such as that leave-one-out cross-validation is not sufficient [64] . It also emphasizes the recommendation of multiple training/test set splits even in the case of very large dataset sizes and of performing cross-validation across classes of compounds in the case of close analogues (instead of molecule-by-molecule splits). In order to have some measure of overfitting, the use of 'benchmark models' such as partial least squares is recommended (depending on the particular problem) in order to determine whether there might be simpler models appropriate to the task (indicating that the more complex model overfits the data). Using a toxicity dataset of phenols against Tetrahymena pyriformis [65] the conclusion that q 2 is not a sufficient predictor for the applicability of a QSAR model to unseen compounds is corroborated, and suggests using the RMS error of prediction (RMSEP) instead. This guideline is presented along with additional important points: that outliers should not necessarily be deleted since this step reduces the chemical space covered by the model, that the number of descriptors in a multivariate model needs to be chosen carefully and finally that an 'appropriate' number of dimensions is required for PLS modeling. In addition, the influence of the number of variables on predictive performance for training and test sets is investigated. Several recent publications have attempted to investigate what the actual scope of a QSAR model is -and attempted to develop guidelines to assess the applicability of a model to a novel compound whose properties are to be predicted [66, 67] . Two measures for applicability are proposed: the similarity of the novel molecule to the nearest molecule in the training set and the number of neighbors of the novel compound within the training set with a similarity greater than a certain cutoff. As expected, molecules with the highest similarity are best predicted, and this was found to be true across datasets as well as across methods. The applicability measures described above can also be used numerically to derive error bars for estimations of how likely the prediction of a specific model is within a certain error threshold. The issue of model validity was also briefly reviewed from a regulatory viewpoint [68] . In a similar vein, a 'classification approach' has been presented for determining the validity of a QSAR model for predicting properties of a novel compound [69] . Focusing on linear models (though the underlying concept is more generally applicable), the predictions made for compounds within the initial training set are differentiated between 'good residuals' and 'bad residuals'. Using three different datasets (an artemisinin dataset as well as two boiling point datasets) machine-learning methods were employed to predict whether a novel compound belongs to the 'good' or 'bad' class of residuals, thereby making predictions as to whether its properties can be predicted -with a success rate of between 73% and 94%. A stepwise approach for determining model applicability [70] considers physicochemical properties, structural properties, a mechanistic understanding of the phenomenon and, if applicable, the reliability of simulated metabolism in a step-by-step manner. With several QSAR datasets, it could be shown that for substances that are well covered by the training set improved predictions can be made for novel compounds, in agreement with the conclusions stated above. The performance of similarity searching methods varies widely, comprising both target-and ligand-based approaches. While large enrichment factors (often in the hundreds) are reported, the question arises of how much 'added value' more sophisticated methods actually provide, compared to very simple approaches, and where the gain-to-cost ratio actually shows an optimum. A recent study illustrated that simple 'atom count descriptors' (which do not capture any structural knowledge but represent a molecule by a set of integers which represent the number of atoms of each element) are able to have comparable performance to state-of-the-art fingerprints [6] . Thus, when averaged over multiple target classes, the added value of virtual screening approaches is probably closer to two (compared to trivial descriptors) than in the region of often published double-digit numbers (compared to random selection). It should be added that performance of 'dumb' and more sophisticated descriptors varied widely, between virtually no difference in performance up to high single-digit performance improvements of state-of-the-art fingerprints (which are, with respect to retrieval rate and on a MDDR-dataset, circular fingerprint descriptors). 


Section:novel methods
Section:new properties of old methods
Section:method validation