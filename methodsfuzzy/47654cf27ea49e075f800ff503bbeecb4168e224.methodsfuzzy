Data analysis. Descriptive analysis. For every neighborhood N, the weekly number of animals and batches moved into N, were aggregated according to movement type ( Fig. 1 -represented by the blue arrows, additional information about data processing is found in Supporting Information S2). A batch is defined as a group of pigs transported from a point A to point B. Descriptive (average, minimum, maximum) weekly summaries of movements by the four movement types (finisher, weaning, breeding, or internal) were calculated for both number of animals and batches. The proportion of movements for each movement type was calculated relative to the total number of movements in 2014. The datasets used for this study are managed by the Morrison Swine Health Monitoring Project (MSHMP). The identity and location of the specific production systems that provided the data has been kept anonymous at the request of the production systems. Machine learning. In order to determine the association between PEDV outbreaks in sow farms and possible predictor variables, a set of supervised machine-learning (ML) algorithms: Random Forest (RF) 33, 53 , Support Vector Machine (SVM) 54 , and Gradient Boosting Machine (GBM) 55 were used. Considering the econometric theorem 'no free lunch' , which states that there is no single best algorithm or "silver bullet" that performs best in all situations 56 , we operated under the assumption that an algorithm that is ideal for one class of problems may perform poorly for other classes of problems 57 . Considering the above, we explored the most popular ML algorithms used for complex problems, RF, SVM and GBM, in order to carefully choose the algorithm with best performance and most suitable for our research question 53 . Data preparation. The outcome modeled through ML was the occurrence (yes) or absence (no) of a new PEDV outbreak within a sow farm during each week in 2014 as a function of environmental, landscape, and movement-related variables (Supplementary Information S1). Overall, 10.5% of sow farms experienced outbreaks at some point during 2014; outbreaks did not occur in 89.5% of sow farms. Given that imbalances between yes/ no classes will result in prediction biases in the machine-learning analysis 58 , in that algorithms tend to achieve high overall accuracy by predicting the majority class yet allowing for high error rates for predicting the minority class. Thus, we used a down-sampling strategy via "downSample" function in the R package Caret 59 in which the majority class was randomly down-sampled to match the frequency of the rarest class. Prior to down-sampling, the original data was randomly and uniformly divided into a training (80%) and an independent test set (20%). Data splitting is a common approach used for evaluation and validation of model performance when external data is limited or not available. The training set was used to train the ML algorithms via a k-fold cross-validation process and the quasi-independent test set (20% data not used for model building) was used for validation. Algorithm training. The supervised machine learning algorithms RF, SVM and GBM were trained (80% of data) using the complete set of explanatory variables. RF was performed with randomForest package 60 , and SVM and GBM were performed with the caret package 59 . In the training steps, we adopted a repeated 10-fold cross-validation to better estimate model performance and in order to prevent overfitting and artificial inflation of accuracy due to use of the same data for training and validation steps of the analysis. All algorithms were applied with default parameter setting. Model performance. Model performance was assessed by calculating the total training accuracy, specificity, and sensitivity based on the construction of a confusion matrix. Briefly, the confusion matrix displays the number of observed outbreaks that were correctly (true positive, TP) or incorrectly (false positive, FP) predicted by the model, as well as the number of farms where no outbreak occurred that were correctly (true negative, TN) or incorrectly (false negative, FN) predicted. Accuracy (ACC) was calculated as the overall proportion of observations correctly predicted. Specificity (SPE) was calculated by dividing TN by the sum of TN and FP (reported as a percentage). Sensitivity (SEN) was calculated as TP divided by the sum of TP and FN (reported as a percentage). In addition, the Receiver Operating Characteristic (ROC) curve was graphed for each algorithm and the area under the curve (AUC) was calculated as an additional assessment of model performance. For each ML algorithm, AUC, accuracy, sensitivity, and specificity were calculated based on the overall average confusion matrix across all folds in the cross-validation. The best algorithm was selected by comparing AUC, accuracy, sensitivity, and specificity for each of the three approaches. Model evaluation on independent data. A crucial step in the evaluation of ML algorithms is to access their prediction performance in independent data. The 20% of the data that was set aside from the original data set was used as a quasi-independent test set of observations. These data were fed into the ML algorithm, allowing the algorithm to predict the outcome for the new data. AUC, accuracy, sensitivity, and specificity were calculated. Variable selection. To rank the importance of each variable in improving the accuracy of model predictions, we used the unscaled version of the function "VarImp" from the caret package. Briefly, regardless of the algorithm, the variable's importance score (i.e., Gini Index) represents how relevant each variable was for predicting PEDV outbreaks, with larger values representing more important predictors. In addition, we calculated the accuracy decrease that quantified the extent to which the accuracy of predictions changed when the variable was permuted relative to the outcome. For the best-fit algorithm (random forest), we examined how often each variable was selected by the algorithm to subdivide the data in the individual decision trees that make up the random forest; each subdivision in the tree is referred to as "node. " A p-value was additionally calculated for each variable to evaluate whether it was used in the random forest more often than if variables were included in decision trees at random. P-values were based on a binomial distribution of the number of nodes split on the variable assuming that variables are randomly drawn 33, 34 . The influence of the most important variables was further analyzed via partial dependence plots, which provide insights on the marginal effect of each predictor on the likelihood of a PEDV outbreak while controlling for the effects of all other variables. The partial dependence of a variable's effect is best understood by visually examining general patterns in relation to the values of the predictor variable 61 . Because we are modeling binary classification (i.e., presence/absence of PEDV outbreaks), partial dependence values are reported on the "logit" scale and are computed in relation to the probability for the positive class 62 ; larger values indicate higher risk of PEDV outbreak. 


Section:material and methods