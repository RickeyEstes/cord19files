The random forest, artificial neural nets, and classification tree algorithms were selected for forecasting PEDV trends. The following sections provide a brief overview of the methods. The random forest algorithm (Breiman, 2001 ) is a non-parametric predictive modeling method which works by: 1) constructing multiple classification or regression trees, and 2) aggregating results from these trees to generate a prediction ('y' or response variable) for a specified set of input values ('x' or explanatory variables). Regression trees are constructed for a continuous response variable with the final prediction 'y' determined by averaging results across all trees. For a categorical response variable, classification trees are constructed and the final prediction 'y' is determined by a majority class vote across all trees. Furthermore, rather than determine the best node split by looking at all explanatory variables at a given node (as is the case with standard classification and regression tree algorithms), the random forest algorithm randomly selects a subset of explanatory variables at each nodewhich reduces the correlation between subtrees (Ho, 2002) -and then determines the best (or homogenous) binary split at the node. The random forest implementation in R -randomForest (Liaw and Wiener, 2002) -was used in this study. It provides, amongst other features, tuning functions for ascertaining the number of explanatory variables which should be randomly sampled at each node, as well as the optimal number of variables for predicting 'y'. In addition, the random forest implementation provides a variable importance measure, which ranks each explanatory variable per the mean decrease in prediction accuracy when the variable is randomly permuted and other explanatory variables left unchanged. Neural nets (or artificial neural networks) are predictive algorithms developed to mimic biological activity in the human brain, specifically the learning patterns for neurons. Neural networks have an input layer, hidden layer(s), and an output layer made up of interconnected neurons and an activation function. Predictors are supplied to the input layer, which transfers these values to one or more hidden layer(s) for processing via a system of weighted connections. These hidden layer(s) in turn link to an output layer which provides the final prediction result. Tuning parameters, such as the maximum number of learning iterations, learning rate, and number of hidden layers and weights, can be set for artificial neural networks (Shmueli et al., 2010) . The current neural network implementations in R are nnet (Venables and Ripley, 2002) and neuralnet (Fritsch and Guenther, 2016) . The nnet implementation permits one hidden layer, and has several tuning parameters, such as size (for specifying the number of neurons in the hidden layer), decay (a weight decay value to aid in the model optimization process and avoid overfitting), and maxit (the maximum number of permitted iterations). Classification trees are predictive algorithms which utilize recursive partitioning, a step-by-step process which splits a node into sub-nodes by evaluating a Boolean condition at each node. Observations which meet the Boolean condition are placed in one node, while the remaining observations are placed in another node. The process is repeated until a terminal node (which can no longer be split) is reached and a class label is assigned. Sub-trees are built with each recursive split, and each split (or partition) is constructed such that the resulting nodes are homogenous in nature (Izenman, 2008) . The classification tree implementation in R -rpart (Therneau et al., 2017) was used in this study. The rpart implementation has several tuning parameters, such as the minimum number of observations which must be present in a node for a split to be attempted, the number of cross-validations performed on observations to determine the best split, as well as complexity parameter cp, where any split that does not decrease the overall lack of fit by a factor of cp is not attempted. 


Section:classification methods