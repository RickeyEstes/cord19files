The study was conducted on three commercial dairy farms of the Veneto Region (northeastern Italy) during winter months (November to February) from the year 2014 to the year 2016. The number of farms to involve was conveniently established in order to meet the minimum number of calves needed for the study. The latter was estimated based on the formula by Daniel (1999) : for a confidence level set at 95%, an expected calf disease prevalence set at 20% (Lundborg et al., 2005; Windeyer et al., 2014) , and a relative precision set at 10%, the estimated calf sample size was 61. However, the minimum sample size for this study was set at 67 calves considering a dropout rate of 10%. The farms included in the study were similar in terms of cow breed reared (Italian Holstein), herd size (50-150 lactating cows), housing (loose with cubicles), feeding (total mixed ration) and milking (milking parlor) systems, management of calving (use of calving pen; parturition monitored by the farmer; calf separated from dam immediately after birth) and calves (navel disinfection; colostrum of the own dam provided by nipple-bottle; single housing until at least 5 wk of age), and adoption of a voluntary plan for bovine viral diarrhea (BVD) control that required blood sampling on newborn calves to identify persistently infected animals. On the three farms, cows were moved to a strawbedded calving pen 3 days before the expected calving date, according to the routine management. Only calves both born from eutocic calving and separated from dams within 10 min of birth were included in the study, irrespective of their sex and genotype (Holstein purebred or Holstein-beef crossbred). Navel disinfection was performed as first by dipping into a 7% tincture of iodine, then calves were moved to clean straw-bedded single pens, where they were provided their dams' colostrum by nipple-bottle according to the own farm practices. Fresh water, grass hay, and calf starter feed were available from the third day of life. The person in charge of carrying out the study visited the farms two times per week together with the veterinarian responsible for the BVD control plan. At each farm visit, the veterinarian collected blood samples from the jugular vein of calves between 1 and 5 days of age using a 10 mL Vacutainer ® tube without anticoagulant (Becton Dickinson, Franklin Lakes, NJ, USA). The same blood sample collected for the BVD control plan was used to assess also calf PI level. The health of the enrolled calves was monitored at each farm visit from birth to 30 days of age, and data on disease and mortality occurrence (i.e., type of disease, cause of mortality, age at disease onset or mortality occurrence, and antibiotic treatments on sick calves) were recorded. Additionally, the veterinarian collected fecal samples into a 100 mL tube from the rectum of those calves that both showed signs of clinical diarrhea (i.e., profuse liquid feces) and had not been treated yet at the time of the farm visit. Fecal samples were intended to be tested for enteric pathogens. Sick calves were treated according to the own farm therapeutic protocol. Blood and fecal samples were transported to the laboratory at refrigeration temperature (4°C) and delivered within 1 h of collection. At the laboratory, blood samples were centrifuged at 3076 × g for 10 min at 20°C and serum was spiked into two tubes, one for BVD virus testing and the other for Ig detection. Serum and fecal samples were stored at −20°C until the day of the analysis. Serum Ig concentration was determined by the same method used in Lora et al. (2017) , based on agarose gel electrophoresis. This method was conveniently chosen rather than the RID (radial immunodiffusion) gold standard method for IgG assessment based on both the diagnostic services provided by the laboratory and its recognized good accuracy for Ig assessment compared with RID (from 89% to 97%) (Pfeiffer et al., 1977; Rumbaugh et al., 1978; Massimini et al., 2006) . Fecal samples were thawed at room temperature and were tested by two commercial ELISA kits ("Rota-Corona-K99" kit and "Cryptosporidium Ag test" kit, Idexx, Montpellier, France) to detect antigens of E. coli K99, rotavirus, coronavirus, and Cryptosporidium spp. The ELISA tests were performed according to the manufacturer instructions, and the plates were read by spectrophotometer (Tecan Sunrise, Tecan trading AG, Männedorf, Switzerland) at a wavelength of 450 nm. The sensitivity and specificity of the tests reported in literature were 68% and 95% for rotavirus ELISA and 33% and 80% for coronavirus ELISA when compared with lateral flow immunochromatography (Izzo et al., 2012) , 100% and 91% for E. coli K99 ELISA when tested against PCR (Izzo et al., 2011) , and 94% and 96% for Cryptosporidium spp. ELISA when evaluated against other three diagnostic techniques (another EISA test, an immunofluorescence assay, and an immunochromatographic assay) (Geurden et al., 2008) . Differences in serum Ig concentration between sick and healthy calves, dead and survived calves, and sick calves treated or not with antibiotics were assessed as first by PROC GLM (SAS Institute Inc., Cary, NC), including the farm in the model as fixed effect. The assumptions of the linear model were visually inspected by plotting the graphs of the residuals of the model. Because the effects of calf sex, calf breed, and year of sampling were nested within farm, the latter was considered as proxy variable of the other three in this and the subsequent statistical analyses. Calves were then divided into two categories based on serum Ig concentration: having FTPI (serum Ig concentration < 10.0 g/L) or not having it (serum Ig concentration ≥10 g/L). As in previous works (Furman-Fratczak et al., 2011; Lora et al., 2017) , the threshold for FTPI definition was chosen based on the level commonly indicated for IgG concentration, as the latter accounts for approximatively 90% of serum Ig concentration (Godden, 2008) . The effect of FTPI, considered as predictive factor, on disease and mortality occurrence, antibiotic treatments for sick calves, and specific enteropathogen infections was investigated by PROC LOGISTIC (SAS Institute Inc., Cary, NC), including the farm in the model. Particularly, an exact logistic regression was performed for mortality, due to the low number of occurrence. Moreover, the effect of FTPI on specific enteropathogen infections was evaluated by considering only healthy calves and calves whose fecal samples were positive for the enteropathogen included in the model. Odds ratio and 95% CI were calculated for the dependent variables that were significantly affected by FTPI. Finally, a Kaplan-Meier survival plot was used to evaluate the age (days) at first disease outbreak of the 78 calves stratified by presence or absence of FTPI. The equality over strata was tested by Log-rank and Wilcoxon statistics. A Cox proportional hazard model was then used to assess the risk of disease outbreak over the time, and hazard rate ratios and 95% CI were determined. Test for proportionality was carried out by visual inspection of the Schoenfeld residuals. The level of significance was set at P < .05 for all the statistical analyses performed in this study. 


Section:material and methods