We included a total of 469 patients with oncological and hematological malignancies, of whom 8.9% (n = 42) were admitted to the ICU and 18.7% (n = 88) did not survive until the 30-day followup. There was a strong association of initial proADM levels and 30-day mortality risk (odds ratio (OR) per 10-fold increase 9.9, 95% CI 4.3, 22.9) with an AUC of 0.67 (95% CI 0.60, 0.74). This association remained signifi cant after multivariate adjustment for initial vital signs (blood pressure, pulse, temperature) and comorbidities (chronic heart failure, chronic obstructive pulmonary disease, diabetes, coronary heart disease) with an adjusted OR of 9.0 (95% CI 3.1, 26.4). There was also a signifi cant association of proADM and LOS (adjusted regression coeffi cient per 10-fold increase: 6.6, 95% CI 2.0, 11.2). Conclusion This study including consecutive patients with oncological and hematological malignancies found a moderate association of proADM with 30-day mortality and LOS. proADM in combination with clinical parameters may help to improve site-of-care decisions for these patients in the future. Introduction A prospective multicentre observational study was carried out to assess the extent to which critical care teams manage patients in hospital who are cared for outside the critical care unit. The Health Service Executive (HSE) in Ireland is in the process of implementing a national Early Warning Score (EWS) and, at an EWS of 7 or above, a referral to critical care is recommended. This study recorded the EWS of patients referred to the critical care team and describes the subsequent interventions made by the critical care team and patient outcomes. Methods Six critical care departments in university-affi liated hospitals across Ireland collected data on all referrals to the critical care team over a 6-week period. Data were anonymised, coded and analysed centrally. Results A cumulative total of 399 calls were made to the critical care teams in the six hospitals. The most common reason for referral was to request a critical care review of a patient (n = 319, 79.9%). Other reasons for referral included cardiac arrest, request to transfer patients from other hospitals and requests for vascular access. The average duration spent by the critical care team reviewing patients on the wards was 57 minutes. This increased up to 67 minutes for cardiac arrest calls. Of the 319 critical care reviews, 160 (50.2%) patients were subsequently admitted to critical care. A total 118 of this 160 had EWS of 7 or above, while 42 scored less than 7 but were still deemed to require admission to critical care. Conclusion Regardless of the EWS, critical care teams are heavily involved in the management of patients outside critical care units. Fifty per cent of patients reviewed by the critical care team subsequently required admission to a critical care unit. The trigger threshold (7 and above) for referral to a critical care team currently recommended by the EWS escalation protocol is more likely to predict need for critical care admission. However, one in four patients referred below the threshold also required admission to a critical care environment. This study questions the safety of introducing such a protocol into acute hospitals. Will noncritical care staff be forced to wait until patients deteriorate further and reach the trigger threshold for referral or will the role of the critical care team expand further to look after all patients with abnormal EWS in hospital? Introduction We aimed to assess actions taken in response to variations in the National Early Warning (NEW) score and to identify factors associated with a poor response. The NEW score is a physiological score, which prescribes an appropriate response for the deteriorating patient in need of urgent medical care. This allows enhanced observation and clinical review of patients, identifying patients at risk of acute mortality. Methods We performed a prospective observational study of adult patients admitted to an acute medical ward in a London district general hospital over a 2-week period. Patient characteristics, NEW score, time of day, day of week and clinical response data were collected for the fi rst 24 hours of admission. Patients with less than a 12-hour hospital stay were excluded. The primary outcome measure was the quality of clinical response. Data were analysed with univariate and multivariate logistic regression. Results During the study period 200 patients were included with a median age of 70 (20 to 102) years. NEW scores were evenly distributed between day and night (52% vs. 48%) with a greater proportion on weekdays compared with weekend days (82% vs. 18%). The majority of patients scored <5 (93% vs. 7%). Forty-seven (27%) patients received an inadequate clinical response. Univariate analysis showed no association with time of day (night 34% vs. day 38%, OR 0.83 (0.47 to 1.49), P = 0.556). However, day of the week (weekend 56% vs. weekday 32%, OR 2.8 (1.30 to 5.84), P = 0.01) and increasing score (NEWS ≥5 100% vs. NEWS <5 31%, OR 65 (3.8 to 1100), P < 0.0001) were signifi cantly associated with an inadequate response. Day of the week was independently associated with an inadequate response after adjusting for confounders (OR 3.08 (1.27 to 7.46), P = 0.013). Conclusion Clinical response to NEW score triggers is signifi cantly worse at weekends, highlighting an important patient safety concern. Reference Introduction The purpose of this study was to evaluate the impact of obesity on outcomes in patients with severe sepsis. Since obesity is considered an infl ammatory disease and is associated with elevations in several infl ammatory mediators important in the outcome of sepsis, the relationship between obesity and outcome in septic patients was studied. Methods This retrospective cohort study included all patients over the age of 40 with a confi rmed diagnosis of severe sepsis and an ICU stay at our academic medical center from 1 January 2005 to 31 March 2011. Obesity was defi ned as a body mass index of 30 or greater. Data on other patient demographics and APACHE II score at the time of sepsis were collected from patient charts. Outcomes measured included inhospital mortality, development of acute respiratory distress syndrome (ARDS), days on mechanical ventilation, hospital cost, and length of stay. We identifi ed 824 patients who met the inclusion criteria for this study. Of these patients, 257 (31.2%) were classifi ed as obese. The mean APACHE II score was similar between obese and nonobese patients (23.3 vs. 22.4; P = 0.068). Obese patients had a similar rate of in-hospital mortality (31.9% vs. 33.7%; P = 0.810) compared with nonobese patients, but a signifi cantly higher rate of development of ARDS (49.4% vs. 34.4%; P <0.001). Obese patients also had signifi cantly Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 S16 more days on mechanical ventilation (6.2 days vs. 5.0 days; P = 0.005). There was no relationship between mortality in obese patients on mechanical ventilation (34.4% vs. 39.5%; P = 0.26) or ARDS (33.9% vs. 42.6%; P = 0.13) compared with nonobese patients. Hospital costs and length of stay did not diff er between the groups. Conclusion Obesity signifi cantly increased the incidence of ARDS and days on mechanical ventilation in patients with sepsis. Previous work has reported that obesity is associated with elevations in infl ammatory cytokines and adipokines, particularly IL-6, which is a known risk factor for ARDS. The higher rate of ARDS in obese patients with sepsis identifi es a high-risk group where new therapies may be most benefi cial and where new methods of preventing ARDS can be targeted. Introduction Studies suggest that obesity may infl uence mortality in patients who develop sepsis. However, the mechanisms linked to improved outcomes are unclear. Our aim was to assess the impact of obesity on mortality at 30 and 180 days and cytokine expression. Methods We used a platform of a negative randomized control trial in subjects (n = 51) with a diagnosis of severe sepsis with ≥1 organ failure. The cohort of severe septic subjects was stratifi ed by obesity status based on the body mass index (BMI >30). Primary outcomes: 30-day and 180-day mortality; secondary outcome: diff erence in median (IQR) of fi ve infl ammatory cytokines including tumor necrosis factor alpha (TNFα), TNFα-receptor 2, interleukin (IL)-6, IL-1-receptor-antagonist (IL-1ra) and IL-10. The measurement of median baseline cytokine levels was done in serum by Luminex technology. Statistical signifi cance was defi ned as P <0.05. Results Fifty-one subjects with severe sepsis were included in the study; 37% of the patients were obese (BMI >30). Paradoxically, obese severe septic patients had lower 30-day mortality (n = 1 (5%) vs. n = 9 (28%), P = 0.069) and 180-day mortality (n = 1 (5%) vs. n = 13 (41%), P = 0.008), when compared with nonobese. The expression of TNFα, TNFα-receptor 2, IL-6, IL-1ra and IL-10 was not statistically signifi cant diff erent among obese versus nonobese severe septic patients. Conclusion Obesity is associated with lower mortality rates at 30 and 180 days in patients diagnosed with severe sepsis. This survival benefi t was not associated with lower cytokine production among obese patients. Further studies are needed to assess the mechanisms associated with the survival benefi t related to obesity in patients with severe sepsis. Introduction Predicting long-term outcome in patients surviving a pneumonic or nonpneumonic COPD exacerbation remains challenging. This study investigates the association of clinical parameters and the prognostic blood marker pro-adrenomedullin (proADM) measured upon hospital discharge with 6-year mortality in well-defi ned cohort of COPD patients. Methods We prospectively followed consecutive COPD patients from a previous Swiss multicenter trial (2006 to 2008) [1] over a 6-year followup and investigated all-cause mortality following hospital discharge. Patients and/or treating general practitioners were contacted by telephone interview to assess the vital status of patients. We used Cox regression models and the area under the receiver operating characteristics curve (AUC) to investigate associations of baseline predictors and mortality. Overall mortality in the 469 included COPD patients was 55% (95% CI 0.5 to 0.6) with a 14% (95% CI 0.1 to 0.2) mortality incidence rate per year. Patients with pneumonic COPD exacerbation had a more pronounced infl ammatory response compared with patients with nonpneumonic exacerbation with regard to levels of initial C-reactive protein levels (median 158 mg/dl vs. 39 mg/dl, P <0.0001), procalctionin (median 0.4 μg/l vs. 0.1 μg/l, P <0.0001) and proADM (median 1.3 nmol/l vs. 0.9 nmol/l, P <0.0001), but long-term survival was similar (HR 1.0, 95% CI 0.8 to 1.2) . In univariate regression models, proADM was signifi cantly associated with mortality after 1, 3 and 6 years (HR 16.1 (95% CI 6.9 to 37.7), 10.5 (95% CI 5.7 to 19.6) and 10.4 (95% CI 6.2 to 17.7), respectively). There was no eff ect modifi cation by type of exacerbation. A model including clinical parameters (age, coronary heart disease, heart failure, diabetes mellitus, chronic renal failure, neoplastic disease, pneumonia, smokers) and proADM showed good discrimination of long-term survivors from nonsurvivors with AUC of 0.74 (95% CI 0.6 to 0.7). Conclusion Clinical parameters and discharge levels of proADM allow accurate long-term prognostication in COPD patients independent of initial type of exacerbation. The focus on the best use of long-term prognostic information to improve patient care and clinical outcomes seems promising/rational. Reference Introduction Based on expert opinion and case note review, the UK National Confi dential Enquiry into Peri-operative Outcome has recommended provision of perioperative level 2 and 3 care to support major surgery in older people, and particularly those with comorbidity [1] . We wished to identify whether we could predict if the need was uniform and whether any factors could predict the degree of organ supports needed. Methods A retrospective note review of all patients admitted to a level 2 critical care unit in the 12-month period from 1 January 2012 to 31 December 2012 undergoing revision hip surgery either as a two-stage or single-stage process. Surgery was undertaken at a national referral unit and chosen to represent an appropriate group of older, comorbid patients. Predefi ned preoperative and perioperative data were collected from chart review, along with postoperative physiological data whilst the patient was in critical care. This included frailty, comorbidities, operative blood loss, anaesthetic technique and level and duration or organ supports including the need for additional medical review whilst on the unit. Frailty was assessed preoperatively using the Rockwood assessment tool by trained staff [2] . Data were analysed using Microsoft Excel for Mac 2011 and Stata/IC 11.2 for Mac. Results A total of 182 patients with a mean age of 69.8 years (range 29 to 92) were identifi ed. Frail patients were signifi cantly more likely to need additional medical input in the postoperative period whilst on critical care ( Figure 1 , P = 0.002) but this was not signifi cantly linked to need for vasopressors, evidence of sepsis or choice of anaesthetic technique. Conclusion In complex revision orthopaedic surgery, the need for postoperative level 2/3 support cannot be predicted from any preoperative or intraoperative factors but patient frailty does indicate the need for medical input in the postoperative period. References Introduction ICU or hospital mortality rates have been reported as the endpoint of ICU therapy for many years. The aim of this study was to determine the 1-year mortality after discharge from the ICU in patients who were treated in the ICU for more than 72 hours and to identify predictors for 1-year mortality. Methods This study was conducted in a 20-bed mixed ICU of a teaching hospital. The study sample was extracted from a dataset of all ICU patients treated for more than 72 hours between 1 January 2007 and 1 October 2012. Demographic characteristics and clinical characteristics at admission and during the ICU stay were collected. Characteristics of patients alive 1 year after ICU discharge were compared with patients who died within the fi rst year after ICU discharge. Descriptive statistics were calculated. Multivariate analysis of 1-year mortality was performed using a logistic regression model with backward elimination. Survival was analysed by the Kaplan-Meier method using the time interval from day of ICU discharge until death. Results During the study period, 740 patients were treated for more than 72 hours in the ICU. The ICU mortality was 106/740 (14%). The data of 617 ICU survivors were further analysed (17 patients were lost to follow up). Conclusion Of patients being treated for more than 72 hours in the ICU, 28% died within 1 year after ICU discharge. One-half of them within the hospital stay after ICU discharge. High age at ICU admission, high APACHE IV predicted mortality score, high number of comorbidities, readmission and an admission diagnosis within the categories 'cardiovascular' and 'sepsis' are associated with an increased 1-year mortality after ICU discharge in this population. The burden of patients dying after ICU discharge underlines the necessity for clear ICU discharge criteria and post-ICU care. physical composite score (PCS) 36.2, mental composite score 48.1; 50 = national average). Reduced muscle strength was associated with low scores on the SF-36 physical function and general health domains. Performance on the 6MWT correlated with the SF-36 including the PCS (P = 0.001). Screening positive for anxiety was associated with both poor 6MWT performance and reporting dysfunction on the EQ-5D domains. ICU/hospital length of stay, number of days ventilated, severity of illness and organ dysfunction were not found to be predictive of muscle strength or physical functioning. Conclusion Our study gives qualitative evidence that survivors of critical illness have reduced muscle strength, physical functioning and HRQL after hospital discharge. Also, we have shown muscle weakness is predictive of overall physical functioning, which in turn impacted HRQL and mental health. No ICU risk factors were identifi ed that predicted defi cits in muscle strength or physical functioning. Introduction The aim of this study was to determine an appropriate risk model to identify patients at high risk of prolonged ICU stay and to aid patient consent prior to cardiac surgery. Methods Data were prospectively collected on 5,440 consecutive cardiac surgery cases between April 2009 and March 2012. The primary outcome measure was the combined outcome of prolonged ICU stay (length of stay greater than 20 days) and/or in-hospital mortality. Logistic regression was performed to assess the predictability of logistic EuroSCORE against the primary outcome. Low-risk, mediumrisk and high-risk groups were identifi ed and subsequent risk of 1-year mortality assessed. Survival status was determined at 1 year. Results A total of 192 (3.5%) patients had a prolonged ICU stay and 187 (3.4%) in-hospital deaths occurred, resulting in a combined primary outcome of 349 (6.4%). At 1 year, 371 (6.8%) deaths occurred. The risk of death in-hospital and at 1 year was signifi cantly higher in patients with prolonged ICU stay (in-hospital mortality, 15.6% vs. 3.0%; P <0.001/1 year, 27.6% vs. 6.1%; P <0.001). The mean logistic EuroSCORE for all patients was 10.9. Patients with prolonged ICU stay had a signifi cantly higher logistic EuroSCORE (20.3 vs. 10.6; P <0.001). The logistic EuroSCORE was a reasonable predictor of prolonged ICU/ in-hospital mortality (OR 1.04, 95% CI 1.04 to 1.05, P <0.001) with a receiver operating characteristic (ROC) curve of 0.72. The relationship between a patient's logistic EuroSCORE and predicted risk of prolonged ICU is shown in the fi gure; including low-risk, medium-risk and high risk groups. Around 50% of the entire cohort of patients had a logistic EuroSCORE of 10 or less and an associated risk of prolonged ICU stay of 5% or less. See Figure 1 . Conclusion Using an existing risk prediction model, a patient's risk of prolonged ICU stay can be calculated using contemporaneous data. This information could be relevant for aiding in providing informed consent for cardiac surgery patients. Introduction Information about lung cancer patients surviving critical illnesses is very scarce. Our aim was to evaluate the outcomes and continuing of anticancer treatments in lung cancer patients surviving ICU admission. Methods Secondary analysis of a prospective multicenter study including patients admitted for >24 hours to 22 ICUs in six countries from Europe and South America during 2011. Readmissions and patients in cancer remission >5 years were excluded. Logistic regression was used to identify predictors for hospital mortality. Results A total of 449 patients (small-cell (SCLC) = 55; non-SCLC = 394)) were admitted to ICUs, and out of them 275 (SCLC = 29; NSCLC = 246) were discharged alive from the hospital. Among them, 200 (73%) patients were alive and 72 (26%) had died at 6 months; three (1%) patients were lost to follow-up. Mortality rates were far lower in the patient subset with nonrecurrent/progressive cancer and a good performance status (PS), even those with sepsis, multiple organ dysfunctions, and need for ventilatory support. Cancer recurrence or progression occurred in 53 (26%) hospital survivors. Anticancer treatments were recommended for 108 (39%) hospital survivors and administered to 102. Treatments used were variable combinations of surgical resection (7%), radiation therapy (34%), and chemotherapy (80%). The initial treatment plan required reduction or modifi cation in 35 (34%) patients. Post-hospital mortality was nonsignifi cantly lower in the patients given the initial treatment plan than in the other patients (17% vs. 32%, P = 0.065). Poor PS was the only factor associated with a lower probability of receiving the initial treatment plan (OR = 0.20; 95% CI, 0.05 to 0.87; P = 0.032). At 6 months, 71% patients were at home, 15% were hospitalized, and 7% were in hospice care; the location was unknown for 6% patients. PS at 6 months was 3 to 4 in 19 (9.5%) survivors. Conclusion Post-hospital mortality in critically ill lung cancer patients is relatively high and many patients require anticancer treatments after discharge. PS before ICU admission is a major determinant of both mortality and ability to receive optimal anticancer treatment in these patients. The survival rate at day 7 in the BMMNC group was 80.0%, and that in the CR group was 47.6%, revealing that the 7-day survival was signifi cantly improved by the BMMNC injection (P <0.05). Crush injury-induced upregulation of serum IL-6 was signifi cantly reduced by the BMMNC treatment at all time points (P <0.05). The level of TNFα decreased signifi cantly in the BMMNC group compared with that in the CR group 24 hours after the compression release (P <0.05). These fi ndings suggest that transplantation of BMMNCs has an ability to evade the devastating condition following crush injury by suppressing systemic infl ammation. Conclusion The administration of BMMNCs reduced production of infl ammatory cytokines and improved survival rate in a rat model of crush injury. Cell therapy using BMMNCs might become a novel therapy against crush injury. Introduction Some patients presenting to the emergency department (ED) currently face inacceptable delays in initial treatment due to suboptimal initial triage. Triage scores, such as the Manchester Triage System (MTS), have not been well validated in unselected medical patients. Herein, we performed a prospective cohort study to assess the prognostic potential of the MTS and the prognostic biomarker proadrenomedullin (ProADM) to identify patients at high initial treatment priority, patients with admission to the ICU, and patients who die within a 30-day follow-up. Methods This is a prospective, observational cohort study including all consecutive medical patients seeking ED care between June 2013 and October 2013, except nonadult and nonmedical patients. We collected detailed clinical information including the initial MTS and measured ProADM levels on admission in all patients. Initial treatment priority was adjudicated by two independent, blinded physicians based on all available results at the time of ED discharge to the medical ward. To assess outcomes, data from electronic medical records were used and all patients were contacted by telephone 30 days after hospital admission. The prognostic performance of MTS and ProADM was assessed in multivariate regression models with area under the receiver operating curve (AUC) as an overall measure of discrimination. Results We included a total of 1,452 patients (58% males, mean age 66.6 years). A total of 20.1% (n = 292) were classifi ed as high treatment priority, 5.4% (n = 79) were admitted to the ICU and 4.4% (n = 64) died within 30 days. The initial MTS showed a good prognostic accuracy to predict treatment priority (AUC 0.75) and ICU admission (AUC 0.76), but not for mortality prediction (AUC 0.58). Initial ProADM levels were independent predictors for all three outcomes and signifi cantly improved the MTS score to AUCs of 0.78 for treatment priority, 0.80 for ICU admission and 0.84 for mortality. Conclusion Within this large cohort of consecutive unselected medical patients seeking ED care, the MTS instrument in combination with a prognostic biomarker (ProADM) allowed accurate initial risk assessment in regard to treatment priority, ICU admission and mortality. A combined score has the potential to signifi cantly improve initial risk assessment in patients, which may translate into faster and more targeted care and better clinical patient outcomes. Introduction At the hospital we are faced with situations of violence, whether verbal or physical, especially in the emergency department (ED) [1] . The objective of this study is to evaluate the phenomenon of violence at hospitals in Lebanon, especially in the ED, and to recommend techniques to prevent it. Methods A questionnaire consisting of 18 questions was sent to the caregivers in the ED of three randomly selected hospitals in Beirut, Lebanon in 2012. A total of 111 people (nurses, aides, doctors, interns, residents, social workers and security guards) responded to the survey questionnaire. Results The majority of the surveyed people are young women (62%) aged between 20 and 40 years (78%) with a nursing degree (74%) and professional experience <5 years (48%). In total, 59% of respondents have experienced violence in the ED during the night (58%) from the patients (31%) or their companions (68%). The caregivers most aff ected by the violence are nurses (54%) and employees of reception (46%). Violence can be verbal (threats 47%, insults 36%, criticism 18%) or physical (hitting 43%, slapping 40%, stabbing 17%). The dissatisfaction of the patient in his care (42%) and his anxiety (33%) are the most important factors in the generation of violence that may have repercussions on the care workers and their psychological status. Conclusion Violence in the ED may be due to the heavy workload of the caregivers causing a delay in care. Secondly, patients in the ED may feel insuffi ciently informed and heard by the nursing staff . The priority given to emergencies depending on the severity and not the order of arrival can be misunderstood [2] . Therefore, we recommend the following actions: encourage caregivers to improve their knowledge and training on the management of patients in emergency situations; train emergency caregivers to mediation, nonviolent communication and managing stressful situations; and increase the number of nurses and security guards in the ED and motivate them to ensure a better quality of care and minimize the delay in their care. In 1 year from December 2012 to November 2013 inclusive, the ICU recorded 84 alcohol-related admissions, accounting for approximately 9% of annual unplanned admissions. With an average length of stay (5.8 days) similar to that of all other unplanned admissions, this totalled 534 ICU bed-days. A total of 86% of patients with alcohol-related conditions were male with an average age of 46.4 years (range 15 to 83 years), and the majority (42%) presented with chronic conditions partially attributable to alcohol consumption. The number of admissions per month varied from zero in May to a peak of 14 in November, with the majority (40%) of admissions occurring over the autumn months ( Figure 1 ). Eighty-nine per cent of patients with alcohol-related conditions required support for at least two organ failures, which subsequently equated to an overall cost to the unit of £725,308 and 12% of an approximate £6 million annual budget. The initial audit examined 901 patient-days. Urea and electrolytes (U&Es) and full blood counts (FBC) were requested in line with the guidelines. Liver function tests (LFTs), bone profi le, magnesium and a clotting screen were ordered approximately four times more than advocated. It was shown that many bone profi les and magnesium tests were probably inappropriate requests. Moreover, twice as much blood was taken from patients compared with that recommended by guidelines (almost 16 litres in total in January). The cost of the routine blood tests in January was €11,019. If guidelines had been followed, the estimated yearly saving would be €65,588. During the repeat audit, 731 patient-days were examined. The amount of times a U&E or FBC were requested was largely unchanged, but the amount of times a LFT, bone profi le, magnesium and clotting screen were ordered reduced by approximately 50%. Almost one-third more blood was taken from patients when compared with the suggested volume in the guidelines. The cost of the blood tests done in July was €5,423. Despite an improvement in the frequency of blood testing, an estimated €21,907 per year could still be saved. In the pre-implementation phase there were a total of 4,602 blood draws in 5,227 total patient-days, (0.88 blood draws/patientdays). After consolidation, there were 1,095 blood draws in 1,491 patient-days (0.73 blood draws/patient-day; 17% reduction). Of these line entries, 24.7% were arterial line entry, 50.5% central line entry and 12.5% were by peripheral venipuncture. After policy implementation, these numbers were 10.9%, 49.7%, and 23.8%, respectively. The average central line unique entry after blood draw consolidation decreased from 10 to 6 line entries/central line-day. Consolidation of blood draws was associated with a cost saving of $7,200/year. Conclusion Consolidating time frames for blood draws in the PICU was associated with decreased central line entries, decreased utilization of vascular access teams, and decreased phlebotomy cost. We hypothesize that this policy will be associated with a decreased incidence of CLABSI when more patients are included for analysis. We assessed the clinical impact of goal-directed coagulation management based on rotational thromboelastometry (ROTEM) in patients undergoing an emergent cardiovascular surgical procedure. Methods Over a 2-year period, data from 71 patients were collected prospectively and blood samples were obtained for coagulation testing. Administration of packed red blood cells (PRBC) and hemostatic products was guided by an algorithm using ROTEMderived information and hemoglobin level. Based on the amount of PRBC transfused, two groups were considered: high bleeders (≥5 PRBC; HB) and low bleeders (<5 PRBC; LB). Data were analyzed using the chisquare test, unpaired t test and ANOVA as appropriate. Results Preoperatively, the HB group (n = 31) was characterized by lower blood fi brinogen and decreased clot amplitude at ROTEM compared with the LB group (n = 40). Intraoperatively, larger amounts of fi brinogen, fresh frozen plasma and platelets were deemed necessary to normalize the coagulation parameters in the HB group. Postoperatively, the incidence of major thromboembolic and ischemic events did not diff er between the two groups (<10%) and the observed in-hospital mortality was signifi cantly less than expected by the POSSUM score (22% vs. 35% in HB group and 5% vs. 13% in LB group). Conclusion ROTEM-derived information is helpful to detect early coagulation abnormalities and to monitor the response to hemostatic therapy. Early goal-directed management of coagulopathy may contribute to improve outcome after cardiovascular surgery. Reference There is a need for further clarifi cation around coagulopathy and interventional radiology in the critical care setting. The low absolute incidence of bleeding complications and risk of complications from transfusion lends further support to the view that FFP should be used therapeutically rather than as prophylactic 'cover' [1] . Our results show values of heparin activity measured in bags and effl uents with and without in-line fi ltration after 24-hour infusion for both types of bag assessed (Tables 1 and 2) . Results are expressed as median values (minimum to maximum) in percent.  The results confi rmed increased and prolonged bleeding of edoxaban-treated animals following standardized kidney injury compared with vehicle administration. Parallel monitoring of biomarkers of hemostasis showed a prolongation of PT, aPTT, WBCT, and changes in thrombin generation parameters. Subsequent administration of Beriplex® resulted in a dose-dependent reversal of edoxaban-induced bleeding as indicated by reduced time to hemostasis and total blood loss. Both parameters achieved statistical signifi cance compared with placebo at the Beriplex® dose of 50 IU/kg under fully blinded study conditions. The biomarkers correlating best with Beriplex®-mediated edoxaban anticoagulation reversal included PT, WBCT and endogenous thrombin potential. Conclusion In summary, Beriplex® treatment eff ectively reversed edoxaban-induced anticoagulation in an animal model of acute bleeding at clinically relevant dose levels. dabigatran-treated animals were randomized (n = 6/group) to a single injection of idarucizumab at 30, 60 or 120 mg/kg i.v. or vehicle (control animals). Blood loss and hemodynamic variables were monitored over 4 hours or until time of death. Data were analyzed by ANOVA (± SD) and by the log-rank test. Results Dabigatran levels were 1,147 ± 370 ng/ml with no diff erences between groups prior to injury. BL in sham animals was 409 ± 53 ml 10 minutes after injury and 700 ± 107 ml after 4 hours (survival rate 100%). Anticoagulation with dabigatran (control animals) resulted in signifi cantly higher BL 10 minutes after injury (801 ± 66 ml, P <0.05). Mortality in these animals was 100%, with a mean survival time of 121 minutes (range: 90 to 153 minutes; P <0.05 vs. sham and idarucizumab-treated animals). Total BL in dabigatran-treated animals was 2,977 ± 316 ml. In contrast, treatment with idarucizumab was associated with a dose-dependent reduction in BL. Conclusion Bivalirudin is a valuable option for anticoagulation in patients with VAD, and can be easily monitored with aPTT. The use of a bivalirudin-based anticoagulation strategy in the early postoperative period may overcome many limitations of heparin, and above all the risk of HIT which is higher in patients undergoing cardiac surgery. Bivalirudin should no longer be regarded as a second-line therapy for anticoagulation in patients with VAD. Introduction Free hemoglobin (fHb) can scavenge nitric oxide and induce vasoconstriction [1] . The fHb content may be higher in older blood bags. We studied whether old red blood cell (RBC) transfusion increases plasma fHb in septic patients and if this aff ects the microvascular response. Methods Twenty septic patients randomly received either fresh (<10 days storage) or old (>15 days) RBC transfusion. Plasma fHb was measured before and 1 hour after transfusion; the sublingual microcirculation was assessed with sidestream dark-fi eld imaging. The perfused boundary region (PBR) was measured as an index of glycocalyx damage [2] . The thenar Tissue Hb index (THI) was measured (near-infrared spectroscopy). Results fHb increased in the old RBC group ( Figure 1 ). THI increased in both groups, while SDF parameters were unaltered. Negative correlations were found between ΔfHb and changes in total vessel density (r = -0.57, P <0.01; Figure 2 ) and THI (r = -0.71, P <0.001). These relations were lacking in patients with PBR <2.68 μm.  We report a 57-year-old female, without a previous medical record. She had an acute onset of fever, cough, muscle pain and progressive dyspnea leading to acute respiratory distress syndrome. The test for infl uenza A H1N1 was positive. She was recovering, but on day 12 of admission, after 1 hour of platelet transfusion, she started with intensive tachycardia, dyspnea and hypoxemia. Her mechanical ventilation parameters increased dramatically. She was in plan for extubation with FiO 2 of 30% and positive end-expiratory pressure of 8, which became 100% and 14 respectively. The P:F ratio dropped to 62. Her leukocytes were 10.6×10 9 /l a few hours earlier and went down to 1.5×10 9 /l after the onset. Previous lactate was normal, but jumped to 42 mg/dl. She was free of vasopressors and after the off ending transfusion went through refractory shock and died approximately 24 hours after the blood transfusion. Conclusion For our knowledge this is the fi rst case reported of TRALI in an infl uenza A (H1N1) patient. Although blood transfusion can be life saving, it also can be a life-threatening intervention. Prevention is still the best hit. References Introduction Prothrombin complex concentrate (PCC) has been suggested as a measure to terminate trauma and dabigatran-induced bleeding. Owing to the confl icting data concerning such therapy, we investigated the impact of a four-factor PCC to terminate massive bleeding following the infl iction of multiple trauma in dabigatran anticoagulated pigs. Methods After ethical approval, 24 male pigs were administered dabigatran etexilate (30 mg/kg bid p.o.) for 3 days. On day 4, dabigatran in anaesthetised animals was infused prior to injury to achieve supratherapeutic levels. Twelve minutes after infl iction of bilateral femur fractures and standardised blunt liver injury, animals randomly received PCC (25, 50 or 100 IU/kg; n = 6) or placebo (n = 6). Time-adjusted blood loss as primary endpoint (observation period 300 minutes) and a panel of coagulation variables were continually measured. Data were analysed by two-way ANOVA. Data are mean ± SEM. Results Concentrations of dabigatran prior to infl iction of trauma was comparable between groups (590 ± 40 ng/ml). Anticoagulation with dabigatran and trauma caused severe coagulopathy as shown by prolonged TEM variables (CT, CFT), PT and aPTT. Following PCC application these eff ects were partially reversed. Due to ongoing blood loss both PT and TEM variables prolonged over time in PCC 25 IU/kg substituted animals. Accordingly, no-PCC (38.5 ± 4.7 ml/minute) and PCC 25 IU/kg (22.6 ± 5.5 ml/minute) animals showed highest blood loss (P <0.05 vs. PCC 50 IU/kg and PCC 100 IU/kg) with a mean survival time of 106 minutes (no-PCC animals) and 204 minutes for PCC 25 IU/kg animals, respectively. All animals of the PCC 50 IU/kg and PCC 100 IU/kg group survived. Blood loss in both groups was comparable (PCC 50 IU/ kg: 5.9 ± 0.2 ml/minute; PCC 100 IU/kg 6.0 ± 0.3 ml/minute). The success rate of the infl ationary NIBP (completed only by infl ationary method) was 69.0%. The bias and precision of systolic pressure and diastolic pressure (diff erence of systolic and diastolic pressure between infl ationary and defl ationary NIBP) were -0.6 ± 8.8 and 3.5 ± 7.5 mmHg, respectively ( Figure 1 ). Infl ationary NIBP could also determine NIBP more quickly compared with defl ationary NIBP (16.8 vs. 29.1 seconds, median) ( Figure 2 ). Conclusion These data suggest that infl ationary NIBP has reasonable accuracy and suffi cient rapidity compared with defl ationary NIBP in emergency room patients. References Introduction  The evolution of the arterial pulse along the arterial tree is shown in Figure 1 . We also predict arterial pulses for increasing left ventricular ejection energies. Conclusion Our simple model explains many features of the arterial pulse observed in clinical practice such as the development of the dicrotic notch, the change in shape along the arterial tree and the steepening and acceleration with hypertension. Some phenomena that have traditionally been attributed to arterial wave refl ections or resonance of the invasive arterial pressure measurement can instead be explained by intrinsic properties of the arterial pulse. Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 Introduction Pay-for-performance programs and economic constraints call for solutions improving the quality of healthcare without increasing costs. Many studies have shown decreased morbidity in major surgery when perioperative goal-directed therapy (PGDT) is used. We assessed the clinical and economic burden of postsurgical complications in the University HealthSystem Consortium (UHC) in order to predict potential savings with PGDT. Methods Data from adults who had 10 major surgical procedures in 2011 were screened in the UHC database. Thirteen postsurgical complications were tabulated. In-hospital mortality, hospital length of stay and costs from patients with and without complications were compared. The risk ratios reported by the most recent meta-analysis were used to estimate the potential reduction in postsurgical morbidity with PGDT. Potential cost savings were calculated from the actual and anticipated morbidity rates. Patients were randomly selected and catheterization was performed in a neutral position of the head (n = 50) and by turning the head 45° to the opposite side (n = 50). The US-guided catheterization procedure was performed in accordance with general principles. Once the needle entered the IJV and blood was aspirated, the US probe was released from the hand, and the catheter was placed and fi xed according to the Seldinger technique. The data of the intervention side for each process, the count number of successful catheter insertion, whether there is arterial access or not and the duration of procedure (from skin contact of the needle to catheter insertion) were recorded. The localization of the IJV in relation to the CA is 66% anterolateral, 4% anterior and 30% lateral in a neutral position, and 62% anterolateral, 28% anterior, 10% lateral position in 45° rotation. So while there is no change in a signifi cant proportion of patients in the localization of the IJV in relation to the CA, the anterior placement rate increasing the risk of CA puncture was signifi cantly higher in a position of rotation compared with the neutral position (P = 0.001). No signifi cant diff erence was found between procedure durations. Complications were recorded when observed. Conclusion In our study, it has been shown that anterior placement of the IJV in the neutral position is less but this has no advantage in avoiding arterial puncture. The smaller area of procedure in a neutral position can cause diffi culties in practice. However, the processing times between each head position were not diff erent. Nevertheless, further studies evaluating whether there are comparable complication rates and the same duration of procedure in emergency and trauma patients in whom head rotation cannot be possible are needed. In the retrospective control arm, 105 PICCs were reviewed for tip position. Optimal, adequate, and aberrant positions were found in 22 (21%), 49 (47%), and 34 (32%) respectively, in comparison with 17 (43%), 15 (38%), and eight (20%) in the intervention group (P <0.05 between both groups). In the control arm, 11 (10%) PICCs terminated outside the central venous system, whereas none failed to achieve central venous access in the intervention arm. Conclusion Using the standardized method described above, PICC tip positioning can be greatly improved. In our results, 100% of catheters placed using the standardized method allowed for central venous access. This pilot study paves the way for a larger, multicentric evaluation of the bedside installation method of PICC. References Introduction According to the European Society of Parenteral and Enteral Nutrition guidelines [1] , post-insertion chest X-ray (CXR) is not necessary if the location of the tip has been verifi ed during the procedure and if pleura-pulmonary damage has been ruled out by other methods. The aim of this study is to assess feasibility and safety of an echo-ECG-guided method of central venous catheter (CVC) insertion and to evaluate whether post-insertion CXR can be avoided. Methods We enrolled only patients admitted to our ICU and candidate to elective CVC insertion, who had a detectable P wave on surface ECG. Our insertion protocol included: preliminary ultrasound (US) scan of central veins and pleural space; US-guided puncture and US control of the correct direction of the guidewire; intracavitary ECG method for tip location (cavo-atrial junction (CAJ) = maximal P wave); and US scan of pleural space to rule out pneumothorax (PNX). Post-insertion CXR was performed in all patients to rule out PNX and verify tip location close to the CAJ (CAJ = 3 cm below the carina [2] ). Tip location between 1 and 5 cm below the carina -in the lower 1/3 of the superior vena cava (SVC) or in the higher 1/3 of the right atrium (RA) -was considered acceptable [1] . Introduction Chest auscultation and chest X-ray are commonly used to detect postoperative abnormalities and complications in patients admitted to intensive care after cardiac surgery [1, 2] . The aim of the study was to evaluate whether chest ultrasound represents an eff ective alternative to bedside chest X-ray to identify early postoperative abnormalities. Methods A total of 151 consecutive patients (103 male and 47 female) were studied by chest auscultation, ultrasound and X-ray upon admission to intensive care after cardiac surgery. Six pathologic entities were explored by each method: postero-lateral pleural eff usion and/ or alveolar consolidation (PLAPS), alveolar-interstitial syndrome (AIS), alveolar consolidation (AC), pneumothorax (PTX), pleural eff usion (PE), and pericardial eff usion with or without cardiac tamponade. Positions of the endotracheal tube and central venous catheter were also checked. Results Ninety-four of the 151 patients included (62%) showed abnormalities on chest X-ray (AC 9%, AIS 25%, PLAPS 42%, PE 3.3%, PTX 2%). Compared with chest X-ray, chest ultrasound had a sensitivity of 86% and a specifi city of 99% for AC, a sensitivity of 95% and a specifi city of 100% for AIS, a sensitivity of 97% and a specifi city of 98% for PLAPS, a sensitivity of 99% and a specifi city of 100% for PE, and a sensitivity and specifi city of 100% for PTX. Furthermore, chest ultrasound detected all pericardial eff usions while neither chest X-ray nor chest auscultation were able to identify them. Chest ultrasound identifi ed all cases of endotracheal tube (two patients) and central venous catheter (two patients). There was a highly signifi cant correlation between abnormalities detected by chest ultrasound and X-ray (k = 0.90), but a poor correlation between chest auscultation and X-ray abnormalities (k = 0.15). Conclusion Chest auscultation may help identify endotracheal misplacement and tension pneumothorax but it may miss most of major abnormalities. Chest ultrasound represents a valid alternative to chest X-ray to detect all postoperative abnormalities and misplacements. Introduction Assessment of volume status and responsiveness guides resuscitation strategy. Non-invasive techniques are desirable. Changes in carotid fl ow time have been proposed as a marker of volume status, but few data support their use. We sought to determine whether carotid fl ow time decreased in the volume-depleted state of acute blood loss, and whether volume-depleted individuals would demonstrate an increase in carotid fl ow time after a passive leg raise (PLR) maneuver. Methods Volunteers aged 18 to 55 presenting to the hospital's blood donor center for whole blood donation were eligible to participate. Individuals with a history of aortic or carotid artery disease, atrial fi brillation, or a contraindication to blood donation were excluded. Prior to blood donation, an investigator performed an ultrasound of the right common carotid artery with a high-frequency linear transducer, obtaining a Doppler tracing of carotid artery fl ow. Measurements of peak velocity, systole time, and carotid fl ow time were obtained. A PLR was performed for 30 seconds, followed by repeat measurements of carotid velocity and fl ow time. Whole blood was then collected according to the blood donor center's protocol. Immediately after blood donation, repeat measurements of carotid fl ow and velocity were obtained in the supine position and after a PLR. Carotid fl ow times corrected for heart rate (FTc) were analyzed with Student's t test. The institutional review board approved the study. Results Eighty donors were screened for participation by two investigators; 68 consented and completed donation (60.3% female, mean age 31). Donors had mean blood loss of 450 ml. The mean FTc supine after blood donation was 296 ms; this was signifi cantly diff erent from the FTc prior to donation (supine = 320 ms, PLR = 323 ms; P <0.0001). The mean FTc following blood donation and PLR was 321 ms, signifi cantly diff erent from the supine position after donation (P <0.0001), but not pre-donation measurements. Conclusion Ultrasound measurement of carotid fl ow time was signifi cantly decreased in the setting of acute blood loss. An autobolus by PLR after blood loss restored FTc to pre-donation levels. Further investigation of FTc as a non-invasive predictor of volume responsiveness is warranted. Introduction Central venous catheters play an important role in patient care; however, their use is associated with various complications and more frequently through the subclavian vein (SCV) route. A previous study showed that ultrasound-guided cannulation of the SCV in critical care patients is superior to the landmark method and should be the method of choice in these patients [1] . The aim of this study was to compare short-axis and long-axis approaches for ultrasound-guided subclavian vein cannulation with respect to indicators of success. Methods Eighty-three patients undergoing cardiac surgery and requiring central venous cannulation were randomized to receive longaxis or short-axis ultrasound-guided cannulation of the subclavian vein by a skilled anesthesiologist. First-pass success, unsuccessful placement, number of attempts, number of needle passes, skin and vessel puncture, time to successful catheterization and complications were considered as outcomes. The subclavian vein was successfully cannulated by ultrasoundguided techniques in all patients. Central venous cannulation failed in two and 10 cases respectively with short-axis and long-axis view and the other view was used successfully. The fi rst-pass success rate was signifi cantly higher in the short-axis group (73%) compared with the long-axis group (40%) (P = 0.005). The procedure time, number of attempts, needle redirection, and skin and vessel punctures were signifi cantly lower in the short-axis than long-axis group (P <0.05). The overall number of complications did not diff er signifi cantly between groups even if artery puncture and hematoma occurred more frequently in the long-axis group. Moreover, the need to change the ultrasoundguided insertion technique was more frequent in the long-axis group. Conclusion Ultrasound-guided subclavian vein cannulation by an experienced operator has a higher fi rst-pass success rate and lower access time using the short-axis than long-axis approach. Reference (Figure 1 ). The Bland-Altman plot showed 95% limits of agreement from -8.96 to +8.83% and mean diff erence (bias) of -0.07% ( Figure 2 ). Conclusion We may use %ΔSV measured by TTE after PLR to predict FR, which is noninvasive and less time-consuming than other invasive techniques. Introduction Echocardiography is commonly used during both venoarterial (V-A) and venovenous extracorporeal membrane oxygenation (ECMO). In many circumstances, transoesophageal echocardiography (TOE) is the preferred monitoring tool. It can aid in cannula positioning, especially during double-lumen cannula placement for V-V ECMO, weaning of V-A ECMO and diagnose causes of high-access pressures and circuit fl ow problems. We use TOE as our preferred monitoring equipment before, during and after establishing ECMO. We sought to investigate how often information gained from TOE imaging had a major impact on management decisions. Methods A single-centre observational study at a tertiary referral institution. All patients supported with V-A or V-V ECMO during an 18-month period were included. Routine procedures such as wire position checks during cannulation or information gained to assist weaning from V-A support were not included. Results Twenty patients were supported with either V-A (all peripheral) or V-V ECMO during the observation period. In 12 patients (60%) TOE was instrumental in diagnosing potentially fatal complications or altered clinical management. In three patients on V-A support, afterload reduction and modulation of inotropic support was necessary due to extensive spontaneous echo contrast formation in the left ventricle and stagnant pulmonary blood fl ow; two out of these three patients, immediately after establishing support, required intra-aortic balloon counterpulsation to reverse clot formation around the aortic valve and root. Introduction Right-sided precordial leads (V3R to V5R) and posterior chest leads (V7 to V9) provide important information for the right ventricle and posterior wall. These additional lead electrocardiograms (ECGs) improve diagnostic value in acute coronary syndrome patients [1] . However, these additional electrocardiograms are not routinely recorded due to the time-consuming procedure involved. Recently these synthesized six additional lead ECGs using the standard 12-lead ECG system (Nihonkoden Co. Ltd) have been developed [2, 3] . But the accuracy is not clear. The purpose of the present study was to evaluate the accuracy of synthesized ECGs at the ST part. Methods Standard 12-lead and actual V3R to V5R, V7 to V9 lead ECGs at Tokyo Medical University Hospital were successfully recorded and compared with synthesized ECGs at the J point, M point that was defi ned for the point after 1/16 RR interval and T wave amplitude. ECGs of the complete right branch block, complete left branch block and pacing rhythm were excluded. Results A total of 1,216 ECGs were correctly recorded. The diff erences of actual and synthesized at the J point, M point and T wave amplitude were very small. Means of the diff erence ± 2SD were V3R/V4R/V5R/V7/ V8/V9: J point, 17 ± 1/14 ± 1/13 ± 1/12 ± 1/15 ± 1/18 ± 1 μV; M point, 15 ± 1/13 ± 1/12 ± 1/12 ± 1/12 ± 1/13 ± 1 μV; and T wave amplitude, 20 ± 3/32 ± 2/16 ± 2/37 ± 2/39 ± 2/43 ± 3 μV. There were positive correlations between all actual and synthesized ECGs of J point, M point and T wave amplitude ( Figure 1 ). Conclusion The ST part of synthesized V3R to V5R and V7 to V9 lead ECGs appears to be highly reliable. Synthesized additional lead ECGs might be useful to diagnose ischemic heart disease. Introduction Acute and chronic systemic infl ammatory conditions are associated with aortic stiff ening. Carotid-femoral pulse wave velocity (PWV), a marker of aortic stiff ness, increases in patients with infl ammatory diseases and independently correlates to levels of C-reactive protein (CRP). The eff ects of massive infl ammatory response in early sepsis on mechanical properties of the aorta have not been investigated. The objective of the current study was to prospectively assess aortic stiff ness in patients with early severe sepsis and septic shock and relate it to infl ammatory and haemodynamic variables and outcome. Methods We recruited patients meeting criteria for severe sepsis and septic shock within 24 hours of admission to ICU. After haemodynamic stabilisation, PWV was recorded at inclusion and after 48 hours using dual-channel plethysmography. Severity of illness was assessed with APACHE II and serial SOFA scores, haemodynamic and infl ammatory parameters (CRP, procalcitonin and fi brinogen) recorded. A 28-day follow-up was performed to distinguish between survivors and nonsurvivors. Results Twenty consecutive general ICU patients (six with severe sepsis and 14 with septic shock) were enrolled in the study; median age 59 years (IQR 56.5 to 72), APACHE II score 17 (13 to 20.5), SOFA score 5 (IQR 4 to 9). At 28 days, six patients had died. Median initial PWV was 10.4 (IQR 6.9 to 12.1) m/second in patients with severe sepsis, and 6.8 (IQR 5.3 to 7.5) m/second in patients with septic shock (P = 0.13). After 48 hours, PWV in the severe sepsis and septic shock groups had become similar, 9.3 (IQR 7.3 to 11.1) m/second and 9.2 (IQR 7.8 to 13) m/second respectively (P = 0.96). PWV had signifi cantly increased in survivors (7.8 to 12.3 m/second) (P = 0.04) versus nonsurvivors (6 to 7.8 m/second) (P = 0.69). Higher PWV correlated with increasing systolic pressure and lower CRP levels (r = 0.73, P = 0.01). Conclusion In early sepsis, aortic stiff ness is decreased in patients with greater disease severity, and in survivors increases to median levels within 48 hours. The main factors associated with lower pulse wave velocity are lower systolic pressure and higher CRP levels. The association of high serum CRP levels with low aortic stiff ness in patients with sepsis does not match data described in the literature [1] . Reference  The correlation between the PA-S and the PiCCO system was (R 2 = 0.746) over a range of CO from 1 to 6 l/minute. The Bland-Altman analysis demonstrated a bias of -0.05 l/minute and precision of 0.50 l/ minute. Conclusion This animal study demonstrated the feasibility of the new PA-S for determination of indicator dilution CO in a limited range. The system showed good correlation with the PiCCO system even in the case of vasoconstriction as a result of blood loss. Since the PA-S utilizes transpulmonary indicator dilution, other variables such as intrathoracic blood volume, global end-diastolic volume and extravascular lung water can be calculated. Additionally, continuous CO can be obtained from the optical sensor signal utilizing arterial waveform analysis. References  We performed measurements at 57 points. There was a good correlation between the two devices regarding CO and GEDV, but VolumeView showed higher GEDV than PiCCO. Regarding EVLW, there was no signifi cant correlation between two systems. VolumeView showed signifi cantly higher EVLW than PiCCO ( Figure 1 and Table 1 ). Introduction Goal-directed therapy used in the perioperative period of patients undergoing cardiac surgery shortens the length of ICU stay [1] . We aimed to compare the postoperative results of the liberal and restrictive fl uid strategy used in patients undergoing pulmonary resection surgery (PRC). Methods We have been using the restrictive fl uid strategy since March 2013 in our institute. Patients who were on the liberal fl uid regime were analyzed retrospectively. From March 2013 until today, patients who were on restrictive fl uid strategy were analyzed prospectively. A total of 125 patients were included in the study. Age, duration of anesthesia, type of fl uids given intraoperatively, fl uid index (ml/kg/hour), fl uid intake/output balance, creatinine and lactate levels were compared with pulmonary and renal morbidity, and length of stay in ICU, using multivariate analysis. Results A signifi cant correlation (P <0.05) was established between the amount of crystalloid given intraoperatively, fl uid index and fl uid balance with pulmonary morbidity (n = 52). The fl uid index and inotropes usage were correlated with the postoperative creatinine levels (P <0.05). There was no correlation between perioperative lactate levels with fl uid balance and fl uid index. Intraoperative blood loss, the amount of given crystalloid, colloid, blood and FFP, fl uid balance, duration of anesthesia and postoperative blood transfusion were found to be related (P <0.05) with the length of ICU stay. Four percent of the patients required renal replacement therapy and the overall mortality was 0.8%. Conclusion To reduce the morbidity of patients undergoing major surgery, the protocols of using the restrictive fl uid strategy in the perioperative period and simultaneous protection of end organs, especially the kidneys, is currently the subject of this discussion. We observed that the restrictive fl uid strategy did not lead to global organ hypoperfusion, which was monitored by lactate. Even though there was a negative correlation between the fl uid index with creatinine levels and renal failure, the need for renal replacement therapy was observed only in one case. As a conclusion, the postoperative pulmonary morbidity and length of ICU stay can be reduced in patients who undergo PRC by using the restrictive fl uid strategy (4.2 ± 0.3 ml/kg/hour), without causing any vital organ dysfunction.  The number of complications tended to be lower in the intervention group (11 vs. 20). Eleven patients in the intervention group had no complication, versus seven in the control group. There was no signifi cant diff erence between groups in length of stay in ICU or in hospital. Administration resulted in a 5% increase of tissue oxygenation. The cardiac index increased 0.3 (0.0 to 0.6) l/minute/m 2 ( Figure 1 ). The overall protocol adherence was 94%. Conclusion Intraoperative optimisation of tissue oxygenation will potentially result in better outcome after high-risk abdominal surgery. The protocol used may be considered feasible for clinical practice. Introduction Passive leg raising (PLR) has been suggested as a simple diagnostic tool to guide fl uid administration in critically ill patients [1] . We included 21 patients in each group. There was no signifi cant diff erences in the fl uid balance between the control and study group after 24 hours (5.0 ± 2.9 l vs. 3.6 ± 2.7 l, P = 0.11) and 48 hours (5.7 ± 3.5 l vs. 4.8 ± 3.7 l, P = 0.39). However, compliance with the protocol was poor (56%). After 2/11 positive tests, fl uid was not administered; and after 21/39 tests, fl uid was administered despite a negative test result ( Figure 1 ). The mean age was 62.9 ± 15.8 years. Of the patients, 75% were male, 45% were Caucasian and 40% were African American. The 30-day mortality for the two groups were similar for the ATII cohort and the placebo cohort (50% vs. 60%, P = 1.00). ATII resulted in marked reduction in norepinephrine dosing in all patients. The mean norepinephrine dose for the placebo cohort was 20.1 ± 16.8 μg/ minute vs. 7.3 ± 11.9 μg/minute for the ATII cohort (P = 0.022). The most common adverse event was hypertension, which occurred in 20% of patients receiving ATII. Conclusion ATII is an eff ective vasopressor agent in patients with distributive shock requiring multiple vasopressors. The initial dose range of ATII that appears to be appropriate for patients with distributive shock is 2 to 10 ng/kg/minute. Further studies to assess the use of ATII in patients with distributive shock are warranted. Introduction Patients with septic shock die mainly due to refractory shock. Vasopressin is commonly used as an adjunct to catecholamines to support blood pressure in refractory septic shock, but its eff ect on mortality is unknown. We hypothesized that vasopressin as compared with norepinephrine would decrease mortality among cancer patients with septic shock. Methods In this, randomized, double-blind trial, we assigned patients who had cancer and septic shock and needed a vasopressor to receive norepinephrine or vasopressin in addition to open-label vasopressors. All vasopressor infusions were titrated and tapered according to protocols to maintain a target blood pressure. The primary endpoint was the mortality rate 28 days after the start of infusions. Results A total of 107 patients underwent randomization in this fi rst part of trial, and were infused with the study drug (53 patients received vasopressin, and 54 norepinephrine), and were included in the analysis. There was no signifi cant diff erence between the vasopressin and norepinephrine groups in the 28-day mortality rate (67.9 and 58.5%, respectively; P = 0.31). There were no signifi cant diff erences in the overall rates of serious adverse events (5.3% and 5.5%, respectively; P = 1.00). Conclusion Vasopressin did not reduce mortality rates as compared with norepinephrine among patients with cancer and septic shock who were treated with catecholamine vasopressors. Introduction Clinical study suggests that beta-blockers, by decreasing the heart rate (HR) together with an increase in stroke volume (SV), do not negatively aff ect cardiac output (CO), allowing an economization of cardiac work and oxygen consumption in patients with septic shock [1] . Whether this hemodynamic profi le leads to an amelioration of myocardial performance is still unclear. The objective of the present study was therefore to elucidate whether a reduction in HR with esmolol is associated with an improvement of cardiac effi ciency in patients with septic shock who remained tachycardic after hemodynamic optimization. Methods After 24 to 36 hours of initial hemodynamic stabilization, 24 septic shock patients with HR >95 bpm and requiring norepinephrine (NE) to maintain mean arterial pressure (MAP) between 65 and 75 mmHg Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 despite adequate volume resuscitation received a continuous esmolol infusion to maintain the HR between 94 and 80 bpm. NE was titrated to achieve a MAP between 65 and 75 mmHg. To investigate myocardial performance, we simultaneously assessed LV ejection fraction (LVEF), tricuspidal annular plane solid excursion (TAPSE) by echocardiography, the dP/dt MAX and the cardiac cycle effi ciency (CCE) both estimated from the arterial pressure waveform. Data were obtained at baseline and after achieving the predefi ned HR threshold (T1). Results For a MAP between 65 and 75 mmHg, esmolol administration signifi cantly decreased HR (115 ± 10 to 91 ± 7 bpm), NE (0.7 ± 0.4 to 0.5 ± 0.3 μg/kg/minute), and dP/dt MAX (1.1 ± 0.3 to 0.8 ± 0.3 ms/ mmHg). Conversely, TAPSE (15 ± 3 to 20 ± 3 mm), CCE (-0.2 ± 0.4 to -0.03 ± 0.4 units) and SV (37 ± 8 to 42 ± 10 ml) signifi cantly increased at the end of the study period (all P <0.05). CO (4.1 ± 0.8 to 3.9 ± 0.8 l/ minute) and LVEF (46 ± 10 to 48 ± 10%) did not change. Conclusion In patients with established septic shock who remained tachycardic after hemodynamic optimization, titration of esmolol to reduce the HR to a predefi ned threshold economized cardiac function, resulting in a maintained CO with a lower HR and a higher stroke volume. Such a hemodynamic profi le was characterized by an improved cardiac effi ciency, as indicated by the decrease in dP/dt MAX associated with an increase in CCE. Finally, echocardiographic data suggest that reducing HR with esmolol positively aff ects right ventricular function.  Of the 48 patients included on the study, 26 (54%) were males, and 31 (64%) were younger than 18 years old. These patients developed cardiogenic shock during 72 hours after cardiac surgery. In all cases, ECMO was inserted after cardiac surgery. Of all patients, 32 (66%) were central ECMO, inserted in the operative room, and 16 were percutaneous, inserted in the ICU. The median duration of ventricular assistance was 6 days (IQR 0 to 41), the length of ICU stay was 16 days (IQR 1 to 111), and hospital stay was 29 days (IQR 1 to 198). Twenty patients survived (41%) and were discharged from our hospital. Conclusion The use of mechanical circulatory assists devices is an effi cient tool to manage seriously ill patients after cardiac surgery. This tool should be considered early in the diagnosis of cardiogenic shock after cardiac surgery. Introduction Mortality from cardiogenic shock remains high [1] and, despite a physiological rationale, intra-aortic balloon counterpulsation (IABP) has recently been shown to be ineff ective in reducing mortality [2, 3] . Veno-arterial extracorporeal membrane oxygenation (V-A ECMO) may off er a survival advantage over IABP. The objective of this study was to describe the characteristics and outcomes of patients supported with IABP or Impella and to identify the characteristics of patients who die, despite mechanical assistance, for whom a proposed V-A ECMO programme may be benefi cial. Methods A retrospective cohort study in a 30-bed, medical-surgical ICU. All adult patients supported with IABP or Impella over 2 years to March 2013 were identifi ed and data were extracted by case-note review. Subgroup analysis was carried out for patients aged ≤65 and for those who fulfi lled the modifi ed Melbourne criteria for V-A ECMO [4] . Data collected included demographic data, physiology and organ support at baseline and at 6, 12, and 24 hours, ICU and hospital outcomes and cause of death. Comparisons between survivors and nonsurvivors were made with t test/chi-squared tests as appropriate. Results A total of 129 patients were identifi ed: 78% were male, mean age was 70 years (SD ±11.8), mean APACHE II score was 20 (±5) and ICU mortality was 44%. Comparing survivors with nonsurvivors the only statistically signifi cant diff erence was metabolic acidosis (-6.8 ± 5.3 vs. -10.9 ± 7.0 mEq/l; P <0.05). Heart rate, mean arterial pressure, lactate, central venous oxygen saturation, cardiac index, arterial blood pH and mechanical ventilation failed to show a signifi cant diff erence. Eleven of these patients would have fulfi lled the proposed criteria for V-A ECMO, with an ICU mortality of 36%. Conclusion Only metabolic acidosis was associated with mortality in patients supported with mechanical assist devices. Our data do not allow discrimination of survivors from nonsurvivors. Patients who fulfi lled the proposed criteria for V-A ECMO showed a similar mortality to a recent series treated with V-A ECMO [4] . The proposed criteria do not identify a cohort, in this population, that would expect a mortality benefi t from V-A ECMO. The lactate group received more fl uids and dobutamine. However, there were no signifi cant diff erences in lactate levels between the groups. The rate of complications was similar between groups (11% vs. 7%, P = 0.087). Length of ICU stay was higher in the lactate group (3.5 vs. 2.4 days, P = 0.047) when compared with the control group. Conclusion In patients with hyperlactatemia on ICU admission, lactate-guided therapy did not reduce complications and was related to a longer ICU length of stay. This study suggests that goal-directed therapy aiming to decrease initial lactate levels does not result in clinical benefi t. Introduction Blood lactate clearance, a surrogate of tissue hypoxia, is associated with increased mortality in septic patients. However, no study has directly measured lactate clearance at the tissue level in the post-resuscitation period of sepsis. This study aimed to examine the relative kinetics of blood and tissue lactate clearances and to investigate whether these are associated with outcome in ICU patients having severe sepsis or septic shock during the post-resuscitation phase. The contractile responses to electrical fi eld stimulation were abolished by tetrodotoxin, guanethidine and prazosin, indicating that the contractile eff ect is due to the action of noradrenaline on alpha adrenoreceptors. Ranolazine diminished (P <0.05) neurogenic adrenergic contractions induced by electrical fi eld stimulation in aortic rings with and without endothelium. Ranolazine produced concentration-dependent relaxation in rings precontracted with noradrenaline (Emax 86 ± 6%, n = 10; P <0.05) but not in rings precontracted with endothelin-1, thromboxane A2 and KCl. Neither L-NAME (10 -4 M), an inhibitor of nitric oxide synthase, nor indomethacin (10 -5 M), an inhibitor of cyclooxygenase, modifi ed the relaxation induced by ranolazine. The calcium antagonist nifedipine (10 -6 M) reduced the relaxation induced by ranolazine. Conclusion These results indicate that ranolazine diminished the contractile response induced by adrenergic stimulation, suggesting an eff ect as an adrenergic blocker. The relaxant eff ects of ranolazine on rat aortic vessels is not dependent on the endothelium-derived factors (nitric oxide or dilator prostanoids) but involves an interference with the entry of calcium through dihydropyridine calcium channels. Introduction Early extubation post coronary artery bypass grafting does not increase perioperative morbidity and reduces the length of stay (LOS) in the ICU and in hospital [1] . Use of low-dose opioidbased general anaesthesia and time-directed protocols for fasttrack interventions does not increase mortality or postoperative complications in low-moderate-risk patients and has been found to have a reduced time to extubation and shortened ICU stay [2] . Our mean time to extubation is 6 hours, although patients are assessed to be safe to be weaned from mechanical ventilation at 2 hours following arrival in the ICU. This study aims to identify factors that delay extubation in patients undergoing routine cardiac surgery at our institution. Methods A prospective analysis was performed on all patients post adult cardiac surgery from 14 May 2013 to 10 July 2013. Emergency surgical patients and those with intraoperative complications were excluded. Results A two-sample t test was used to analyse the data. Patient demographics are presented in Table 1 . There were signifi cant delays in time of extubation in those who received morphine prior to extubation compared with those that did not (P = 0.0184) ( Table 2 ). There were no signifi cant diff erences in LOS in ICU or hospital. Factors such as age, EUROSCORE and type of operation did not have an infl uence on time to extubation. Conclusion Administering morphine prior to extubation causes signifi cant delays in weaning from mechanical ventilation. We plan to introduce intraoperative and postoperative protocols to facilitate rapid weaning from mechanical ventilation for elective cardiac surgical patients. The SI did not correlate with MaR (P = 0.2720, R 2 = 0.09191). The population was divided into the lowest half of the SI (representing the most hostile milieu; SI: 23 to 57%) and the upper half of the SI (representing the friendliest post-infarction milieu; SI: 71 to 95%). The patients' profi le of adaptive infl ammatory response was characterized by fl ow cytometry. The two groups did not diff er with regard to their T-regulatory response (CD25 + FoxP3 + , P = 0.7203) or NK-cell (CD3 -CD56 + , Introduction Elevated cardiac troponin levels are common in ICU patients even in the absence of acute coronary syndromes and may be predictive of mortality. The recently introduced high-sensitivity cardiac troponin T (HS cTnT) assay has resulted in an increased detection of elevated cTnT in ICU patients [1] . The aim of this study was to determine the prevalence of elevated cTnT using the HS assay and its relationship with mortality. Methods A retrospective observational study was performed on all ICU admissions over a 12-month period. Data were obtained from the clinical information system (ICIP; Philips) and the ICU audit databases (AcuBase). Data collected included patient demographics, peak cTnT value, APACHE II score, requirement for organ support and mortality. The primary outcome measure was hospital mortality. Data were analysed using SPSS v.17.0. cTnT levels were divided into categories for analysis: normal (<14 ng/l) and elevated. The elevated category was further subdivided into quartiles. Univariate analysis was performed between potential risk factors and mortality followed by multivariate regression analysis to ascertain independent predictors of mortality. Results There were 417 admissions to the ICU during the study period, 89 of whom were excluded because of an absent cTnT value, leaving 328 patients included in the analysis. cTnT was elevated in 85% of patients. ICU mortality was 19% and hospital mortality was 28%. Hospital mortality (%) per cTnT category was: <14 ng/l = 2%; 14 to 38 ng/l = 19%; 39 to 90 ng/l = 26%; 91 to 252 ng/l = 39%; >252 ng/l = 43%. On univariate analysis, cTnT levels, age, ventilation and APACHE II score were signifi cantly associated with mortality. cTnT levels were signifi cant in multivariate regression independent of age and ventilation but did not reach signifi cance (P = 0.06) in a multivariate analysis that included the APACHE II score. Conclusion In 85% of general ICU patients, troponin measured by HS cTnT assay was elevated. cTnT levels were signifi cantly associated with mortality and are predictive of mortality independent of age and mechanical ventilation, but not independently of APACHE II score. There was a high correlation between troponin levels and APACHE II scores. Reference  Introduction The aim of this study was to clarify the risk factors for acute renal impairment (ARI) in patients with severe acute pancreatitis (SAP Introduction Controversy surrounds the empirical use of antibiotics in severe acute pancreatitis (SAP). There are concerns that the widespread use of antibiotic therapy in the absence of documented infection may lead to selection of drug-resistant organisms [1] . The aim of this study was to review the profi le of pancreatic fl uid isolates in patients with SAP admitted to the ICU. Methods Data were reviewed for 38 patients admitted to the ICU over a 5-year period. We evaluated organisms cultured from pancreatic specimens, as well as the prevalence of drug-resistant organisms in this group of patients. Results Aspirate of pancreatic material for culture was obtained in 55% of patients (n = 21). The mean time to acquisition of samples for culture from admission to ICU was 15.5 days. Fluid was sterile in 67% (n = 14) of initial samples. Gram-positive organisms were cultured from 43% (n = 9) of samples, Gram-negative organisms from 5% (n = 1) and yeasts from 5% (n = 1). Antibiotic therapy was administered in 95% of patients prior to samples being obtained for culture. On review of all samples received from patients (including nonpancreatic specimens), vancomycin-resistant enterococci (VRE) were isolated in 13 patients. Linezolid-resistant enterococci (LRE) were isolated in six patients, fi ve of whom had VRE isolated prior to the culture of LRE. Extendedspectrum beta-lactamase organisms were isolated in two patients, and carbapenem nonsusceptible Gram-negative organisms in three patients. The mean APACHE II score was 18.5 and overall hospital mortality was 26%. Conclusion In the majority of patients, initial aspirates of pancreatic material were sterile. This may be a result of prior antibiotic usage. Where organisms were cultured from initial aspirates, Gram-positive organisms predominated, possibly as a result of prior anti-Gramnegative antibiotic use. Therefore, in patients with ongoing sepsis who are receiving broad-spectrum antibiotic therapy, consideration needs to be given to the empiric treatment of Gram-positive infection, and in particular drug-resistant organisms such as VRE. Local epidemiology should be taken into account. Rationale use of antibiotics, in accordance with best-practice guidelines, may limit development of drug resistance; however, other risk factors for resistance may exist in this group and this would need to be further evaluated. Reference Introduction Liver transplantation (LT) in acetaminophen-induced acute liver failure (APAP-ALF) patients often presents signifi cant challenges. The King's College Criteria (KCC) have been validated on admission but not in later phases of illness. The aim was to improve determinations of prognosis on and after admission in APAP-ALF patients using the classifi cation and regression tree (CART) methodology to construct optimal binary splits on independent variables to predict outcome. Methods CART models were applied to US ALFSG registry data for prediction of 21-day spontaneous survival on admission and late stage (days 3 to 7). Analyses were carried out using R software (package rpart) for all (n = 803) APAP-ALF patients enrolled between January 1998 and September 2013 with complete outcome data. Training data were used to build CART trees and test data were used to evaluate prediction accuracy (AC), sensitivity (Sn) and specifi city (Sp). Introduction Extracorporeal membrane oxygenation (ECMO) is increasingly used for the treatment of refractory but potentially reversible respiratory and/or cardiac failure. Data on perioperative support with veno-arterial (V-A) and veno-venous (V-V) ECMO for adult liver transplant recipients are scarce [1, 2] . We report our experience of ECMO support in patients with acute liver failure (ALF) as a bridge to transplant and postoperative ECMO use following complications after surgery. Methods A retrospective study in a specialist tertiary referral ICU. Patients supported with V-V or V-A ECMO before, during, or after orthotopic liver transplant (OLT) were identifi ed. Results In total, four patients were supported during a 12-month period. Two patients required V-V and two patients V-A support. Two patients with ALF were bridged to OLT, one patient V-V ECMO for refractory respiratory failure and the other patient required emergency V-A support for treatment of intraoperative arrest. Both patients were successfully transplanted but died subsequently on ECMO: disseminated aspergillosis and haemophagocytic syndrome, and anoxic brain injury respectively. Two patients received postoperative ECMO support. The fi rst was treated with V-V ECMO for refractory persistent hypoxaemia following OLT for hepato-pulmonary syndrome, the second received emergency V-A support (eCPR) following cardiac tamponade and arrest on postoperative day 2. Both patients made a full recovery. Conclusion Emergency ECMO support before and after liver transplant is feasible. Despite the poor outcome in patients with ALF, we consider ECMO a valuable option to bridge selected patients to transplant. References frequently observed and is termed post-reperfusion syndrome. Recent studies showed the existence of a specifi c heart disease associated with cirrhosis termed cirrhotic cardiomyopathy (CCM). The aim of this study was to investigate whether the CCM has an infl uence on the development or the severity of post-reperfusion syndrome. Methods Fifty-two consecutive liver transplant patients were included in a retrospective observational study. The variables recorded were: age, etiology of the liver disease, MELD and MELD Na scores, the associated pathologies, the length of the QT interval, and plasma levels of brain natriuretic peptide (BNP). The patients with known renal or heart disease and the recipients of organs from extended criteria donors were excluded from the study. The QT interval was corrected for the heart rate using Bazett's formula (QTc). Statistical analysis was performed using SPSS Statistics v.19.1. Results In our study the criteria used to defi ne post-reperfusion syndrome relied on the hemodynamic changes that occurred at reperfusion. Preoperative echocardiography showed normal systolic and diastolic function at rest in all of the patients. For the identifi cation of patients at risk for CCM we used two of the supportive criteria from the recent defi nition of CCM: prolonged QTc interval and increased BNP levels. The study group included 28 men (53.8%) and 24 women. Mean (± SD) age was 50.5 (± 11.4). Mean MELD and MELD Na scores were respectively 15.51 (± 5.43) and 18.9(± 6.22). The value of BNP correlated well with the length of the QTc interval (P = 0.005), and with MELD and MELD Na scores (P = 0.025 and P = 0.001). In our study, post-reperfusion syndrome occurred in 63.4% of the patients. We could not fi nd a correlation between post-reperfusion syndrome and the BNP levels (P = 0.85) or the prolonged QTc interval (P = 0.38). The post-reperfusion syndrome did not correlate with the severity of the liver disease as assessed by MELD and MELD Na scores. The severity of post-reperfusion syndrome did not correlate with QTc prolongation or BNP levels. Conclusion Reperfusion is a critical time during liver transplantation. The clinical predictors of post-reperfusion syndrome are still under debate [1] . Our study showed that the post-reperfusion syndrome is not correlated with the severity of the liver disease or with the presence of risk factors indicating CCM. Introduction Combined heart-liver transplantation (CHLT) is an uncommonly performed procedure for patients with coexisting cardiac and liver disease [1] . The purpose of this study was to examine and describe the perioperative management of patients undergoing CHLT. Methods A retrospective review was performed of patients undergoing CHLT at our institution from 1999 to 2013. Results Twenty-seven CHLTs were performed, with 4/27 including simultaneous kidney transplantation. Familial amyloidosis was the indication for 21 CHLTs (78%), and 12 of these explanted livers were used for domino transplantations. Nineteen patients (70%) were receiving inotropic infusions at the time of organ availability. The median preoperative MELD score was 12, and elevations in preoperative international normalized ratio were due to warfarin in all but one patient. Liver transplantation immediately preceded cardiac transplantation in 2/27 cases to reduce high-titer donor-specifi c antibodies. Venovenous bypass was utilized in 14 operations (52%) performed with the caval interposition liver transplantation approach, cardiopulmonary bypass during liver transplantation in two cases (7%), and no bypass in 11 operations (41%) performed with a caval sparing (piggyback) surgical technique. Postoperatively, the median duration of mechanical ventilation, ICU stay, and hospital stay until discharge were 1 day, 5.5 days, and 15 days, respectively. Transfusions in the fi rst 48 hours following CHLT were not substantial in the majority of patients. One patient died within 30 days of CHLT. Conclusion CHLT is a life-saving operation that is performed with relatively low mortality and can be successfully performed in select patients with congenital heart disease. Patients undergoing CHLT at our institution had relatively preserved hepatic function but limited cardiac function often requiring inotropic support. Cardiac transplantation typically precedes liver transplantation during CHLT given the decreased ischemic tolerance of the cardiac graft [2] . However, liver transplantation prior to cardiac transplantation may serve to mitigate high-titer donor-specifi c antibodies. Various aforementioned operative approaches may be successfully utilized for the liver transplantation portion of these procedures. We attribute the favorable outcomes and perioperative courses to the multidisciplinary approach to care that CHLT patients receive at our institution. References Introduction The balance between coagulation and fi brinolysis was a prominent factor in the pathophysiology of sepsis, but this mechanism has been poorly understood. We aimed to determine whether collapsing this balance during the fi rst day of sepsis correlates with progression of organ dysfunction and subsequent death. Methods This study included all patients with sepsis admitted to a tertiary referral hospital in Japan. Global coagulation tests and hemostatic molecular markers such as fi brin/fi brinogen degradation products (FDP), D-dimer, thrombin-antithrombin complex (TAT) and plasmin-alpha 2 plasmin inhibitor complex (PIC) were measured within 12 hours after admission, and then SOFA and APACHE II scores and in-hospital mortality were evaluated. Patients were divided into three groups based on the levels of TAT/PIC and FDP/D-dimer and diff erences of clinical outcome between groups were assessed by chi-square analysis and ANOVA. Results We enrolled 101 patients; 87 patients survived, and 14 died. Mortality was signifi cantly higher in the high TAT/PIC group (0%, 19% and 24% for low, middle and high TAT/PIC groups, respectively; P = 0.011). In addition, SOFA and APACHE II scores were signifi cantly higher in the low FDP/D-dimer group (APACHE II = 22.3, 18.9 and 15.3; P <0.01, SOFA = 8.6, 6.5 and 5.1; P <0.01, for low, middle and high FDP/ D-dimer groups, respectively). See Figure 1 . Conclusion We demonstrated that the balance between coagulation and fi brinolysis, assessed with FDP/D-dimer and TAT/PIC ratios, was correlated with disease severity and clinical outcomes in sepsis, suggesting that impaired balance of the hemostatic system might play a pivotal role in progression of sepsis pathophysiology. Introduction Hemodynamic disorders in critically ill patients are often connected with bacterial load. Bacterial load is usually associated with bacteremia, LPS, high level of IL-6, PCT and also with aromatic microbial metabolites [1] [2] [3] , and so forth. In our opinion, microbial metabolites can participate in hemodynamic disorders in critically ill patients, particularly due to their infl uence on NO production [4] and intestinal permeability. Methods In a prospective study we observed critically ill patients on the day of admission to a polyvalent ICU, severe cardiac pathology was excluded. The level of phenylpropionic, phenyllactic, p-OHphenyllactic, p-OH-phenylacetic acids and total phenylcarboxylic acids (PhCAs) were measured in blood serum using gas chromatography (GC-FID). The level of PCT and NT-proBNP were measured using Elecsys 2010. Comparison between patients with hypotension (on vasopressor support) (group A) and without (group B) was performed. We studied 50 ICU patients with diff erent diseases: pneumonia (n = 15), severe kidney failure (n = 13), abdominal surgical pathology (n = 10), alcoholic cirrhosis (n = 5), soft-tissue infection (n = 7). In group A (24/50) the median of PhCAs was 17.8 (IR 11.4 to 30.0) μmol/l, and in group B (26/50) it was 7.2 (IR 3.7 to 13.2) μmol/l, P = 0.003 (t test). In group A, all patients (with or without documented infections) had symptoms of infection manifestation [5] , 20/24 (83.3%) of them died. In group B, the symptoms of infection manifestation were revealed in 12/26 (46%) cases, and the mortality was signifi cantly lower, 3/26 (11.5%) (P <0.05). General mortality was 23/50 (46%). The profi le of PhCAs diff ered in groups A versus B. Conclusion The total level of PhCAs in critically ill patients with hypotension was considerably higher than in hemodynamically stable patients. The participation of microbial factor in pathogenesis of hemodynamic disorders in the presence of systemic infl ammation may be validated with the load of microbial metabolites (PhCAs). Introduction Coagulation abnormalities are common in severe sepsis or septic shock [1] . Methods A prospective observational cohort study of 100 patients above 18 years of age diagnosed with severe sepsis or septic shock on admission. The fi rst blood sample collected on admission was analyzed. Data were collected through a predesigned pro forma. Those with previous history of any coagulation disorders were excluded. Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 Results Univariate analysis showed signifi cant correlation of APACHE II, platelet, PT, aPTT, fi brinogen and D-dimer with mortality in patients with severe sepsis or septic shock. Multivariate analysis showed APACHE II >20 (P = 0.001), fi brinogen <2 (P = 0.019) and D-dimer >1(P = 0.06) are independent predictors of mortality in severe sepsis or septic shock. See Table 1 . Introduction Systemic infl ammation caused by infection or trauma often leads to adverse outcome in critically ill patients. Binding of ligands to the receptor for advanced glycation end products (RAGE) activates several pathways, including the nuclear factor-kappa B pathway, which generates infl ammatory cytokines, proteases and oxidative stress. RAGE activation has been suggested to link amplifi cation and perpetuation of infl ammation to subsequent organ damage and adverse outcome in sepsis, acute lung injury and myocardial dysfunction [1] . The soluble receptor, sRAGE, is thought to act as a decoy, thus protecting against further RAGE activation. High mobility group box 1 (HMGB1) is a nuclear protein that is released during cellular stress and damage. S100A12 is a neutrophil-derived protein that acts as a proinfl ammatory danger signal. Both are ligands for RAGE. We hypothesized that excessive RAGE activation is linked to adverse outcome in critically ill patients and that a diff erent pattern of RAGE activation and infl ammation may be present in patients according to underlying pathology. Methods We measured sRAGE, HMGB1 and S100A12 serum levels upon admission, day 7 and the last day in the ICU in 405 critically ill surgical patients who needed intensive care for at least 7 days and in 69 matched healthy controls. We assessed the relation of these levels with clinical complications and outcome, in comparison with C-reactive protein (CRP) as a routinely measured clinical parameter of infl ammation. Results Upon ICU admission, levels of sRAGE, HMGB1, S100A12 and CRP were higher as compared with healthy levels. HMGB1, S100A12 and CRP remained elevated throughout the ICU stay but sRAGE decreased to levels lower than in healthy volunteers by day 7. sRAGE and CRP showed distinct time profi les during the ICU stay in patients undergoing cardiac versus other surgery and in patients with versus without sepsis upon admission. Elevated sRAGE upon admission, unlike CRP, was associated with need for renal replacement therapy, liver dysfunction, circulatory failure and mortality. Except for mortality, these associations remained in multivariate logistic regression analysis correcting for baseline risk factors. Conclusion Critical illness alters several components of the RAGE axis. Elevated sRAGE levels upon admission to the ICU were associated with adverse outcome, independent of baseline pathology. Reference the endotoxin activity assay (EAA), a newly developed rapid assay of endotoxin. Blood endotoxin levels (EA levels) of 314 patients admitted to our university hospital ICU were measured within 24 hours from admission, and their correlation with disease severity and outcome was examined. Methods The study is a single-center retrospective analysis of critically ill patients admitted to our university hospital ICU from November 2006 to March 2012. All patients whose EA and procalcitonin levels were measured and severity criteria of disease recorded were enrolled. A total of 314 patients were analyzed. Results The mean ± SD of all ICU-admitted patients (n = 314) was 0.39 ± 0.25, and that of healthy volunteers (n = 61) was 0.10 ± 0.09. The mean APACHE II score at admission increased in parallel with increased EA levels. ). There were no signifi cant diff erences regarding baseline demographic and clinical data apart from blood pressures, which were lower in sepsis group. Serum leptin on day 2 only was higher in the sepsis group (44.2 ± 17.7 μg/l vs. 31.1 ± 2.1 μg/l, P = 0.008) with no diff erence on days 0 and 4 of admission. We detected a serum leptin level of 38.05 μg/l on day 2 to be 93% sensitive and 100% specifi c to diagnose sepsis. The three serum CRP levels were higher in the sepsis group compared with the SIRS group (61.2 ± 9 mg/l vs. 48.9 ± 7.1 mg/l, P <0.001 on day 0, 71.5 ± 9.6 mg/l and 196.8 ± 39.8 mg/l in the sepsis group vs. 56.9 ± 8 mg/l and 73.7 ± 32.5 mg/l in the SIRS group for days 2 and 4 respectively, P <0.001 for both). We found a CRP of 67.5 mg/l on day 2 having 87% sensitivity and 93% specifi city for the diagnosis of sepsis. Conclusion We concluded that although serum leptin may not be benefi cial in early diff erentiation between sepsis and non-infectious SIRS on admission, it may be highly specifi c on the second day. Figure 1 ). Plasma CCP on day 0 had a good capacity for the diagnosis of PA NP: CCP on day 0 ≤17.5 ng/ml yielded a sensitivity of 86.5% and specifi city of 66.7% (AUC 0.74; 95% CI 0.630 to 0.829; P = 0.0001; Figure 2) . Conclusion Plasma CCP level ≤17.5 ng/ml is a sensitive and specifi c candidate diagnostic biomarker of PA NP. We analyzed 150 consecutive episodes of severe sepsis (16%) or septic shock (84%) admitted to the UCI. The median age of the study sample was 64 (interquartile range (IQR): 22.3 years; male: 60%). The main sources of infection were: respiratory tract 38%, intra-abdomen Introduction Flow-cytometric analysis is still restricted to cancer and immunocompromised patients. There are no clinical studies that evaluate the immunological changes in traumatic brain injury (TBI) patients. The objective of this study is to determine whether patients with severe TBI (GCS <9) manifest early (<48 hours after injury) signs of immunosuppression and whether this condition increases the incidence of infection during the ICU stay. Of the 83 ICUs that were contacted, 69 (83%) responded to the survey. Only 7% of ICUs still perform daily routine CXRs for all patients while 65% of ICUs say never to perform CXRs on a routine basis. A daily meeting with a radiologist is established in the majority of ICUs and is judged to be important or even essential. The therapeutic effi cacy of routine CXRs was assumed by intensivists to be lower than 10% or to be between 10 and 20%. The effi cacy of on-demand CXRs was assumed to be between 10 and 60%. There is consensus between intensivists to perform a routine CXR after endotrachial intubation, chest tube placement or central venous catheterization. Conclusion The strategy of daily routine CXRs for critically ill patients has developed from a common practice (63%) in 2006 [2] to a rare practice (7%) nowadays. Intensivists still assume the value of routine CXRs to be higher than the effi cacy that is reported in the literature. This might be due to the clinical value of negative fi ndings, which has not been studied before. Of 50 patients we reviewed, 43 (86%) were intubated in the ED, 45 had SC ordered (only eight (18%) by ED physicians), and 37 (74%) had SC collected. There was no diff erence in age, gender or severity of illness as measured by APACHE score between the two groups. ICU mortality was lower in the SC collected group (24% vs. 69%, P = 0.007), as was hospital mortality (30% vs. 77%, P = 0.007) and antibiotics were de-escalated more often (89% vs. 8%, P <0.001). Patient with SC collected showed a trend toward signifi cantly more ventilator-free days (6.5 vs. 0, P = 0.053). Conclusion Sputum cultures were rarely ordered by ED physicians, and when not obtained in intubated patients with pneumonia, ICU and hospital mortality was higher, there was less antibiotic de-escalation, and a trend toward fewer ventilator-free days. Eff orts to improve collection of sputum cultures in these patients are warranted. The mean age of the study group was 73 ± 10 years; 11 of them were female and mean APACHE II score was 19 ± 6. RSBI did not diff er signifi cantly between spontaneous mode and other combinations, but the best correlation with spontaneous mode was found with PS:5 PEEP:0 (P = 0.0001, r = 0.719), and the worst with PS:0 PEEP:5 combination. RSBI calculated in each combination showed no predictive value for weaning success. Respiration rate (f) was higher in the SBT failure group than the SBT success group. When measured at PS:0 PEEP:5 and PS:5 PEEP:0 combinations, the threshold value of f was found to be 27/minute (P = 0.03). Conclusion Although there was a correlation between RSBI measured in the T-tube and RSBI measured in diff erent mode and pressure combinations, especially with the combination of PS:5 PEEP:0, a threshold value for RSBI cannot be detected during MV to predict SBT success. showed that noninvasive neurally adjusted ventilatory assist (nNAVA) improves patient-ventilator interaction and synchrony. More recently we described a new setting for nNAVA (nNAVA15) able to reduce the peak of electrical activity of the diaphragm (EAdipeak) and dyspnea (assessed by a visual analogue scale, VASd), compared with both nPSV and nNAVA, in patients undergoing NIV through a helmet, by improving the rate pressurization. We therefore designed a physiological study to evaluate and compare the eff ects of nNAVA15 with nPSV and nNAVA on VASd, EAdipeak, pressurization rate and arterial blood gases (ABGs). Methods Fourteen patients undergoing noninvasive ventilation because of acute respiratory failure underwent three randomized 30-minute trials: nPSV (inspiratory support above positive endexpiratory pressure (PEEP) ≥8 cmH 2 O, fastest rate of pressurization); nNAVA (NAVA level to achieve a comparable EAdipeak as during nPSV); nNAVA15 (NAVA level at 15 cmH 2 O/μV and the maximum inspiratory airway pressure (Paw) set at the value corresponding to PEEP + inspiratory support during nPSV). Oxygen inspiratory fraction and PEEP remained unmodifi ed throughout the study period. The last minute of each trial was analyzed. Paw-time products of the initial 200 ms from the onset of ventilator pressurization (PTP200), of the initial 300 and 500 ms from the onset of the EAdi swing indexed to the ideal PTP (PTP300i and PTP500i, respectively), and of the triggering area (PTPt) were computed. ABGs and VASd were assessed at the end of each trial. Conclusion Compared with nPSV and nNAVA, nNAVA15 through a mask reduces VASd, assuring an optimal pressurization rate and triggering performance, without aff ecting the breathing pattern, neural drive and ABGs. Introduction Neurally adjusted ventilatory assist (NAVA) has so far been used in minimally sedated intensive care patients. NAVA has not been applied in patients in the operation room. The eff ect of diff erent sedatives/anesthetics on the electrical activity of the diaphragm (Edi) has not so far been studied. The aim of our study was to compare the eff ect of sevofl urane and propofol on the Edi signal and breathing pattern during sedation and anesthesia and also in combination with remifentanil. Methods A randomized cross-over study comparing sevofl urane and propofol sedation and anesthesia in 10 juvenile pigs. Remifentanil 0.1 μg/kg/minute was added after a period of anesthetic agent administration. The animals were ventilated with NAVA with fi xed level throughout the study. Respiratory variables were measured for the last 5 minutes of each 15-minute exposure. The Edi signal and spontaneous breathing were preserved with both anesthetics. The breathing variability, expressed as the coeffi cient of variation (SD/mean) of the tidal volume (CVvt), was high with both drugs. CVvt was greater during with propofol than with sevofl urane (CVvt 32 vs. 18% during sedation and CVvt 23 vs. 14% during anesthesia). The frequency of sighs was higher with propofol both during sedation (29 vs. 12 sighs/hour) and anesthesia (21 vs. 1 sighs/hour) than with sevofl urane. Conclusion NAVA can be applied during propofol and sevofl urane anesthesia in pigs, with well-preserved Edi and spontaneous breathing. The natural variability is maintained with NAVA even during anesthesia. In contrast to sevofl urane, propofol sedation and anesthesia is associated with a high frequency of sighs and post-sigh apneas, probably due to a centrally induced mechanism. Our data warrant studies of NAVA in humans undergoing anesthesia and surgery when neuromuscular blockade is not required. In both groups, analysis of the alveolar area revealed that alveolar size at the second peak of the manoeuvre did not diff er signifi cantly compared with the fi rst peak (100.97% in ZEEP group, 102.37% in the PEEP group). During the plateau phases there was a slight increase in alveolar size at higher plateau pressures (slope of linear regression at plateau 4 mbar: 0.1 %/500 ms for ZEEP group, 0.18%/500 ms for PEEP group; at plateau 8 mbar: 0.42%/500 ms for ZEEP group, 0.565%/500 ms for PEEP group). After the manoeuvres, compliance increased to 137.73% in the ZEEP group and 119.91% in the PEEP group. Conclusion In the healthy lung, once recruited, alveoli stay stable in size over wide pressure ranges. Further recruitment manoeuvres do not lead to further increase of alveolar size, but increase of compliance. During plateau phases, alveolar size increases dependent on pressure. This leads to the assumption that recruitment is not only pressure dependent but also time dependent. References Introduction Lung-protective mechanical ventilation requires positive end-expiratory pressure (PEEP) and tidal volume (VT) to be chosen with regard to the individual state of the lung. The shape of the intratidal compliance-volume profi les might refl ect the state of the lung (atelectatic, open, overdistended) and could therefore be classifi ed into shape categories that translate into PEEP suggestions [1] . Intratidal resistance-volume profi les might refl ect intratidal opening and collapse of the lung [2] . Using respiratory data from an animal study we suggest a classifi cation into resistance shape categories based on the slope of the R(V) profi les. Methods Fifteen pigs with lavage-induced lung damage were ventilated at two PEEP levels (0 and 12 mbar) and three tidal volumes (8, 12 and 16 ml/kg bodyweight). Compliance (C(V)) and resistance (R(V)) profi les for each individual animal and ventilation setting were calculated from respiratory data using the gliding-SLICE method [3] . C(V) profi les were associated with one of the six suggested shape categories. The dependency of the mean R(V) slope of all animals on the ventilation setting was used as a basis for classifi cation into resistance shape categories. Resistance shape categories were compared with compliance shape categories for each individual animal to test whether similar PEEP suggestions result from both methods. Results Small PEEP and VT were typically associated with increasing C(V), and decreasing C(V) corresponds to large PEEP and VT. A classifi cation of each C(V) profi le into one of six compliance shape categories was possible. The shapes of the R(V) profi les of individual animals were remarkably similar. The R(V) slope was typically largest for a PEEP and VT setting at which derecruitment was likely and smallest where overdistension was likely. Based on this a classifi cation scheme was defi ned: 10 <slope <21 mbar s/L2 (category 1, 'increase PEEP'), slope ≥21 mbar s/L2 (category 2, 'keep PEEP') and slope ≤10 mbar s/L2 (category 3, 'decrease PEEP'). Comparison of resistance and compliance shape categories for single animals shows a good correlation. Conclusion Resistance shape categories might provide additional guidance for PEEP setting. Combining compliance and resistance shape categories could improve lung-protective ventilation. Introduction A protective ventilatory strategy should prevent VILI, but in patients with larger nonaerated areas hyperinfl ation may occur during tidal ventilation even during a protective ventilatory strategy [1] . The gliding sign is used as a marker of pneumothorax and, in a study [2] , to quantify preoperatively the degree of pleural adhesion in thoracic surgery patients. In our study we measured the variations of gliding (G) and static compliance ( Introduction Bacterial pneumonia is a common indication for mechanical ventilation in the ICU. Ventilation with high positive endexpiratory pressure (PEEP) and low tidal volume (VT) is recommended in patients with adult respiratory distress syndrome. This improves clinical outcome compared with ventilation with low PEEP and medium-high VT [1] . However, the eff ect of VT and PEEP on bacterial growth in lung tissue is not known. This study contrasted the eff ect of a protective ventilator protocol with a standard medium-high VT and lower PEEP protocol on lung bacterial growth, lung edema formation and lung injury. It was performed in a porcine model of intensive care with the frequently found pathogen Pseudomonas aeruginosa. Methods Sixteen pigs were anesthetized and randomized to mechanical ventilation with two diff erent ventilator settings for 6 hours; Prot-V (PEEP 10 cmH 2 O, VT 6 ml/kg, n = 8) and Control (PEEP 5 cmH 2 O, VT 10 ml/kg, n = 8). At 0 hours, 1×10 11 colony-forming units (cfu) of P. aeruginosa were instilled intratracheally. At the end of the experiment, six postmortem lung biopsies from predefi ned declivial locations were taken from each animal for cultures and weight measurements. Results P. aeruginosa growth in lung tissue and wet to dry ratio were lower in the Prot-V group than in the Control group (P <0.05 and P <0.05). PaO 2 /FiO 2 was higher in the Prot-V group than in the Control group (P <0.05) ( Table 1) . Conclusion Protective ventilation with low VT and higher PEEP reduces P. aeruginosa growth in lung tissue, lung edema formation and lung injury in contrast with medium-high VT and lower PEEP ventilation in this porcine pneumonia model. Previously, an oxygenation-based method was shown more related to the CT-measured eff ect of PEEP than lung mechanics [1] , indicating lung aeration is better quantifi ed using V/Q mismatch. Pulmonary shunt and low and high V/Q mismatch can be estimated from varying FIO 2 and measuring ventilation and blood gas contents [2] . Methods Preliminary results in six ARDS patients. CT scans were taken in static conditions at PEEP 5, 45 and 15 to 20 cmH 2 O. V/Q was estimated at 5 and 15 to 20 cmH 2 O as: shunt, low V/Q as alveolar to lung capillary PO 2 diff erence (ΔAcPO 2 ), high V/Q as alveolar to lung capillary PCO 2 diff erence (ΔAcPCO 2 ) [2] . Nonaeration, poor aeration, and normal aeration plus hyperinfl ation were calculated from Hounsfi eld units. Aeration and V/Q were compared (Pearson, ρ). Results PEEP improved V/Q in four patients, shunt reducing 7 to 42% with no/small increase in ΔAcPCO 2 . Two deteriorated, with large ΔAcPCO 2 or shunt increase. No systematic changes in ΔAcPO 2 were seen. Figure 1 shows response to PEEP in two patients. Changes in nonaerated regions and shunt were correlated (ρ = 0.94, P = 0.002). No correlations were found between poorly aerated regions and ΔAcPO 2 (ρ = -0.09, P = 0.84) or hyperinfl ated regions and ΔAcPCO 2 (ρ = 0.07, P = 0.88). Conclusion In these preliminary cases, changes in shunt and nonaerated tissue correlated well. However, results indicate poor agreement between changes in low and high V/Q and lung morphology. Introduction In data collected during the Third International Study on Mechanical Ventilation, we compared data from the Netherlands with a global cohort. We hypothesized that tidal volumes (Vt) were smaller and applied positive end-expiratory pressure (PEEP) was higher in the Netherlands, compared with the global cohort. We also compared use of non-invasive ventilation (NIV) and outcomes in both cohorts. Methods A post-hoc analysis of a prospective observational study of patients receiving invasive mechanical ventilation was conducted in Figure 1 (abstract P274) . PCC between mean G (cm) and mean Cstat at diff erent PEEP, in axis Pplat. Conclusion According to our hypothesis, Vt was smaller and applied PEEP was higher in the Dutch cohort. Patients in both cohorts received larger Vt than recommended in prevention of ARDS [2] . Hypercapnia is a main criterion for the use of NIV [3] , which suggests that NIV could be used more often in the Netherlands. The lower incidence of delirium worldwide could be caused by diff erences in sedation or may be due to the used methods of screening. The GUI provided a breath-by-breath visualization of the intratidal CV curve and the intuitive individual compliance shape category. Based on the compliance shape category, diff erent guidelines of PEEP titration were applied with the objective of ventilating the patient mechanically within the range of maximal compliance. Conclusion The newly developed GUI allows a breath-by-breath visualization of the intratidal CV curve with high reliability. Automated classifi cation of the intratidal CV curve into compliance shape categories provides the rationale basis for patient-individual PEEP titration. References  We observed dynamic changes in pulmonary shunt fraction, expressed as changes in SF6 retention, within all breathing cycles recorded, before and after induction of ALI. In healthy lungs at PEEP0 there was a decrease in SF retention at the end of inspiration and a return to baseline levels during expiration. In contrast, SF6 retention increased at PEEP0 in ALI during inspiration and decreased during expiration. In healthy pigs ventilated with PEEP15 there was an increase in SF6 retention at the end of inspiration and a return to baseline during expiration similar to the changes observed in pigs with ALI at PEEP0. In ALI at PEEP15, shunt decreased throughout inspiration and returned to baseline levels during expiration. Conclusion Serial dynamic pulmonary shunt measurements during mechanical ventilation showed distinct variations over the respiratory cycle in both healthy and injured lungs. Increasing PEEP from 0 to 15 cmH 2 O altered the patterns of dynamic pulmonary shunt before and after ALI. Thus, serial assessment of dynamic pulmonary shunt fraction by SF6 retention with MMIMS-MIGET could prove useful for optimization of mechanical ventilation in healthy and injured lungs. Quantifying RV function by echocardiography can be challenging due to its shape and position. STE is a relatively new, feasible, sensitive, angle-independent method for describing cardiac deformation (strain) [2] and is particularly useful in analyzing RV function (RV free wall strain, RVfwS), as has been shown in pulmonary hypertension cohorts [3] . Methods Ten pigs, 40 to 90 kg, anaesthetized, fully mechanically ventilated at 6 to 8 mg/kg were subject to stepwise escalating levels of PEEP at 2-minute intervals (0, 5, 10, 15, 20, 25 and 30 cmH 2 O). RV images were obtained using intracardiac echocardiography (for optimal framerate and endocardial defi nition) and were analyzed offl ine for FAC and RVfwS (using Velocity Vector Imaging; Siemens). Results Escalating levels of PEEP were strongly associated with signifi cant reductions in mean blood pressure (R 2 = 0.8, P <0.0001), FAC (R 2 = 0.8, P <0.0001) and RVfwS (R 2 = 0.9, P <0.001). Paired t tests indicated signifi cant reductions in RVfwS with each step increase in PEEP. FAC only showed signifi cant deterioration at 15 cmH 2 O PEEP. Signifi cant hypotension (a decrease of more than 20 mmHg) occurred after 10 cmH 2 O PEEP. RVfwS decreased by a larger extent and earlier than FAC and mean blood pressure with increasing levels of PEEP. Conclusion RVfwS measured by STE is a sensitive method for determining RV dysfunction induced by PEEP. RVfwS displays a stronger association, greater deterioration and earlier reduction than FAC and mean blood pressure with escalating levels of PEEP. This potentially has interesting implications for the role of STE in managing PEEP levels in critically ill patients with acute lung injury. improving oxygenation in ventilated patients with cardiogenic shock and severe progressive hypoxemia. Methods All cardiac and cardiac surgical patients with cardiogenic shock and ALI/ARDS admitted to our ICU were enrolled between January 2012 and September 2013. Data were collected on admission while the patients were on the conventional mode of ventilation and after 48 hours from switching to APRV. All enrolled patients were hemodynamically monitored with a pulmonary artery catheter and frequent echocardiography assessment. A retrospective analysis of these data was performed. Results Completed datasets were obtained from 29 patients. The cardiac index was increased by 30% (P <0.013), serum lactate decreased by 37% (P <0.001), central venous saturation increased by 42% (P <0.001) and peak airway pressure decreased 19% (P <0.001), with 50% increase of mean airway pressure, hypoxemia improved within the fi rst few hours of alveolar recruitment with PaO 2 /FIO 2 increased by 23% (P <0.018), and there was less use of vasopressors, sedation and neuromuscular blockage over the course of APRV application. Conclusion In our patient series, APRV signifi cantly improved oxygenation and allowed for spontaneous breathing and a reduction in peak airway pressures. Furthermore, this strategy improved hemodynamics and facilitated weaning from MV. Therefore, our data suggest that this ventilation modality has favorable results and appears to be an eff ective alternative for lung recruitment in patients with cardiogenic shock and acute lung injury during their course of stay in cardiac ICU [2] .  Introduction Growing evidence suggests that, as long as the total lung capacity is not overcome, dynamic (that is, tidal volume, VT) is more injurious than static (that is, positive end-expiratory pressure, PEEP) lung deformation [1] . Because the lung behaves like a viscoelastic body [2] , hysteresis may play a role in the development of ventilatorinduced lung injury. The aim of the study was to investigate the eff ects of increasing VT or PEEP on lung hysteresis. Methods In eight healthy piglets we measured total hysteresis and the peak inspiratory pressure (Ppeak) while randomly increasing VT (with no PEEP) or PEEP (with fi xed VT). P1 was extrapolated from the drop in airway pressure during an end-inspiratory pause [3] . Hysteresis attributable to lung parenchyma was computed as: total hysteresis -((Ppeak -P1) × VT). The main fi ndings are shown in Figure 1 . P values refer to oneway repeated-measures analysis of variance. Conclusion Lung hysteresis increases with VT, but not with PEEP. Further studies are needed to prospectively evaluate the role of lung hysteresis in the pathogenesis of ventilator-induced lung injury. Introduction Mechanical ventilation (MV) aims to enhance blood oxygenation and to remove carbon dioxide. However, excessive hyperinfl ation by MV may cause lung injury. Methods Eighteen cats (4 ± 1 kg) were anesthetized with propofol (loading dose of 6 mg/kg and constant rate infusion of 0.5 mg/kg/ minute) and neuromuscular blockade was achieved with rocuronium at 1 mg/kg/minute. Their lungs were initially mechanically ventilated in FiO 2 of 40%, with peak inspiratory pressure (Ppeak) of 5 cmH 2 O for 20 minutes, and then the Ppeak was increased by 5 cmH 2 O increments until 15 cmH 2 O every 5 minutes. Following that, Ppeak was decreased by 2 cmH 2 O every 5 minutes until reaching Ppeak of 5 cmH 2 O. The ventilator maintained the respiratory rate and inspiratory time at 15 breaths/minute and 1 second, respectively. Between the Ppeak increments, we applied a 4-second pause for a 5-mm computed tomography (CT) scan of the thorax area. The radiographic attenuation (in Hounsfi eld units, HU) was classifi ed as over-insuffl ation (1,000 to 900 HU), normal insuffl ation (900 to 500 HU) and atelectasic (500 to 100 HU). We split the lungs into three proportional gravitational zones (I, II and III) from apex to base. The three zones presented increased over-insuffl ated areas and decreased areas with normal insuffl ation with increasing Ppeak from 5 to 15 cmH 2 O. At 5 cmH 2 O, the areas of over-insuffl ation and normal insuffl ation in zones I, II and III were 13% and 36%; 4% and 22%; and 0.7% and 15%, respectively. At 15 cmH 2 O, the areas of over-insuffl ation and normal insuffl ation in zones I, II and III were 74% and 55%; 81% and 57%; and 82% and 71%, respectively. Conclusion The higher proportion of overly distended pulmonary areas in high Ppeak may increase the risk of lung injury. The lowest Ppeak (5 cmH 2 O) showed less potential to lung injury as it has higher areas of normal insuffl ation and less areas of over-insuffl ation in all gravitational zones. Introduction Activation of lymphocyte apoptosis while reducing the endogenous nitric oxide is a predictor of adverse outcome in newborns on mechanical ventilation [1] . With the aim to improve the results of treatment we studied the eff ect of inhalable nitric oxide on the immune system in newborns with respiratory diseases on mechanical ventilation [2] . Methods With the permission of the ethics committee in a controlled, randomized, blind clinical trial we included 27 newborns with respiratory diseases on mechanical ventilation. Randomization was performed by the method of envelopes. Group I (n = 17), patients receiving inhalation of nitric oxide at a concentration of 10 ppm for 24 hours controlling the level of methemoglobin (Pulmonox mini; Messer II NO Therapeutics, Austria). Group II (n = 10) did not receive inhaled NO. At admission and at 3 to 5 days we studied subpopulations of lymphocytes by one-parameter immunophenotyping using reagents (Immunotech Beckman Coulter, USA): fi tz-labeled CD3, CD4, CD8, CD14, CD19, CD34, CD56, CD69, CD71, CD95 monoclonal antibody, the relative content of lymphocytes in early and late apoptosis using Annexin V + -labeled FITK and propidium iodide (PL + ), labeled with PE (Saltag, USA), with results on the Beckman Coulter Epics XL cytometer (USA). The statistical power of the study was 80% (α ≤0.05). The group receiving protective ventilation showed lower levels of cfDNA in arterial blood compared with controls (P = 0.02). Transhepatic levels of cfDNA were higher compared with trans-splanchnic levels during the experiment (P = 0.02), but this eff ect was attenuated in the group receiving protective ventilation. No diff erence between the groups was detected in blood samples from the jugular bulb. Conclusion In experimental postoperative sepsis, protective ventila tion suppresses arterial levels of cfDNA. The liver seems to be a signifi cant contributor to systemic cfDNA levels, an eff ect that is suppressed during protective ventilation. The total of 43 patients who met the rescue therapy criteria were further grouped into the HFOV group (24 patients) and the CLPV group (19 patients) depending upon modality of ventilation received after satisfying fi rst-time HFOV eligibility criteria. Both groups were comparable for diff erences with Fisher's t test for qualitative variables and ANOVA for quantitative variables (  The duration of mechanical ventilation and ratio of death were signifi cantly higher in the patients with VAP(+). CPIS levels in the patients with VAP(+) were signifi cantly higher than the patients with VAP(-) in the days after the diagnosis. CPIS levels were also higher in the patients with VAP(+) on the day of diagnosis. At the same day the parameters, which included the CPIS, body temperature, leukocyte number, tracheal secretions, PaO 2 /FiO 2 levels and the presence of infi ltrates on the chest radiograph, were signifi cantly higher in VAP(+) patients (P <0.05). ROC curves were formed for CPIS scores to be used in diagnosis VAP and the cutoff point had a sensitivity of 97.44% and a specifi city of 100% for diagnosing VAP. Conclusion At the end of the study, it was concluded that using the CPIS for early diagnosis and treatment of VAP and thinking that the patients with CPIS >5 were VAP(+) are guiding factors to resolve the problems associated with VAP in ICU patients. Methods A retrospective analysis of 17 patients (male/female ratio: 6/11; median age: 35 (range 16 to 63)) who underwent arteriovenous or venovenous interventional lung assist (iLA; Novalung, Germany) support as bridging to primary LTX (n = 11) or re-LTX (n = 6) between 2005 and 2013. The underlying diagnosis was bronchiolitis obliterans syndrome III in re-LTX patients (n = 6), cystic fi brosis (n = 5), idiopathic pulmonary fi brosis (n = 2), emphysema (n = 1), adult respiratory distress syndrome (n = 1), hemosiderosis (n = 1), and chronic obstructive lung disease (n = 1), respectively. The type of iLA was arteriovenous in 10 and venovenous (iLA active) in seven patients. The median bridging time was 14 (1 to 58) days. The type of transplantation was bilateral LTX (n = 6), size-reduced bilateral LTX (n = 5), lobar bilateral LTX (n = 4), and right single LTX with contralateral pneumonectomy (n = 1), respectively. Hypercapnia was eff ectively corrected in all patients within the fi rst 12 hours of iLA therapy: PaCO 2 levels declined from 145 (70 to 198) to 60 (36 to 99) mmHg, P <0.0001. iLA was initiated during non-invasive ventilation in three patients, of whom one was intubated prior to LTX. All other patients (n = 14) were placed on iLA while on invasive MV. Of those, three patients were extubated and remained on iLA until LTX, one patient was weaned from iLA and remained on MV until LTX, and one patient was weaned from iLA and MV prior to LTX. Five patients were switched to extracorporeal membrane oxygenation (venovenous n = 2, venoarterial n = 3) after 5 (1 to 30) days on iLA support. One patient died prior to LTX due to septic multiorgan failure (SMOF). All others (n = 16; 94%) were successfully transplanted. Of these, two patients died in the ICU due to SMOF. The remaining 14 patients (82%) survived to hospital discharge and were alive at a median follow-up of 20 (1 to 63) months. The extracorporeal device allowed effi cient CO 2 removal rates at FiO 2 1 (Figure 1 ) and 0.21 (Figure 2) , ranging from 40 to 60 ml/ minute. Effi ciency was increased with blood and sweep gas fl ows. Carbia and pH of animals were signifi cantly modifi ed after 10 minutes Introduction Referral for ECMO has been demonstrated to reduce mortality in severe hypoxic respiratory failure [1] [2] [3] . The numbers of patients that undergo ECMO is still small and the service depends on timely referral from regional ICUs. There is evidence that intensivists' views on the role of ECMO are mixed [4] . The purpose of this study is to determine whether there are variations in the geographical distribution of patients that receive ECMO. Methods NHS England provided the home primary care trust (PCT) of all adult patients referred for ECMO for potentially reversible respiratory failure from 2008 to 2012. The referrals from each PCT were indexed to the population of each area to produce a referral rate per 1,000,000 people. Results See Figure 1 . ECMO services have expanded rapidly in the last 5 years in England following the publication of evidence for its effi cacy and concerns regarding an infl uenza pandemic. The referral rates for ECMO for severe hypoxic respiratory failure vary greatly around the country from 88 per 1,000,000 population in Leicester City to no referrals in 32 PCTs. Possible explanations could include: the distributions of swine fl u around the country, referring doctors' beliefs about the effi cacy of ECMO, local access to high-frequency oscillation ventilation and possible reluctance of teaching hospitals to refer to specialist centres. Further investigation to account for this variation appears indicated. Conclusion The referral rates for ECMO vary greatly around the country. References Introduction Using micro-computed tomography (MicroCT), we assessed the eff ectiveness of a cleaning closed-suctioning system (CSS) to remove secretions from the endotracheal tube (ETT) lumen. Biofi lm growing within the ETT, soon after intubation, increases the patient's risk to develop ventilator-associated pneumonia, and new cleaning devices have been designed to keep the ETT clean from secretions [1] . Methods In a bench test, we injected a water-based gel into unused ETTs to evaluate MicroCT scan (SkyScan 1172; Bruker, Belgium) eff ectiveness to measure secretions. In six critically ill patients, a cleaning CSS (Airway Medix Closed Suction system; Biovo, Tel Aviv) was used three times a day to keep the ETT clean. After extubation, we measured ETT secretions volume by MicroCT scanning over a length of 20 cm from the ETT tip. We also collected ETTs from 11 patients treated with a standard CSS as controls, and evaluated ETT microbial colonization. The volume of gel measured by MicroCT strongly correlated with the volume of injected gel (P <0.001, R 2 = 0.99). At extubation, a lower amount of secretions was measured in the ETTs treated with the cleaning CSS as compared with controls (0.031 ± 0.029 vs. 0.350 ± 0.417 mm 3 , P = 0.028), corresponding to a smaller occupation of the cross-sectional area (average 0.3 ± 0.4 vs. 3.8 ± 4.5% respectively, P = 0.030). Microbial colonization tended to be reduced in the ETTs treated with the cleaning CSS (total bacterial charge 1.3 ± 1.7 vs. 3.6 ± 2.7 log(CFU/ml), P = 0.08). Conclusion MicroCT scan showed high precision and accuracy in measuring the volume of secretions in bench tests and can thus be used to evaluate the eff ectiveness of actions or devices studied to reduce ETT biofi lm accumulation. In a small nonrandomized population of critically ill patients, the use of an ETT cleaning device appeared eff ective to reduce the volume of secretions present in the ETT at extubation. Reference Introduction Today's healthcare environment has forced providers to constantly evaluate the materials used, and to fi nd less expensive alternatives. Recently, low-cost endotracheal tubes (ETTs) have been introduced to the market. The aim of this study was to test these tubes (Portex AirCare and Cardinal Health ETT) compared with endotracheal tubes with known performance (Hi-Lo Covidient pre ISO standard and Taper-Guard Covidient, post ISO standard). Methods We used the required test setups according to the ISO standard for cuff sealing performance. The tubes were tested versus each other in size 7.0, 7.5 and 8.0 mm. Kink performance was done in the routine manner. Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 Results The leak test showed that the Portex AirCare ETT leaked signifi cantly more compared with both the Hi-Lo (P <0.05) or Taper-Guard (P <0.001) ETTs. The standard deviation leak rate for AirCare was large, suggesting varying seal performance between tubes from various lots. The kink test showed no diff erence among tubes. Conclusion Although tubes look the same there may be diff erences in performance. This study demonstrated that the new cheaper ETTs had greater cuff leak compared with the two tubes used for comparison. We can only speculate whether diff erences found in this study is a result of cost cutting. However, the great variability in sealing performance between ETTs of the same size from diff erent lots would indicate that manufacturing controls may be less stringent. Although we cannot demonstrate that the higher incidence of leak will result in adverse patient outcomes, one can surmise that the possibility exists. Thus, selection of endotracheal tubes should not be based on purchase price alone but should take into account documented performance in standardized tests. Methods A retrospective review was performed using data in the last 5 years. All OPs were from the ICU of our university hospital. Only OPs who underwent a PDT were selected with BMI >30 kg/m 2 . All OPs needed prolonged mechanical ventilation. A total of 67 OPs were identifi ed, with 60 PDTs placed using the Ciaglia Blue Rhino (CBR) Introducer Kit and seven PDTs with the UniPerc PDT Kit. All PDTs were performed by dedicated staff including residents. Valuation of clinical anatomical and physio-pathological features of the OPs and US scan of the neck came before the procedure. At the beginning of the procedure we placed a 5 mm ID orotracheal tube by tube exchange with video-FBS assistance, as already described [3] . An 8 to 9 mm ID wire-reinforced silicone tracheostomy tube (rTT) with adjustable fl ange was chosen instead of a standard PVC or silastic TT (sTT) in all OPs treated with CBR because of the anatomical particularities of OPs and because of external traction by the weight of the tubing attached to the TT. An extralong TT (eTT) was chosen because the pretracheal tissue was too thick for a regular-sized TT in 16 OPs with BMI >40 kg/m 2 treated with CBR. For three OPs treated with CBR we needed to change rTT to eTT because of tube dislodgement and subocclusion X-ray and video-FBS diagnosis. The UniPerc technique was chosen for OPs with BMI >40 kg/m 2 . Results No major complications (aborting procedure, >50 ml bleeding, TT misplacement, death) were observed. We had only minor complications (<50 ml bleeding: 3%; ring fracture: 2%; diffi cult insertion: 21% only with CBR rTT because of the step between the tip of the rTT and its introducer). UniPerc eTT placement has always been easy. Conclusion In our experience, the data do not support what previous studies have shown suggesting increased risk of complication in OPs [1, 2] . We know that the sTT could not be eff ective in OPs. The use of US, video-FBS assistance [3] , and rTT with an adjustable fl ange allows a safe and eff ective adjustment to anatomical OP particularities, avoiding collected risks. References Introduction High-fl ow oxygen therapy (HFOT) delivered through nasal cannulas can improve oxygenation as compared with low-fl ow oxygen devices. It has been shown that nasal HFOT can generate a positive airway pressure, which increases linearly with the gas fl ow rate. Data on the use of HFOT delivered through a tracheostomy are scarce. The aim of the present randomized, controlled, cross-over trial was to assess the eff ects of HFOT delivered through a tracheostomy on arterial blood gases and endotracheal pressure in critically ill patients. Methods Tracheostomized patients underwent HFOT with three gas fl ow rates (10 l/minute, 30 l/minute and 50 l/minute), randomly applied for 20-minute periods. At the end of each period, arterial blood gases, respiratory rate, and endotracheal pressure (Ptrach) were measured. Ptrach was recorded over the last 3 minutes of each study period: the maximum expiratory pressure (MEPtrach) and mean expiratory pressure were measured and averaged for all respiratory cycles during 1-minute recording with stable breathing. FiO 2 was kept constant during the whole study. Results Seventeen tracheostomized patients were enrolled (SAPS II 52 ± 10, PaO 2 96 ± 27 mmHg, PaCO 2 33 ± 10 mmHg). Increasing the gas fl ow rate from 10 l/minute to 30 l/minute was associated with an increase in PaO 2 /FiO 2 that did not improve further when 50 l/minute was used (259 ± 66, 317 ± 79, and 325 ± 76, respectively, P <0.001). The same trend was observed with PaO 2 (89 ± 19 mmHg, 109 ± 26 mmHg, and 113 ± 29 mmHg, respectively, P <0.001) and SaO 2 (96 ± 3%, 98 ± 2%, 98 ± 2%, respectively, P <0.001). PaCO 2 (32 ± 8 mmHg on average) and respiratory rate (27 ± 7 breaths/minute on average) did not change with diff erent gas fl ow rates. MEPtrach (0.96 ± 0.43 cmH 2 O, 1.32 ± 0.4 cmH 2 O, and 1.89 ± 0.5 cmH 2 O at 10 l/minute, 30 l/minute and 50 l/minute, respectively, P <0.01) and mean expiratory pressure (0.54 ± 0.27 cmH 2 O, 0.91 ± 0.29 cmH 2 O, and 1.36 ± 0.35 cmH 2 O, at 10 l/ minute, 30 l/minute and 50 l/minute, respectively, P <0.01) increased with fl ow. Changes in PaO 2 /FiO 2 were not correlated with changes in expiratory pressures. Conclusion When HFOT is used through a tracheostomy at increasing gas fl ow rate, oxygenation increases up to 30 l/minute while CO 2 clearance and the respiratory rate do not vary. Tracheal expiratory pressure increases with fl ow, but changes are small and probably of limited clinical relevance. Changes in oxygenation are not related to the variations of tracheal expiratory pressure. HFOT through a tracheostomy has diff erent eff ects from when a nasal interface is used. We developed a prototype tracheostomy tube that had an increased length for a given internal diameter. We also increased the mobility to fl ange, in order to reduce pressure areas and assist with fi xation. A third modifi cation was to ensure a maximal internal diameter for a given external diameter. We piloted its use in 20 patients. No adverse events were observed and the clinical impression was that the increased length was benefi cial to the patients. Conclusion Following the success of the pilot study, this new tracheostomy tube was developed and then marketed by Kapitex. This tube was named the Tracoe Twist Plus. We have now used this tube for 2 years. We have since performed an extensive retrospective audit of tracheostomy usage and the eff ect of the introduction of the novel Tracoe Twist Plus, and the complication rate was reduced in terms of displacement and obstruction. Reference in ICU patients, MV with a defl ated cuff in patients with a tracheostomy can be provided safely and comfortably, by use of a BiPAP Vision®. Air leakage to the upper airway enables speech [2] . By adding a Passy-Muir® speaking valve as second step, the quality of speech and cough will improve. The ability to speak provides an important improvement in communication. Methods The aim of this study was to compare weaning from MV by gradually decreasing the level of support in cuff -defl ated ventilation with use of a BiPAP Vision® and a Passy Muir® speaking valve, or by trials of spontaneous breathing with use of a speaking valve, both for progressively longer periods of time. We examined the diff erences in the ability to speak, the duration of the weaning period, the occurrence of delirium and the frequency of tracheal suctioning. We performed a single-centre retrospective and prospective observational study in a 22-bed mixed ICU during 1 year. Data were collected using the patient data management system. Baseline criteria were age, gender, APACHE IV score, ICU length of stay and duration of MV before placement of the tracheostomy. Results Ten patients were included, fi ve in the BiPAP group and fi ve in the spontaneous group. There were no signifi cant diff erences in the baseline criteria. On the second day after tracheostomy, three out of fi ve patients in the BiPAP group were able to speak compared with one in the spontaneous method group. A diff erence in speaking ability remained until day 9 (see Figure 1 ). At fi rst time of speaking, the BiPAP group had higher PEEP level (10 vs. 7.5 cmH 2 O) and higher SOFA score (6.2 vs. 4.6) compared with the spontaneous group. There was no signifi cant diff erence in delirium, duration of weaning and tracheal suctioning between both groups. Conclusion Cuff -defl ated MV in ICU patients enables speaking during ventilator dependence. With this technique the ability to speak started in an earlier phase of weaning compared with weaning with spontaneous breathing trials and a speaking valve. References  In vitro evaluation showed that the DLET had the lowest K 2 (7 = 11.33; 7.5 = 8.74; 8 = 7.57; 7f = 6.13; 7.5f = 10.52; 8f = 12.28; F4 = 130.0; F5 = 11.12; DLET = 5.25 cmH 2 O/l/minute). During in vivo evaluation, PT was performed with the conventional ETT with FOB and DLET for fi ve patients in each group (age 69 ± 13 vs. 71 ± 16; SAPS II: 56 ± 14 vs. 52 ± 20; GCS 3 vs. 4). Gas exchange before and after the procedure did not diff er between the groups, but the Δ values of pH, PaO 2 and PaCO 2 measured before and after the procedure were, ETT+FOB versus DLET: ΔpH: -0.05 ± 0.05 versus 0.01 ± 0.02, P = 0.04; ΔPaO 2 : -112.6 ± 112.6 versus 41.6 ± 25.3, P = 0.01; ΔPaCO 2 : 14.5 ± 10.8 versus 0.6 ± 1.1, P = 0.02; ΔHCO 3 : 0.5 ± 2 versus -0.04 ± 0.3, P = 0.6. Conclusion The DLET resulted in adequate airway patency and minimal obstruction due to the lower channel exclusively dedicated to patient's ventilation. Gas exchange in PT with the DLET remained stable without any variation in oxygenation and carbon dioxide levels, although the same settings of mechanical ventilation. Introduction Respiratory weaning in ICUs can be a lengthy and expensive process [1] , but may be facilitated by the use of tracheostomies. Discharging patients with tracheostomies to general wards improves ICU bed availability but raises potential patient safety issues. This is demonstrated by the increased mortality compared with patients decannulated before discharge from the ICU [2] . We investigated how often ICUs in the UK discharge patients with tracheostomies to wards, which wards these are and whether systems are in place to ensure adequate safety on discharge. Methods We telephoned 217 ICUs in the UK. Nursing staff answered a series of questions regarding the discharge of patients with tracheostomies to the wards and their follow-up. We obtained information from 203 ICUs. A total of 201 units used tracheostomies for respiratory weaning. In total, 151 routinely discharged patients to wards with tracheostomies, 11 never did and 39 did occasionally. Five discharged to the high dependency unit only, 60 to respiratory wards only, 70 to specialist wards and 15 to any or most wards. Eighty-fi ve out of 190 units discharged patients with tracheostomy cuff s both up and down, 72 discharged with the cuff down or cuffl ess and 16 with the cuff 'usually down' . A total of 141 hospitals had routine follow-up for tracheostomy patients from critical care outreach or other services. Critical care outreach was available 24 hours a day in 65 hospitals. Conclusion The vast majority of ICUs in the UK perform tracheostomies for respiratory weaning and many routinely discharge patients to the wards prior to decannulation. Routine follow-up is usually available, but cover may only be available during the day. Patients may go to a specialist ward with trained nurses but this is not always the case. Patients are often discharged to wards with their tracheostomy cuff up, raising major safety issues if their tracheostomy tubes block and nurses are not trained for such emergencies. Twenty-four hours a day critical care outreach cover may improve patient safety, but further research and the production of guidelines is needed to facilitate the safe discharge of patients with tracheostomies from ICU to the wards. We contacted a total of 246 general ICUs, 72% of which we received a response. The average number of beds per ICU from all units who responded was 11. Ninety-eight per cent of ICUs that we questioned did use PCT. For three units, the average bed number per unit was 11 and the other 2% of ICUs who did not use PCT had fi ve beds per unit on average. The proportion of ICUs that employed subglottic suction ports on their ETTs was 43% having on average 11 beds per unit, whilst the proportion of ICUs that did not employ subglottic suction ports was 57%, also with 11 beds per unit on average. Regarding PCT subglottic suction ports, 38% of ICUs did utilise these tubes whilst 62% did not. Of the group of ICUs that did use subglottic suction ports on their tracheostomy tubes, the average beds per unit was 12. Of the group of ICUs that did not use subglottic suction ports on their tracheostomy tubes, the average beds per unit was 10. Conclusion Signifi cant diff erences in practise exist with PCT and subglottic suction ports on tubes. The size of the ICUs in these groups is variable. The larger units are more likely to use PCT over the smaller units. Regarding subglottic suction ports on ETT and tracheostomy tubes, the size of the ICU does not necessarily dictate their use. We propose that all ICUs review their policy on the use of PCT and subglottic suction-assisted tubes to help improve surgical complications, cost, VAP rates and ICU stays. Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 Introduction Over 5,000 tracheostomies are performed in the UK per year [1] . The 4th National Audit Project identifi ed signifi cant morbidity and mortality associated with tracheostomy care [1] . The National Tracheostomy Safety Project (NTSP) 2013 manual highlighted the need for: local policy; an appropriate care environment; immediate availability of emergency equipment; trained staff and local training programmes; and bed-head sign and emergency algorithms for tracheostomy patients [2] . Following these guidelines, we asked: how are adult tracheostomy patients managed post discharge from the intensive care/high dependence unit (ICU/HDU) throughout the UK? Methods In November 2013, 200 adult ICU/HDUs throughout the UK were contacted to take part in a telephone survey. Data were collected on tracheostomy weaning, post-ICU/HDU care, safety guidelines, emergency protocols and training for clinicians and nurses. Results Out of the 200 adult ICU/HDUs contacted, 134 took part in the survey. Out of these, 44% have a tracheostomy weaning protocol, 69% initiate weaning whilst the patient is mechanically ventilated, and 92% use a speaking valve in their weaning process. Also, 87% allow tracheostomy patients to have oral nutritional intake and in 59% of these the decision involves speech and language therapy. Post ICU/ HDU, 67% of units discharge to specialised wards, 22% to nonspecialised wards, 4% to dedicated step-down units and 6% do not step down their tracheostomy patients. A critical care outreach team reviews the patients in 73% of the hospitals surveyed. Furthermore, only 11% of the hospitals have a consultant lead tracheostomy ward round and 17% have a tracheostomy multidisciplinary team (MDT). Also within these hospitals, 57% have their own tracheostomy safety guidelines and 70% have emergency tracheostomy management protocols. On the wards: 34% have tracheostomy bed-head information signs, 93% have emergency bed-side tracheostomy equipment, 89% have a tracheostomy training programme, and 50% have a MDT approach to decannulation. Conclusion There is a wide variation in post-ICU/HDU management of tracheostomy patients throughout the UK. Although there are well established UK national guidelines for the management of tracheostomy patients, outside the ICU/HDU environment there is a lack of full implementation of the NTSP recommendations, increasing the risk of tracheostomy-related morbidity and mortality. Introduction Ventilator-associated pneumonia (VAP) is the most common nosocomial infection among ventilated patients and is associated with increased mortality and morbidity [1] . Oral chlorhexidine has been used to decontaminate the airway in critically ill patients, as studies suggest a risk reduction in VAP [2] . Chlorhexidine reacts with soaps in toothpaste to form inactive insoluble salts [3] . A minimum delay of 30 minutes between tooth brushing and the subsequent application of chlorhexidine is therefore recommended [4] . Methods A telephone questionnaire was conducted on all ICUs in the UK to assess current oral decontamination procedures with regards to chlorhexidine use and the timing of tooth brushing with toothpaste. Results Sixty-fi ve per cent of ICUs in the UK responded to our survey (n = 157). Ninety-seven per cent (n = 152) used chlorhexidine and 96% (n = 150) used it as part of a ventilator care bundle. Forty-six per cent (n = 70) used a gel, 32% (n = 48) used a mouthwash and 23% (n = 34) used both preparations. The frequency of chlorhexidine application varied between ICUs; 15 (9.9%) applied 4-hourly, 91 (59.9%) 6-hourly, 20 (13.2%) 8-hourly, 19 (12.5%) 12-hourly and seven (4.6%) applied at variable times. Ninety-seven per cent (n = 152) brushed patient's teeth; 86% (n = 130) used toothpaste, 3% (n = 5) used chlorhexidine gel and 11% (n = 17) used both. Ninety-seven per cent (n = 147) of ICUs using chlorhexidine also brushed patient's teeth with toothpaste. Forty-eight per cent (n = 70) administered chlorhexidine within 30 minutes of toothpaste application (Table 1) . Conclusion Chlorhexidine is being used too soon after the application of toothpaste in 48% of ICUs in the UK. This results in attenuation of its eff ect and may remove its benefi cial risk reduction in VAP. Awareness of this interaction should be emphasised. We had 77 responses. The majority were from doctors (88%) and the rest from nurses. Of respondents, 63% worked in district general hospitals, 28% in teaching hospitals and the rest in specialist units. Overall, 54% of respondents worked in units using SSD. From these responses, the types of patients receiving SSD are summarised in Table 1 . One hundred per cent of units used intermittent SSD. Seventyone per cent of respondents reported that SSD tubes were stored only on their ICU, with 26% reporting availability in acute areas and the rest hospital wide. Twenty-eight per cent of respondents indicated it was unit policy to reintubate patients to facilitate SSD. More than 90% of units had a ventilator care bundle and regularly measured cuff pressures. Overall, 83% of those surveyed thought SSD was benefi cial in the prevention of VAP. Conclusion Despite specifi c recommendations from the UK Department of Health [2] , only about one-half of respondents work in ICUs where SSD has been adopted. Most studies show benefi t in patients expected to be ventilated for greater than 72 hours [1] , but most units used SSD in all intubated patients. The reintubation rate to facilitate SSD was also reasonably high, despite a lack of evidence to support this practice. In the vast majority of hospitals, SSD endotracheal tubes are stored only on the ICU and so the need for reintubation may result from a lack of available appropriate tubes at the point of fi rst intubation. Introduction Emulsifi ed perfl uorocarbons (PFC) are synthetic hydrocarbons that can carry 50 times more oxygen than human plasma. Their properties may be advantageous in applications requiring preservation of tissue viability in oxygen-deprived states, which makes them a potential candidate for combat and civilian prehospital resuscitation. Our hypothesis was that an intravenous dose of PFC increases vital organ tissue oxygenation, improves survival and reduces or prevents the development of ventilator-associated ARDS. Here we report data from the second part (ARDS only) of a multiphase swine study to investigate the benefi ts of PFC in treating hemorrhagic shock and preventing ARDS. Methods Anesthetized Yorkshire swine were randomized (n = 6/ group) to receive a bolus of the PFC Oxycyte™ either 45 minutes before (PFC-B) or after (PFC-A) induction of ARDS or nothing as a control (NON). ARDS was induced via intravenous oleic acid infusion (time 0 (T0)) over 30 minutes. Animals were monitored for physiological and hematological parameters. They were euthanized at T180 minutes and a full necropsy and histopathological analysis was performed. Results Survival was 100% in the NON group, 80% in the PFC-A group and 20% in the PFC-B group. Mean arterial pressure (MAP) and mean pulmonary artery pressure (MPAP) were signifi cantly increased during infusion of PFC and during ARDS in the PFC-B group, while cardiac output (CO) was signifi cantly reduced. In the PFC-A group it was observed that MAP and MPAP increased and CO decreased during ARDS induction, but not during PFC infusion. Those changes were signifi cant in comparison with the NON group. Oxygen delivery and consumption in the PFC-A group were signifi cantly increased. Histopathological analysis is currently being performed. Interim analysis showed a trend to reduced alveolar damage in PFC-A animals. Conclusion Administration of PFC before induction of ARDS was detrimental, while giving PFC after ARDS improved oxygen delivery and increased oxygen consumption. Although survival in this group was lower than in the NON control group (80% vs. 100%, not signifi cant), a reduction in alveolar damage was observed. This might improve longterm outcome after ARDS. Based on these data we will continue to the fi nal phase of this project and evaluate the capacity of PFC to prevent ARDS in combination with HS. Introduction Venovenous extracorporeal membrane oxygenation (VV ECMO) is a treatment option for acute respiratory distress syndrome (ARDS) to minimize ventilator-induced lung injury including lifethreatening pneumothorax. The purpose of our study was to investigate the safety and effi cacy of VV ECMO for preventing pneumothorax in ARDS patients who were complicated with emphysematous/cystic changes in the lung. Methods We have retrospectively analyzed data of ARDS patients complicated with emphysematous/cystic changes in the lung who were admitted to our ICU from 2006 through 2012. We divided the subjects into two groups, patients treated with VV ECMO (ECMO group), and those treated only by conventional ventilator management (non-ECMO group). Correlations between age, sex, underlying disease, PaO 2 / FIO 2 ratio on admission, duration of ICU stay, survival and incidence of pneumothorax were evaluated. Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 Results Forty-one patients were included in this study (ECMO and non-ECMO group, 21 and 20 patients, respectively). There were no signifi cant diff erences between ECMO and non-ECMO groups as regards age, sex, underlying disease, PaO 2 /FIO 2 ratio, duration of ICU stay, and survival. In the ECMO group, the mean duration of ECMO use was 17 ± 13 days, and bleeding due to anticoagulation was observed in fi ve patients. The mean airway pressure in the ECMO group was signifi cantly lower than in the non-ECMO group (12 ± 6 cmH 2 O, 22 ± 6 cmH 2 O, respectively; P <0.0001). The incidence of pneumothorax was also signifi cantly lower in the ECMO group than the non-ECMO group (10%, 45%, respectively; P = 0.015). In Kaplan-Meier analysis, the proportion of pneumothoraxfree patients was signifi cantly higher in the ECMO group (P = 0.014). In multivariate analysis, conventional ventilator management, presence of interstitial pneumonia and the duration of intubation were the independent risk factors of pneumothorax (hazard ratio (HR), 18.0, P = 0.010; HR 33.3, P = 0.025; HR 1.05, P = 0.041, respectively). Conclusion Although the survival rate was not statistically diff erent, the use of ECMO for ARDS patients complicated with emphysematous/ cystic changes in the lung markedly reduced the incidence of pneumothorax. The increase in LW/BW was induced in ALI mice. Histologically, widespread alveolar wall thickening caused by edema, severe hemorrhage in the interstitium and alveolus, and marked and diff use interstitial infi ltration with infl ammatory cells were observed in the ALI group. Meanwhile, the levels of IL-6 in lung tissue were signifi cantly enhanced in the LPS-induced ALI mice. The mRNA expression of T-bet and RORγt was upregulated in ALI mice at 24 hours and 48 hours relative to normal mice (P <0.05 vs. Con). There was no signifi cant diff erence in the expression of GATA-3 among groups at 24 hours and 48 hours. Meanwhile, the levels of IFNγ, IL-17 and IL-6 in lung tissue were signifi cantly enhanced at 24 hours and 48 hours in the LPS-induced ALI mice. In addition, the levels of IL-4 in lung tissue were signifi cantly enhanced at 48 hours in the LPS-induced ALI mice. The expression of T-bet mRNA and RORγt mRNA had a strong correlation with the IL-6 concentration. However, there was no signifi cant correlation of GATA-3 with the IL-6 concentration. In addition, there was a signifi cant correlation of IFNγ, IL-4 and IL-17 with the IL-6 level in LPS-induced ALI at 24 hours and 48 hours. Conclusion ALI provokes Th1 and Th17 polarization response. Th1 and Th17 may participate in the early infl ammatory response to ALI. Introduction Dendritic cells (DC) may play an important role in acute lung injury (ALI) [1] . CD80 is the crucial co-stimulatory molecule that is expressed on the surface of DCs. However, little is known about the expression of CD80. The purpose of this study was to observe the expression of CD80 on circulating, lung and splenic dendritic cells (DC) in ALI mice. Methods Twelve C57BL/6 mice were randomly divided into two groups: control group and ALI group. Blood, lungs and spleens were harvested at 6 hours after LPS or PBS administration. The level of CD80 on DC was assessed by fl ow cytometry (FCM). The IL-6 level in the lung was measured by enzyme-linked immunosorbent assay. Lung wet weight/ body weight (LW/BW) was recorded to assess lung injury. Meanwhile, pathological changes were examined under an optical microscope. Results LPS-ALI resulted in a signifi cant increase in lung W/D ratio. Histologically, widespread alveolar wall thickening caused by edema, severe hemorrhage in the interstitium and alveolus, and marked and diff use interstitial infi ltration with infl ammatory cells were observed in the ALI group. Meanwhile, the levels of IL-6 in lung tissue were signifi cantly enhanced in the LPS-induced ALI mice. FCM analysis showed that the level of CD80 on circulating DC in control group was (3.3 ± 1.5)%, CD80 expression on lung DC was (3.6 ± 1.2)%, and expression of CD80 on splenic DC was (9.0 ± 3.6)%, which was signifi cantly higher than that on circulating DC and lung DC (P <0.05). In the ALI mouse, the level of CD80 on peripheral blood DC was (5.1 ± 2.1)%; the CD80 level on lung DC was (9.6 ± 2.50)%, which was signifi cantly higher than that on peripheral blood DC (P <0.05); and the level of CD80 on splenic DC was (25.2 ± 4.7)%, which was signifi cantly higher than CD80 levels on the peripheral blood and lung DC (P <0.05). The CD80 level on lung and splenic DC in ALI mice was signifi cantly higher than that on lung and splenic DC in control mice (P <0.05 vs. Con). Conclusion There is a dynamic characteristic in the expression of CD80 on DC populations in normal and ALI mice. Elevated expression of CD80 on DC seems to play an important role in the pathogenesis of ALI. Acknowledgements Supported by the Research Project CPSFG 2013M542578, JSPSFG 1301005A, SYS201251 and 2013NJZS50. Reference Introduction Two recent RCTs (OSCAR and OSCILLATE [1, 2] ) showed that high-frequency oscillatory ventilation (HFOV) had no positive impact on mortality. We present our experience over 5 years. Methods Adult ARDS patients who received HFOV from 2008 to 2012 were included. Demographics, illness severity and outcomes were collected retrospectively. Results A total of 118 patients were included; 56.8% were male, mean age was 54.8 years. RRT use was 45% during admission. Vasoactive agent use and neuromuscular blockade infusion rate was 81.9 and 29.7% pre HFOV respectively. The 28-day and 6-month mortality was 61.9 and 70.3%. A total of 60.1% had less than 48 hours conventional ventilation (CV) pre HFOV. The 6-month mortality was 64.8% for this group. Patients who had over 48 hours CV pre HFOV had a 6-month mortality of 76.6%. See Table 1 . Conclusion Mortality rates were higher than in recent trials [1, 2] . Our patients represent a more critically unwell group with lower PF ratios pre HFOV and high vasoactive and RRT use. HFOV may still have a role in the treatment of these very sick patients with treatment refractory to conventional ventilation. The increase in LW/BW induced by LPS was partly prevented by pretreated with losartan. Histologically, losartan eff ectively attenuated the LPS-induced lung hemorrhage, and leukocyte cell infi ltration in the interstitium and alveolus. Meanwhile, the levels of IL-6 in lung tissue were signifi cantly enhanced in the LPS-induced ALI mice. With pretreatment of ALI mice with losartan, the level of IL-6 in lungs markedly decreased. The mRNA expression of T-bet and RORγt was upregulated in ALI mice at 24 hours and 48 hours relative to normal mice (P <0.05 vs. Con). There was no signifi cant diff erence in the expression of GATA-3 among groups at 24 hours and 48 hours. Of note, pretreatment of ALI mice with losartan resulted in signifi cantly reduced mRNA expression of T-bet at 24 hours and 48 hours and RORγt mRNA expression at 48 hours (P <0.05 vs. ALI). Meanwhile, the levels of IFNγ, IL-4, IL-17 and IL-6 in lung tissue were signifi cantly enhanced at 24 hours and 48 hours in the LPS-induced ALI mice. In addition, both IFNγ and IL-17 in lung tissue at 24 hours and 48 hours decreased signifi cantly in losartan-pretreated mice compared with the ALI mice. With pretreatment of ALI mice with losartan, the level of IL-4 in lungs was not changed. Conclusion Ang II-induced Th1 and Th17 polarization response could upregulate infl ammatory response and induce lung injury, and losartan may be a promising substance for clinical use in LPS-induced ALI.  Methods During the last year, all stool specimens received in the microbiology department from patients hospitalized in the 30-bed, multidisciplinary ICU of a tertiary-care hospital were evaluated. Specimens were ordered by physicians in the presence of clinical features compatible with C. diffi cile-associated infection. Each specimen was subjected to diagnostic tests for C. diffi cile infection including toxin enzyme immunoassays for C. diffi cile toxins A and B detection (DUO Toxin A&B; VEDA.LAB, France), and glutamate dehydrogenase (GDH) for cell wall antigen detection (C. DIFF Quik Chek Complete®; USA). Results During the study period, 335 stool specimens were evaluated. Results obtained with the two-stage immunoassay tests are shown in Figure 1 . All infected patients were treated with metronidazole or vancomycin. Following a course of therapy, 2% of the infected patients had recurrence or relapse. The crude mortality rate was 17%. Conclusion GDH antigen was positive in 12% of the stool specimens received from ICU patients with suspected C. diffi cile-associated infections. The majority of these specimens (51.4%) produce both C. diffi cile toxins A and B, whereas toxin B is produced in 31.4% and toxin A in the remaining 17.2%. Thirty-seven arterial lines returned no growth (77.08%). Seven cultures grew organisms likely to be contaminants (14.58%). Four cultures grew signifi cant organisms (yield of 8.33%). There were two cases with documented clinical signs of catheter-related local infection (CRLI) at the arterial line puncture site. In one case of CRLI the primary source of infection was felt to be remote from the arterial line. The second represented a local infection with organisms that are typically skin commensals. Of the four cultures likely to represent invasive pathogens, three had clinical suspicion that the primary source was a site remote from the arterial line. In two of these cases this was confi rmed by growing the same organism at an alternative site more likely to be the source of infection. We recruited 60 patients into the randomized trial; 29 patients into the observational study. Recruitment was halted before the end of the study because of diffi culty in recruiting patients. Markers of infl ammation and all clinical outcomes were comparable between placebo and antifungal treatment groups at baseline and overtime. At baseline, TNFα levels were higher in the VAP with Candida compared with the observational group (mean ± SD) ( [1] . Interindividual variability in PK parameters was ascribed to an exponential model according to the equation: θj = θp × exp(nj), where θj is the estimate for a pharmacokinetic parameter in the jth patient, θp is the typical population PK parameter value (ka, CL/F, V/F), and n is a random variable from a normal distribution with zero mean and variance ω 2 . Residual variability was estimated using additive and additive-proportional error models; Cij = Cj + εadd and Cij = Cj(1 + εp) + εadd, where Cij and Cj are observed-predicted concentrations for the jth patient at time i, respectively, and ε is the error, a random variable with a normal distribution with zero mean and variance σ 2 . Bayesian estimates were obtained and the pharmacokinetic parameters Cmax, Tmax and AUC0-24 hours were calculated. Results Cmax of PZA was above the recommended concentration (>20 mg/l). For RIF the Cmax was below the recommended level (>8 mg/l), and the Cmax of INH was below the recommended levels (>3 mg/l). See Table 1 . Conclusion Large interindividual pharmacokinetic variability and concentrations below the recommended levels for RIF and ISO. We need to monitor drugs and to re-evaluate the doses. Introduction Staphylococcus epidermidis (SE) is the most often isolated species of coagulase-negative staphylococci, which are recognized as one of the main causes of ICU infections [1] . In this study we aimed to study the resistance profi le of SE clinical isolates against last-line antibiotics (vancomycin (VA), teicoplanin (TEC), linezolid (LZ) and daptomycin (DA)) for treating CNS infections, during an 8-year period. Methods From January 2005 until December 2012 we examined 518 nonduplicated SE isolates recovered from blood cultures of 421 patients hospitalized in a surgical ICU of our hospital. Species identifi cation and susceptibility testing were performed using the automated VITEK II system (Biomerieux). Additionally we used the E-test method (Biomerieux, ABI-Biodisk) in order to confi rm some isolation resistances against TEC and LZ found by the VITEK II system and to estimate the MIC levels of DA and VA. Mueller-Hinton agar adjusted to contain physiologic levels of free calcium ions (50 μg/ml) was used when testing DA susceptibility. Isolates with MIC >4 mg/l were considered resistant to TEC and LZ and those with MIC <1 mg/l and MIC <4 mg/l susceptible to DA and VA, respectively. Results The percentage resistance rate of the examined SE isolates is shown in Table 1 . Methicillin resistance was observed with an overall prevalence of approximately 84.6%. All of the resistant isolates to TEC and LZ were also resistant to methicillin. The MIC values of VA were lower than 2 mg/l (Table 1) . Conclusion The examined SE isolates present a scattered resistance to TEC and they show a remarkable continuing increase of resistance to LZ. These fi ndings enforced the necessity to take the appropriate measures in the ICU environment and during the clinical practice to limit the dissemination and the amplifi cation of these resistances. DA and VA possess an excellent in vitro activity against SE isolates and they could be very good alternative solutions for treating ICU infections caused by this species. Introduction Patient-to-patient transmission enables vancomycinresistant enterococci (VRE) outbreaks. Outbreak management is expensive and time consuming, and therefore the possibility of VRE eradication is desirable. Since vancomycin is scarcely absorbed in the gastrointestinal tract, treatment with vancomycin per os may result in very high gastrointestinal concentrations (many times the minimum inhibiting concentration (MIC)). The purpose of this study is to measure in vivo gastrointestinal concentrations of vancomycin in patients that are treated with a standard dose orally, and to investigate in vitro whether vancomycin is able to kill VRE at concentrations up to 2,000 times the MIC. Methods The faecal vancomycin concentration was measured in eight patients who suff ered a Clostridium diffi cile infection and were treated with 4×500 mg vancomycin orally per day. In vitro, a (1:2) dilution series of vancomycin (range 6,250 to 0.4 μg/ml) was created and 1 ml vancomycin solution was then added to 1 ml standardized inoculum. One vancomycin-susceptible enterococcus isolate (VSE, MIC = 3 μg/ ml) and two VRE isolates (MIC = 16 μg/ml) were studied. After 1, 7 and 14 days incubation at 35°C, growth was defi ned as macroscopic visible turbidity. To test for surviving bacteria, all inocula were cultured to sheep blood agar plates, which were read after 24-hour incubation at 35°C. E-tests to measure MIC were performed on relevant samples. The in vitro experiment was performed twice. Results The faecal vancomycin concentration in patients treated orally with vancomycin was 8,000 μg/ml on average. VSE growth at day 14 was detected at up to 1.5 μg/ml vancomycin, whereas VRE growth was detected at up to 98 μg/ml. The MIC of these VRE species growing at 98 μg/ml vancomycin was increased (≥256 μg/ml). For both VSE and VRE, surviving bacteria were detected at very high concentrations of vancomycin (>98 μg/ml): the MIC of these survivors was not increased. Conclusion Oral treatment with vancomycin results in extremely high faecal concentrations. At these high concentrations, VRE bacteria are killed in vitro; however, a minority of the VRE is able to survive. Vancomycin thus seems unsuitable for eradication. However, high concentrations of vancomycin dramatically reduce the bacterial VRE load. Therefore oral treatment with vancomycin may help to terminate VRE outbreaks: a dramatic reduction in bacterial load of the colonised patient will minimise the risk of patient-to-patient transmission. Introduction The optimal duration of antibiotic treatment in critically ill patients remains a subject of debate. In our multidisciplinary ICU, a short course of antibiotic monotherapy (5 to 7 days) is generally used as Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 a strategy to treat bacteraemia, unless specifi cally indicated otherwise (for example, endocarditis, osteomyelitis). We aimed to determine the impact of this strategy on antibiotic resistance patterns and patient outcomes compared with a similar exercise we conducted in 2000 [1] . Methods We conducted a retrospective study of all patients with bacteraemia or fungaemia (community-acquired, hospital-acquired, and ICU-acquired) treated in our university hospital ICU over a 6-month period (December 2012 to May 2013). We compared this against data from blood culture-positive patients admitted between February and July 2000. Information was collected on bacteraemia episodes, causative pathogens, antimicrobial resistance patterns, antibiotic use and duration, and patient outcomes. Notably, our ICU admits many immunosuppressed patients (for example, haemoncology). Results Table 1 presents demographics and incidence of bacteraemia. Antimicrobial resistance remained low in the 2013 cohort with few multi-resistant Gram-negative organisms, few fungaemia episodes and a marked decrease in methicillin-resistant Staphylococcus aureus (MRSA) (Figure 1 ). The number of relapses and breakthrough bacteraemias remained low. Conclusion A strategy of short-course antibiotic monotherapy is associated with low breakthrough and relapse rates and a low rate of antibiotic resistance. Community-acquired bacteraemia 57%; 6 (5 to 6) 65% 5 (3 to 5) Hospital-acquired bacteraemia 78%; 6 (5 to 8) 63%; 5 (4 to 7) ICU-acquired bacteraemia 80%; 5 (5 to 7) 62%; 4 (3 to 6) Introduction The administration of timely and appropriate antibiotic therapy is a well-known prognostic factor among severe sepsis patients [1] [2] [3] [4] . The purpose of this study is to describe the magnitude of the impact of early and appropriate empirical antibiotic therapy on hospital mortality. Conclusion Contrarily to what has been described previously, early and appropriate empirical antibiotic therapy was not associated with better prognosis. The most probable explanation is the higher compliance found with the current recommendations, reinforcing the need for period audits and feedback to the team. References Introduction The Surviving Sepsis Campaign (SSC) has developed guidelines to promote evidence-based management for patients with severe sepsis [1] . Improvements in bundle compliance have been demonstrated over time, but compliance remains below 40%. Sepsis has been studied in acute and critical care environments, but little research has focused on the management in level 1 wards. An initial audit of patients with sepsis who were referred to the GSTT critical care outreach team revealed very low overall compliance to the SSC 3-hour bundle. A novel quality improvement campaign was instituted with the aim of improving bundle compliance. Methods A retrospective cohort study in a university hospital was performed. Patients on level 1 wards with severe sepsis registered in the adult critical care response team (CCRT) database in November 2012 were identifi ed (Cohort A). Physiological observation, track and trigger scores, compliance with the 3-hour bundle elements (measured lactate, blood cultures before antibiotics, fl uid challenge, early antibiotics), antimicrobial stewardship and 28-day mortality were recorded. Following this, a quality improvement project was initiated: central to this was an electronic 'SEPSIS' order set, containing appropriate investigations and a step-by-step management guide for use on level 1 wards. A 'viral' print and social media campaign were also undertaken. Compliance to the SSC early care bundle was re-examined in two cohorts of patients in July 2013; patients that were referred to the CCRT as before (Cohort B) and also patients who had the electronic order set activated (Cohort C). The mean age of all patients studied (n = 79) was 66.5 years. Fifty-three per cent of the patients were male. Thirty-one per cent were in septic shock at the time of sepsis identifi cation. Overall SSC bundle compliance was 6.60% (Cohort A), 24% (Cohort B) and 45.5% (Cohort C). Improvements in other bundle parameters were also seen, including blood culture (54%, 72%, 91%), antibiotic administration (50%, 69%,76%) and fl uid administration in septic shock (50%, 42%, 75%) in Cohort A, Cohort B and Cohort C respectively. Conclusion Baseline compliance with the SSC 3-hour bundle on level 1 wards was very low. An electronic sepsis order set was associated with marked improvement. Novel quality improvement methodology may be important to achieve optimal compliance with evidence-based guidelines and an electronic sepsis order set is recommended. Reference Introduction Lung and cardiac operations cause signifi cant changes in the fl uid balance, and thus have a high incidence of development of postoperative acute kidney injury (AKI). The renal resistive index (RRI) calculated by the pattern of the renal artery fl ow is an indicator of renal artery fl ow. In this research work, we aimed to evaluate the effi ciency of the RRI on the early prediction of postoperative kidney injury in major lung and cardiac operations. Methods Twenty-two patients who have undergone lung or cardiac surgery were included in the study. After the kidneys were localized by ultrasonography, the best regions of blood fl ow were detected using color Doppler and then the arterial waveforms of these regions were obtained and optimized by Doppler. The measurements taken from three diff erent regions were averaged. The RRI was calculated at the preoperative and postoperative fi rst and 24th hours respectively. Results A signifi cant correlation was established between RRI and postoperative creatinine levels (P <0.01). RRI values reached their highest point at the postoperative fi rst day whereas the creatinine levels reached their highest level at the postoperative third day. Although there was no correlation between postoperative creatinine level and duration of staying in hospital, a signifi cant relationship was detected between duration of staying in ICU and the creatinine levels (P <0.01). When the cases were divided into two groups as RRI is less (n = 13) and larger (n = 9) than 0.7, signifi cant diff erences were present with regard to age and creatinine levels. Conclusion The RRI, which is used to evaluate renal arterial fl ow, is directly related with increasing renal vascular resistance in the case of AKI. The usefulness of RRI for prediction of AKI was shown both clinically following a renal allograft and experimentally in the acute tubular necrosis modeling [1, 2] . Also in septic patients this was asserted, as RRI is a better marker than cystatin C, which is one of the popular markers of recent times for prediction of development of AKI [3] . Our results show that RRI could be a simple, non-invasive and useful technique for early diagnosis of AKI in patients undergoing major operations such as lung and cardiac surgeries. The results from 24 day 0 samples identifi ed statistically signifi cant diff erences between SIRS, AKI, AKI + SIRS and healthy controls amongst: CD8 + cytotoxic T cells, CD45 -CD25 +++ regulatory T cells, and CD45 -CD25 ++ cytokine secreting non-T-regulatory cells ( Table 1 ). The percentage of CD69-positive neutrophils was signifi cantly increased across all three groups relative to controls, with little variation between AKI, SIRS and AKI + SIRS patients. Introduction Approximately 50% of acute kidney injury (AKI) is associated with sepsis. Neutrophil gelatinase-associated lipocalin (NGAL) and cystatin C are the two most widely used biomarkers for AKI. However, these two markers are also aff ected by the systemic infl ammatory response, and their diagnostic value in sepsis-induced AKI is disputed [1, 2] . Unlike clinical AKI, animal models can be used to explore single etiology. The purpose of this study is to examine the relationship between infl ammatory mediators and biomarkers for AKI in a sepsis model in rats. Methods Sepsis was induced by cecal ligation and puncture (CLP) in 60 adult SD rats and then observed for AKI and survival. Blood and urine samples were collected at baseline, and 18, 22, and 48 hours after CLP. AKI severity was assessed by RIFLE criteria (creatinine only). The associations between plasma IL-6 and plasma NGAL, plasma cystatin C, urine NGAL and urine cystatin C were analyzed. The area under the receiver-operator characteristic curves (AUC) was used to evaluate the diagnostic capability between severe AKI (RIFLE-I or RIFLE-F) and no AKI (includes RIFLE-R) for diff erent biomarkers. The changes of plasma NGAL, plasma cystatin C, urine NGAL and urine cystatin C with time were similar to the changes of plasma IL-6. However, only plasma NGAL levels were closely correlated with levels of plasma IL-6 (R 2 = 0.36, P <0.05). The analysis for plasma cystatin C, urine NGAL and urine cystatin C at 22 hours for severe AKI showed AUCs of 0.78, 0.71 and 0.75 respectively (all P <0.05), and the AUC for plasma NGAL was 0.62 (P = 0.11). There were no signifi cant diff erences in plasma NGAL at 22 hours between severe AKI and no AKI (2,143.32 vs. 2,077.02 U/ml, P = 0.21). Conclusion In this animal model of CLP sepsis, plasma NGAL levels were aff ected by the systemic infl ammatory response, and did not discriminate for AKI. Urine NGAL, plasma cystatin C and urine cystatin C were able to diff erentiate severe AKI from no AKI in CLP sepsis. References serum creatinine [1] . However, serum creatinine is an insensitive and nonspecifi c biomarker [2] . This study was designed to investigate whether a correlation exists between urinary oxygen tension (UOT) and early markers of AKI. The aim was to evaluate whether UOT could provide warning signs of an insuffi cient renal oxygen supply, which can lead to postoperative AKI. Methods Fourteen subjects undergoing cardiac surgery with CPB were included in this prospective clinical pilot study. UOT was measured perioperatively in all patients, both in the operating room (before, during and after CPB) and in the ICU. Biomarkers of AKI in blood and urine were measured preoperatively and postoperatively at 3, 6, 12 and 24 hours after the initiation of CPB. These included serum creatinine and the early urinary biomarkers kidney injury molecule-1 (KIM-1), neutrophil gelatinase-associated lipocalin (NGAL) and cystatin C. Student's t tests and Mann-Whitney tests were used to compare continuous variables. There was a signifi cant decrease in UOT between the start of CPB (138.44 ± 22.19 mmHg) and the lowest UOT during CPB (107.70 ± 23.28 mmHg) (P = 0.001). Dividing the subjects into two groups according to the Acute Kidney Injury Network (AKIN) classifi cation, no signifi cant diff erences were found in mean UOTs between the group of patients with a normal kidney function (n = 7) and the group with AKIN stage 1 or 2 (n = 7). For KIM-1, a signifi cant diff erence between the two groups was found at 3 hours (P = 0.041) after the initiation of CPB. Further, for NGAL a signifi cant increase in biomarker concentrations compared with the preoperative value was observed in the group with an AKIN stage 1 or 2 at all diff erent postoperative time points (3 hours (P = 0.013), 6 hours (P = 0.003), 12 hours (P = 0.009) and 24 hours (P = 0.003)). On the contrary, there were no signifi cant increased urinary NGAL levels measured in the group with the normal kidney function. Conclusion This pilot study was not able to demonstrate any association between perioperatively measured UOT and markers of postoperative AKI. Additional laboratory and clinical studies will be necessary to further defi ne the relationship between the UOT and new biomarkers of AKI. References Introduction Acute kidney injury (AKI) in surgical critically ill patients is an independent risk factor for early mortality. Two novel urine biomarkers, insulin-like growth factor-binding protein 7 (IGFBP7) and tissue inhibitor of metalloproteinases-2 (TIMP-2), may help to detect clinically silent episodes of AKI in the golden hours prior to irreversible damage of the kidney. We evaluated the early predictive value of these biomarkers for AKI, moderate and severe AKI, early requirement of renal replacement therapy (RRT), and ICU mortality, with a cutoff value of IGFBP7/TIMP-2 >0.3. Methods Four to six hours after admission to the surgical ICU, urine biomarkers were prospectively evaluated in all patients with present exposures and susceptibilities for AKI according to the KDIGO recommendation. The incidence and severity of AKI (KDIGO 2012) and requirement of RRT were assessed over 48 hours after admission. In addition, ICU mortality and variables such a norepinephrine dose, mean arterial pressure, hemoglobin level, cumulative fl uid balance and urine production were noted at the time of biomarker evaluation (4 to 6 hours) and for the fi rst 24 hours after admission. The predictive values of biomarkers were better for early prediction than clinical parameters such as urine output within the fi rst 6 hours after admission to ICU. The study included 44 patients in the citrate group versus 61 in the heparin group. We found no statistical signifi cant diff erences for: age (P = 0.06); SAPS II (P = 0.28); SOFA (P = 0.19); the timing of beginning of the technique (P = 0.61), with 34.8% versus 47.5% of patients in R (RIFLE), 27.8% versus 18% in I (RIFLE) and 16% versus 21% in F (RIFLE); duration of the technique (P = 0.74) and length of stay. Although we noticed a greater loss of dose and absolute creatinine clearance in the citrate group, this had no statistical signifi cance (P = 0.18 and P = 0.13). The mortality found for citrate and heparin groups was 60.4% and 39.4% respectively. The diff erences with statistical signifi cance related to dialitrauma emerged in K + (P = 0.03), Ca 2+ (P = 0.02), Na + (P = 0.004), platelets (P = 0.002), pH (P = 0.02) and bicarbonate (P = 0.0001). Conclusion We may say that there are functional diff erences that must be taken into account. Despite not having statistical signifi cance on this sample, losses of dose and creatinine clearance showed a direct relation with mortality. Reference and mean aortic cross-clamping of 98 minutes (range 25 to 190). We measured MyG, procalcitonin (PCT), and creatinin (sCr) at ICU admission and, if serum MyG was higher than 600 mg/dl, the patient was treated with CVVHD-EMIC2-citrate anticoagulant within 12 hours of ICU admission for 72 hours and a dose of 2,000 ml/hour. Biochemical assays were obtained at 12, 24, and 72 hours and at ICU discharge. Results The pretreatment MyG median value was 10,789 ng/ml; it signifi cantly reduced on average 92.8% during CVVHD (see Figure 1 ) and it remained low at ICU discharge (median value 114 ng/ml). sCr remained stable (average time value equal to 0.94 mg/dl) during CVVHD; PCT also decreased over time with a reduction rate equal to 78% (from 5.35 ± 4.39 mg/dl to 1.23 ± 1.09 mg/dl at the end of CVVHD). Finally, six patients survived at 90 days. Conclusion This small experience confi rms that serum MyG is likely to increase in post-cardiac surgical high-risk patients and suggests a benefi cial eff ect of CRRT treatments with EMIC2 membranes and citrate on serum MyG, potentially preventing AKI. Further larger assessment can be advised for confi rmation. Reference Introduction Rhabdomyolysis is characterized by breakdown of striated muscle due to a great number of causes. Acute kidney injury (AKI) is a common complication as a consequence of high concentrations of circulating myoglobin (Mb). The AKI degree can vary but often requires dialysis, a condition which drastically worsens the ICU stay and prognosis. Since Mb overconcentration represents the cause of AKI, one of the therapy's aims should be its removal to prevent further kidney damage and to allow faster renal recovery. Both intermittent hemodialysis and high-volume CVVHF are poorly eff ective in removing Mb, while smallprotein leakage membranes seem to be promising in this setting. The aim of our study was therefore to measure effi cacy of Mb removal of a new high cutoff membrane (EMIC2; Fresenius, cutoff value 40 kDa) for continuous renal replacement therapies (CRRT) in the ICU setting. Methods We report results of EMIC2-based treatments in seven patients (four male/three female) with diff erent causes of rhabdomyolysis (trauma, sepsis, limb ischemia). Five patients had classic dialysis indications (persistent anuria) while in two patients treatment was prophylactically started. CRRT were delivered in CVVHD mode with the EMIC2 dialyzer and with loco-regional trisodium-citrate anticoagulation. Mb plasma levels were assessed each 12 hours while the removal rate, total body and dialyzer clearances were estimated by kinetic modeling as previously described [1] . Clinical data were also collected and both global and renal patient survival was reported. Results The median Mb value at CRRT start was 6,971 ng/ml (range 4,679 to 48,011 ng/ml). CRRT were delivered with an average blood fl ow rate of 143 ± 45 ml/minute and a dialysate fl ow rate of 2,134 ± 1,334 ml/hour. These operating conditions allowed one to stop treatment on average after 75 ± 47 hours (median 54 hours) with a Mb reduction of 82.2% (range 99.4 to 44.4%). Overall median Mb removal per treatment was 59 mg (range 33 to 279 mg) mainly due to the fi rst 24 hours of treatment (54 mg, range 20 to 187 mg). Only two patients had residual renal function that was in one case measured to account for only 7.45 mg Mb removal during the entire treatment. Six patients survived and recovered renal function with no dialysis need at present follow-up. One patient died during the ICU stay. Conclusion Our data measured high performance of the EMIC2 membrane in Mb removal and confi rm theoretical models indicating that CRRT with a high cutoff membrane can achieve major Mb removal within 24 hours with great superiority in comparison with all other available techniques. Reference Introduction Removing the middle molecular weight substances including cytokines and albumin-bound toxin could be eff ective for patients with acute liver failure (ALF). We have developed a new system, plasma fi ltration with dialysis (plasma diafi ltration (PDF)) [1, 2] , and assessed its effi cacy in multicenter analysis. Methods A subgroup analysis of an observational study conducted in the ICUs of six hospitals. In PDF, simple plasma exchange is performed using a selective membrane plasma separator (Evaxclio EC-2A; Kawasumi Chemical Inc., Tokyo Japan), which has a sieving coeffi cient of 0.3 for albumin, while the dialysate fl ows outside the hollow fi bers. The fl ow rate of the blood, dialysate, substitute and additional substitute was 80 to 100 ml/minute, 600 ml/hour, and 0 to 450 ml/hour according to the rate of water elimination and 150 ml/ hour, respectively. As the substitute from the additional fl uid line, we added 1,200 ml (150 ml/hour) of fresh frozen plasma followed by 50 ml of 25% albumin considering the loss of albumin by diff usion. As an anticoagulant, nafamostat mesilate (Torii Pharmaceutical Co. Ltd, Tokyo, Japan) was used at a rate of 15 to 25 mg/hour. Results A multicenter study was underway from October 2005 to August 2011. We performed PDF on 65 patients with ALF (severe sepsis, 22; post operation, 15; fulminant hepatitis, 11; alcohol hepatitis, 3; graft versus host disease, 4; and others, 10). The serum total bilirubin, plasma PT-INR and the model for end-stage liver disease (MELD) score before the PDF procedure were 15.0 ± 8.15 mg/dl (average ± SD), 2.3 ± 1.5 and 35.8 ± 9.3, respectively. PDF was performed as 9.2 ± 13.2 sessions per patient and the overall 28-day survival rate was 68.5%. According to the severity of the MELD score, we stratifi ed patients into three categories defi ned by the MELD score. The numbers of patients were 15 (23%) in score 20 to 29, 30 (46%) in score 30 to 39 and 19 (29%) in score over 40. Introduction Acute liver failure (ALF) is a critical illness with high mortality. Plasma diafi ltration (PDF) is a blood purifi cation therapy in which simple plasma exchange is performed using a selective membrane plasma separator while the dialysate fl ows outside the hollow fi bers. While several studies demonstrated that PDF therapy is a useful blood purifi cation therapy for patients with ALF, PDF therapy is often diffi cult to employ in ALF patients complicated with multiple organ failure, especially in those with unstable hemodynamics. Furthermore, it is likely to re-occur immediately after PDF therapy. We developed continuous PDF (CPDF) as a new concept in PDF therapy, and assessed its effi cacy and safety in ALF patients compared with conventional plasma exchange (PE) plus continuous hemodiafi ltration (CHDF) therapy in this study. Methods Ten ALF patients (gender: male/female = 6/4, age: 47 ± 14) employed CPDF therapy. The primary outcomes were altered liver function, measured by the model for end-stage liver disease (MELD) score, and total bilirubin and prothrombin time International Normalized Ratio (PT-INR), 5 days after CPDF therapy. Secondary outcomes included Sequential Organ Failure Assessment (SOFA) scores, 5 days after CPDF therapy, and the survival rate 14 days after this therapy. Results The MELD score (34.5 to 28.0; P = 0.005), total bilirubin (10.9 to 7.25 mg/dl; P = 0.048), PT-INR (1.89 to 1.31; P = 0.084), and SOFA score (10.0 to 7.5; P <0.039) were improved 5 days after CPDF therapy. Nine patients were alive and one patient died due to acute pancreatitis, complicated by ALF. The effi cacy of CPDF therapy for maintaining liver function and renal function was not inferior to PE plus CHDF therapy. Parameters of renal function such as the creatinine value were also improved 5 days after CPDF therapy. Circulation parameters such as mean arterial pressure and heart rate were maintained without inotropic and vasopressor support during the CPDF treatment period. The oxygenation index (PaO 2 /FiO 2 ) as a measure of pulmonary function tended to increase after this treatment. We could employ this treatment without any adverse events, such as infections and unstable hemodynamics. Conclusion In the present study, CPDF therapy safely supported liver function and generally improved the condition of critically ill patients with ALF. Introduction It has been demonstrated that blood purifi cation therapy performed by means of venovenous hemodialysis with high cutoff membranes (HCO-CVVHD) may modulate the host infl ammatory response in septic patients with acute kidney injury (AKI), potentially limiting organ dysfunction. Improvement in hemodynamics and respiratory function has been described during HCO-CVVHD treatment [1] . The Sepsis in Florence sTudy (SIFT) has been designed to evaluate changes in infl ammatory biomarkers and tissue oxygenation/perfusion indexes in septic ICU patients with AKI during HCO-CVVHD. Methods Patients with microbiologically confi rmed severe sepsis/ septic shock and AKI (RIFLE criteria F or more) treated with HCO-CVVHD, started within 12 hours from the diagnosis, were prospectively included in the study. The cumulative vasopressor index (CVI), C-reactive protein levels (CRP), serum lactate concentration (Lac) and central venous oxygen saturation (ScvO 2 ) were measured before (T0h) and at 24 hours and 48 hours after HCO-CVVHD initiation. Data are expressed as the median (range). The Mann-Whitney U test was applied to detect diff erences in CVI, CRP, Lac and ScvO 2 at the three time points (statistical signifi cance for P <0.05). Results In 16 ICUs, a total of 16 patients (six cardiac surgery, four abdominal surgery and six medical) met the inclusion criteria and were enrolled in the study. A signifi cant reduction in CRP levels was observed over time: 263 (216 to 358) mg/dl at T0h to 153 (56 to 186) mg/dl at T48h (P <0.05). ScvO 2 signifi cantly increased from 45 (40 to 55)% at T0h to 75 (68 to 77)% at T48h (P <0.05). Finally, serum lactate decreased from 5.1 (3.0 to 9.5) mmol/l at T0h to 1.6 (1.0 to 4.6) mmol/l at T48h (P <0.05). Conversely, CVI did not signifi cantly reduce over time (8.2 (4 to 9) at T0h vs. 4.5 (4 to 8) at T48h, P >0.05). Conclusion Our preliminary data show that patients with sepsisrelated AKI may benefi t from early treatment with HCO-CVVHD. The modulation of proinfl ammatory and anti-infl ammatory mediators, as previously demonstrated [1] , may improve microcirculation, tissue perfusion and cellular oxygenation. Although promising, our results must be confi rmed at the end of the study with larger observations. Finally, a subgroup analysis is absolutely mandatory in order to explore diff erent behaviors of tissue perfusion indexes in diff erent populations of patients. Introduction Little information is available regarding ciprofl oxacin pharmacokinetics and pharmacodynamics in sepsis patients receiving sustained low-effi ciency dialysis (SLED). This study determined the pharmacokinetics and simulated pharmacodynamics of ciprofl oxacin in ICU patients during SLED. Methods This study was a prospective evaluation of ciprofl oxacin pharmacokinetics in patients with sepsis and >18 years of age, urine output <200 ml/day and receiving SLED for at least 8 hours. Following informed consent, plasma samples were collected at baseline and 1, 2, 4, and 8 hours after a ciprofl oxacin 400 mg dose i.v. during SLED and post-SLED therapy at the same times. Dialysate samples were collected at 4-hour intervals during SLED. Pharmacokinetic parameters were determined using WinNonlin and compared between the two periods. Simulated pharmacodynamic parameters were determined for Pseudomonas aeruginosa using MIC = 2. Results A total of seven patients (four male, three female, age 56.9 ± 7.6, APACHE II 26.8 ± 2.4) were enrolled. Ciprofl oxacin was cleared relatively rapidly with a half-life of 6.9 hours and a Ke of 0.108/hour during SLED compared with 11.9 hours and 0.057/hour post-SLED (P <0.05). Simulated pharmacodynamics demonstrated inadequate coverage for P. aeruginosa during SLED with Cmax/MIC ratio 5.7 ± 1.2 and AUC/MIC 77.5 ± 22.3. Conclusion Ciprofl oxacin is rapidly cleared during SLED similar to clearance during normal renal function, which may result in adequate pharmacodynamic coverage for some pathogens. Introduction Antibiotic dosing for patients with acute renal failure receiving continuous renal replacement therapy (CRRT) is a clinical challenge. The aim of this study was to investigate the pharmacokinetics of meropenem (M) during CRRT. Methods A prospective and multicenter study was conducted at seven hospitals. Fifteen critically ill patients undergoing either continuous venovenous hemofi ltration (CVVHF) or hemodiafi ltration (CVVHDF) were included. Serum and ultrafi ltrate (UF) levels of M were determined by liquid chromatography. Blood samples were drawn 24 hours after starting CRRT at 08:00 a.m., 09:00 a.m., 10:00 a.m., 01:00 p.m., 06:00 p.m., 20:00 p.m. and 08:00 a.m. of the following day. CRRT clearance (Cl), total amount of M in the UF (MUF), percentage of the dose extracted by CRRT (EF) and the AUC (ng/hour/ml) were calculated. Results Nine patients were treated with CVVHDF and six with CVVHF. M (0.5 to 2 g) was administered every 6 to 12 h by i.v. infusion over 15 minutes. Data (mean and SD) concerning the dialysate fl ow rate (DF; ml/hour), blood fl ow rate (ml/minute) and the average UF rate (ml/ kg/hour) for CRRT techniques are shown in Table 1 . Pharmacokinetic Introduction Acute kidney injury (AKI) in the critically ill is an independent risk factor for adverse outcome [1] . Previously, it was suggested that high-volume haemofi ltration (HVHF) may confer a mortality benefi t and lead to a reduction in organ failure compared with standard ultrafi ltration rates (UF) [2] . This was not confi rmed by a recent investigation [3] . It has also not been determined whether ideal Introduction Sepsis-induced immunosuppression has long been considered a factor in the late mortality of sepsis patients, but little is known about the immunity of immunocompetent cells and the eff ect of polymyxin B-immobilized fi ber hemoperfusion therapy (PMX-DHP) on sepsis-induced immunosuppression. The present study was designed to evaluate the eff ect of PMX-DHP on recovery from sepsisrelated immunodefi ciency. Methods Patients with septic shock who were treated with PMX-DHP were enrolled in this study. Study 1: (1) numbers of peripheral lymphocytes and CD4 + T cells, especially regulatory T cells (Tregs), and serum cytokine levels were examined to evaluate the eff ects of PMX-DHP in septic shock patients. (2) Peripheral blood mononuclear cells (PBMCs) in these patients were examined to evaluate infl ammatory cytokine production before and after PMX-DHP. The obtained PBMCs were stimulated with interleukin (IL)-2 and IL-12, anti-CD3 antibody, or lipopolysaccharide for 24 hours, and tumor necrosis factor alpha and interferon-gamma (IFNγ) production in the culture supernatants was measured using enzyme-linked immunosorbent assay. Study 2: whole blood from patients with sepsis was incubated with a polymyxin B-immobilized fi lter (cut into small sizes) for small animals for 2 hours (PMX group), or were treated with 200 μg polymyxin B for 2 hours (PLB group), or were not treated (sepsis group). IFNγ production by PBMCs was compared among the three groups. Results Study 1: (1) the number of CD4 + T cells was lower and the percentage of Tregs in CD4 + T cells was higher in septic shock patients compared with those without shock. A signifi cant increase in the number of CD4 + T cells, a signifi cant decrease in the percentage of Tregs in the CD4 + T-cell population, and a signifi cant decrease in serum IL-10 levels were observed 24 hours after PMX-DHP in septic shock patients who survived compared with those who did not. (2) IFNγ production by PBMCs was signifi cantly lower in patients with sepsis than in healthy volunteers. IFNγ production by IL-2-stimulated and IL-12-stimulated PBMCs signifi cantly increased after PMX-DHP therapy. Study 2: IFNγ production by PBMCs in patients with sepsis increased signifi cantly in the PMX and PLB groups compared with that in the sepsis group. Conclusion PMX-DHP directly decreased the number and percentage of Tregs in peripheral blood circulating CD4 + T cells in patients with septic shock. PMX-DHP improved IFNγ production by natural killer (NK)/NKT cells in patients with septic shock. Therefore, PMX-DHP could improve sepsis-related immunosuppression. [1, 2] . The Actigraph device is a sleep watch that has been shown to have equivalent accuracy to polysomnography and previously used during critical illness to show sleep disruption [3] . Our objective was to assess patients long-term sleep quality using the Actigraph device. Methods Study patients were selected from a 24-bed multidisciplinary ICU. Thirteen patients who were ≥18 years old, stayed longer than 4 days in the ICU and did not have and acute brain injury were followed up at 2 months post hospital discharge. The Actigraph device was given to patients to take home and worn for 72 hours. Previously validated algorithms were used to analyze sleep and wake cycles [4] . Additionally, patient completed the Pittsburgh Sleep Quality Index (PQSI), as a measure of subjective sleep quality. Results Sixty-two percent of patients at 2 months post hospital discharge reported poor sleep quality as per the PSQI. The Actigraph results showed patients' average total sleep time was 6.15 hours, with a sleep effi ciency of 78%. The mean time to fall asleep was 12 minutes. Patients had an average of 11 awakenings per night and were awake for an average of 7 minutes during the awakenings. There were no associations found between patients' perceived sleep quality and total sleep, sleep effi ciency or sleep disruptions. Patients' severity of illness, as measured by the APACHE II score, was statistically associated with lower total sleep time (β = -12.6, P = 0.019), reduced sleep effi ciency (β = -1.18, P = 0.042) and higher number of sleep disruptions (β = 0.64, P = 0.023). The number of days ventilated or ICU and hospital length of stay were not statistically associated with the Actigraph sleep parameters. Conclusion Survivors of critical illness have high levels of sleep dysfunction as measured by actigraphy. Patients' severity of illness while critically ill appears to increase the level of long-term sleep dysfunction experienced. There is discordance between objective and subjective measures of sleep quality, which has been shown previously [5] . Objective measures of sleep quality are needed on a larger number of patients to confi rm these fi ndings. Withdrawal Assessment Tool-1 (WAT-1) [1] to evaluate children during weaning from analgesics and sedatives. The patient is diagnosed with withdrawal syndrome when the score is 3 or >3. We compared the subjects who ever had a score over 3 and those with lower scores and assessed the risk factors and outcome of withdrawal syndrome between two groups. In fact, we identifi ed at least 25 epidural hematomas that occurred so far from the following countries: Belgium (n = 1), Brazil (n = 1), France (n = 1), Germany (n = 2), India (n = 2), Italy (n = 1), Japan (n = 2), Korea (n = 1), Malaysia (n = 1), Norway (n = 2), Russia (n = 3), Sweden (n = 1), the UK (n = 3), and the USA (n = 4). Even if from the public health point of view the benefi ts seem to encourage the use of epidural analgesia in cardiac surgery with a possible reduction in perioperative mortality, this topic merits further investigation and the decision to insert the epidural catheter should be discussed with the patient considering both local experience and legal dispute in case of medical complications. Introduction It has been established that early enteral nutrition in critically ill patients improves overall outcome and mortality. In our unit, feeding protocols were established based on the ESPEN recommendations and have been implemented for the last 2 years. The purpose of this study was to evaluate the compliance of our septic patients' nutritional approach with our feeding protocols. The central laboratory sodium measurement was, on average, 1.46 mmol/l more than the ICU assay, limits of agreement 1.18 to 1.74 mmol/l greater, P <0.001. Bland-Altman analysis of the central laboratory result minus the ICU sodium measurement had limits of agreement of 1.3 to -4.2 mmol/l. The correlation between the assay diff erences and total protein concentration and albumin were respectively r = 0.24 (P = 0.01) and r = 0.20 (P = 0.04). The diff erence in plasma sodium concentration between the assays increased as the plasma concentration albumin or total protein concentration decreased (respectively: r 2 = 0.04 and r 2 = 0.06). Conclusion The diff erence between indirect and direct sodium assays was found to be statistically related to the plasma albumin concentration and the total protein concentration. Although the relationship was found to be weak, the total protein concentration should be monitored when measuring sodium by indirect ion-selective electrode. Reference Introduction Water-electrolyte disturbances are one of the most common complications of acute brain injury of various origins, threatening the life of the patient and requiring timely correction. In this work we studied the structure of water-electrolyte complications in patients in the neurological intensive care with acute brain injury. Methods We analyzed 259 cases of water-electrolyte disturbances that developed in patients treated in the Department of Intensive Care of Russian Polenov's Neurosurgical Institute from 2001 to 2012. Patients were between 16 and 55 years old. A total of 142 patients were operated for brain tumor, 72 of them of basal-supratentorial localization; eight severe brain trauma; 62 of the hemorrhagic type of stroke, one herpes encephalitis. We excluded from this study the patients with heart and renal failure receiving diuretics. We measured BP, HR, CVP, hourly and daily urine output, level of K and Na in plasma, brain natriuretic peptide (BNP) one to four times a day, and levels of K and Na in urine in single and daily servings. All patients were receiving dexamethasone at a dose between 8 and 32 mg/day as an anti-edema therapy, and thus levels of ACTH and cortisol were not investigated. Underlying this is an ultradian rhythm of discrete pulses [1] as a result of the feedforward:feedback interactions between cortisol and ACTH [2] . These pulses are critical for normal function; pulsatile and constant infusions yield diff erent transcriptional responses [3] and patients on optimal (nonpulsatile) glucocorticoid replacement have twice the age-related mortality of the general population [4] . We have now characterised the ultradian rhythm and pituitary-adrenal interaction of patients undergoing coronary artery bypass grafting (CABG). Methods Twenty male patients presenting for elective CABG (on-pump and off -pump) were recruited. Blood samples were taken for 24 hours from placement of the fi rst venous access. Cortisol was sampled every 10 minutes, ACTH was sampled every hour and cortisol binding globulin (CBG) was sampled at baseline, at the end of operation and at the end of the 24-hour period. Results Cortisol and ACTH were pulsatile throughout the perioperative period and the cortisol-ACTH interaction persists (Figure 1 ). The sensitivity of this interaction (calculated by the ratio of cortisol to ACTH pulse amplitude) changed at about 8 hours post surgery such that the adrenal sensitivity to ACTH increased. Conclusion Both cortisol and ACTH remain pulsatile during and after cardiac surgery and the pituitary-adrenal interaction persists, although the sensitivity of the adrenal glands changes throughout the perioperative period. Our study shows that endogenous glucocorticoid levels reach very high oscillating levels following cardiac surgery, which not only invalidate the interpretation of point measures of adrenal function to diagnose adrenal insuffi ciency but also demonstrate that constant infusions of hydrocortisone are unphysiological. Introduction Many if not most critically ill patients are treated with insulin during their stay in the ICU [1] . Intensive monitoring of the blood glucose level is a prerequisite for effi cient and safe insulin titrations in these patients [2] . Current continuous glucose measurement techniques rely on subcutaneous glucose measurements [3] or measurements in blood [4] . We hypothesized that changes in volatile organic compound (VOC) concentrations in exhaled breath refl ect changes in the blood glucose level. Changes in VOC concentrations can be analyzed continuously using a so-called electronic nose (eNose) [5] . Our aim was to investigate exhaled breath analysis to predict changes in glucose levels in intubated ICU patients. Methods Exhaled breath was analyzed in 15 intubated ICU patients who were monitored with a subcutaneous CGM device. eNose results were compared with subcutaneous glucose measurements and linear regression models were built, including subject-specifi c models, and whole-sample models. The models were validated using temporal validation by training the model on the fi rst 75% of measurements and prospectively testing on the last 25% of measurements. Performance of the models was measured using an R 2 value, Clarke error grids (CEG) and rate-error grid analysis (R-EGA). Results Changes in VOC concentrations were associated with changes in subcutaneous glucose levels. R 2 performance had a mean value of 0.67 (0.34 to 0.98) for subject-specifi c models, and a mean value of 0.70 (0.52 to 0.96) for the model for the whole sample. However, when externally validating the model, the predictive performance dropped to a mean R 2 of 0.19 (0.00 to 0.70) for subject-specifi c models, and 0.04 for the model for the whole sample. Point accuracy in CEG was mostly good with >99% in zones A and B; trend accuracy, as visualized with R-EGA, was low. Conclusion Exhaled breath prediction of glucose levels seems promising. However, performance of the current models is too low to be used in daily practice. The device met the primary safety and eff ectiveness endpoints of the trial. The 456 sample values recorded by the monitor based on 8-hour calibrations were correlated with samples taken from the YSI and the MARD for the study was 9.40%. The analysis showed that 89.23% of the data fell within the A zone of the Clark error grid, with the rest falling within the B zone. Conclusion The results demonstrate a good correlation with the accepted standard of blood glucose determination in ICU practice. Early detection of glycemic excursions can provide carers with the opportunity for an early intervention and thus achieve the elusive target of TGC around the chosen target range. References Of the 208 patients, 82% were male and the mean age was 51 years. Hospital mortality was 8.7%. Main causes of injury were motor vehicle accidents (39.7%) and falls (43.2%). Injuries below C4 level represented 51.5% of the population. A complete loss of motor function (ASIA level A and B) was found in 34.9% of patients. The mean and median ISS score was 20.7. In total, 78 patients required MV (37.5%) and 30 patients required prolonged MV (14.4%). After multivariate analysis, four predictors of MV were identifi ed: pneumonia (OR = 52.83); ISS score >22 (OR = 4.09); age (OR = 1.02); level C1 to C4 (2.34); and two predictors of prolonged MV: ASIA score A and B (OR = 5.57) and pneumonia (OR = 8.76). Conclusion In our study ISS, cervical level and age were associated with MV but not with the need for prolonged MV, whereas pneumonia was an independent risk factor for both. This is a potentially preventable risk factor where specifi c strategies can be applied to improve patients' outcome. comparing microcirculation of brain-dead diagnosed patients and healthy volunteers. However, the presence of conjunctival fl ow in case of general cerebral fl ow is completely absent, making it diffi cult to use conjunctival fl ow as a substitute for brain fl ow. Previously we developed a model to predict increased ICP, 30 minutes in advance, using the dynamic characteristics of routinely monitored minute-by-minute ICP and mean arterial blood pressure (MAP) signals [1] . The model was developed using data from the Brain-IT database [2] . Here we present external validation results of this model, on a more recent cohort of adult TBI patients from the AVERT-IT project [3] . The fi nal data are currently under statistical evaluation, which will be completed at the time of presentation. However, there is indication of a link between brain glucose levels and CBF values, but it is not clear as to the CBF-PbrO 2 correlation that is the second part of this study under evaluation. This may be due to the fl uctuation of brain glucose because of brain ischemia, hyperemia, hypermetabolism or hypometabolism. So far we are able to establish a correlation of CBF and lactate/pyruvate ratio only in persistently low CBF values. Conclusion This will be a fi nal report of a study in human patients with severe subarachnoid hemorrhage and traumatic brain injury. The results indicate correlations of varying signifi cance between the pooled data still under statistical analysis. We hope that the outcome of our study will be able to answer questions regarding the pathophysiology of severe brain injury and guide us in the titration of therapy, as it is needed by each individual patient [1] [2] [3] [4] . Introduction Several reports indicate the potential usefulness of monitoring brain metabolic parameters and their correlation with the system [1] [2] [3] [4] . We want to establish diff erences and correlation in pCO 2 , lactate, serum sodium and C-reactive protein (CRP) between arterial and jugular venous bulb blood. Methods An observational study. Between 1 January and 31 October 2013 we included neurocritical patients (NCP) with multimodal neuromonitoring (MMN). Daily samples of arterial blood and venous jugular bulb blood were obtained for measuring pCO 2 , lactate, serum sodium and CRP. Results There were 45 NCP, six (13%) with MMN (fi ve men). Mean age was 37 ± 11 years (35 to 61). Diagnostics: two TBI, two SAH, one stroke, one lupus encephalitis. APACHE II was 27 ± 6.5 (25 to 39). Glasgow Coma Scale at admission was 14 ± 4 (4 to 14). pCO 2 (mmHg): arterial 41 ± 6.3 versus jugular 45 ± 7.4 (r = 0.7, P = 0.007). Lactate (mg/dl): arterial 11 ± 5.6 versus jugular 13.5 ± 3.9 (r = 0.7, P = 0.9). Sodium (mEq/dl): arterial 141 ± 4.5 versus 141 ± 4.4 (r = 0.8, P = 0.15). CRP (mg/dl): arterial 8 ± 7.4 versus 17 ± 11.6 (r = 0.9, P <0.001). The correlation and trend curves are shown in Figure 1 . Conclusion A suitable correlation is observed for the arterial-jugular bulb in diff erent variables. There is a signifi cant diff erence in CRP and pCO 2 values being persistently higher in the jugular, particularly for CRP. Studies are required to defi ne its interpretation and potential usefulness. Results Wernicke's area activation was observed in nine patients (eight patients with TBI, one with hypoxia). During the subsequent examination, which lasted from 3 to 12 months, seven patients with activated Wernicke's area showed further consciousness expansion up to the minimal consciousness state (further consciousness expansion was seen in two patients). Two other patients with activated Wernicke's area did not show any signs of consciousness. Two patients revealed signifi cant activity in the Broca's area. Conclusion According to the fi rst results of the study one can conclude that behind the outwardly similar clinical symptoms in patients in VS lies a diverse (due to the organization of brain functions) group of patients. fMRI enables one to reveal the fi rst signs of cognitive activity; that is, reveals the linguistic value of speech addressed to the patient which cannot be detected during routine neurological examination. Introduction A patient is declared brain dead (BD) when physicians determine permanent loss of brain functions. Unfortunately, criteria for defi ning BD vary across diff erent countries [1] . We therefore decide to survey BD diagnostic modalities in Europe in order to describe diff erences. Methods A multiple-choice questionnaire was developed on an online platform [2] . Direct link was sent to national representatives of the European Society of Intensive Care Medicine and NeuroIntensive Care section's members. Thirty-three countries were contacted. Answers were reviewed. In cases of discrepancies or missing data, participants were contacted for further clarifi cation. Descriptive statistics have been applied. Results Twenty-eight participants returned the questionnaire (85%). Every country has either specifi c law (93%) or guidelines issued by the scientifi c society (89%). Clinical examination, essential to the diagnosis, is the only requirement in 50% of countries. Coma, apnea, absence of corneal and cough refl exes are always necessary. Blood pressure and electrolytes are checked in 64% as mandatory prerequisites. The apnea test is legally defi ned in 86% of countries. Eighty-two percent of countries require achievement of a target paCO 2 level while the Netherlands' law states target apnea duration. Number of physicians (median 2, range 1 to 4), number of clinical examinations (median 2, 1 to 3), and minimum observation time (median 3 hours, 0 to 12) are variable requisites in diff erent countries. In 50% of nations, additional tests are required. Hypothermia (4%), anoxic injury (7%), inability to complete clinical examination (61%), toxic drug levels (57%), and inconclusive apnea test (54%) are legal indications to perform additional tests. Cerebral blood fl ow investigation is mandatory in 18% of countries, while it is either optional or used only in selected cases in 82%. Conventional angiography is still the preferred method (50%), followed by transcranial Doppler (43%), angioCT (39%), CT perfusion and angioMR (11%). EEG is always (21%) or optionally (14%) recorded. Russia and Croatia evaluate both EEG and cerebral blood fl ow (7%). Conclusion There are still areas of uncertainty and disparities in brain death diagnosis in European countries. This predisposes to misdiagnosis and confusion both for clinicians and families. Measures to promote uniformity of brain death procedures and clinical practice are therefore desirable. The ICU and hospital mortality was respectively 33.7% (n = 54) and 36.9% (n = 59). The mortality at 1 year was 44.4% (n = 71). The results of the neurological follow-up at 1 year were: GOS 2: 5.6% (n = 9), GOS 3: 10% (n = 16), GOS 4: 13.1% (n = 21), GOS 5: 26.9% (n = 43). Conclusion According to the other studies, our data confi rm that the severe traumatic brain injury is associated with a high mortality at 1 year. One-half of the survivors have a diff erent level of disability. Introduction Recent studies have shown that 1,25-dihydroxyvitamin D3 (vitamin D) defi ciency may aff ect negatively the clinical course of traumatic brain injury (TBI) [1] . This problem becomes important with respect to the older patient considering a 50% prevalence of vitamin D defi ciency [2] . Data from the Third National Health and Nutrition Examination Survey [3] document more than 60% of Caucasians aff ected by D defi ciency [4] so that all patients with TBI of any age are theoretically at risk of unfavorable outcome [2] . The objective of this preliminary study was to determine whether low levels of vitamin D at admission to the ICU (<24 hours) could negatively aff ect neurological recovery of patients with TBI. Methods We retrospectively analyzed the data of 46 patients aff ected by TBI (65% severe, 9.5% moderate, 28.5% moderate) both isolated or associated with other extracranial lesions. The sampling of vitamin D was carried out within 24 hours from ICU admission. We had registered GCS at the moment of presentation (GCS in) and at discharge (GCS out) and their diff erence (GCS diff ) compared with levels of vitamin D. Patients that died in the ICU were assigned a GCS out = 0. See Table 1 . Results Our data, according to other studies [5] , confi rm the presence of a defi ciency of vitamin D (Table 1) ; however, they do not demonstrate a statistical signifi cance correlation at the univariate regression (R = 0.04; P = 0.786) between vitamin D level and outcome from the ICU. There was no correlation stratifying patients for age, for TBI class, for Injury Severity Score and for BMI. Conclusion Vitamin D defi ciency is really prevalent in our TBI cases but does not seem to aff ect neurological recovery at ICU discharge; however, these preliminary results should be exposed to several criticisms and need to be confi rmed with prospective studies. Introduction Severe traumatic brain injury (TBI) is a major cause of death in people between 19 and 45 years old. Gastrointestinal dysfunction is the most common complication due to mucosal ischemia, motility disorders, and disruption of the gut barrier, with severe consequences: malnutrition, weight loss, and high risk of infections [1] . Therefore, maintaining the intestinal barrier function is a systematic engineering project. Selected new probiotics due to the capacity to bind and neutralize toxins, and to interfere with pathogen adherence, by immunomodulatory properties, mopping up the infection, could improve recovery of critically ill patients [2] . Our aim was to assess the eff ects of a new probiotic in an early enteral regimen on clinical outcome of severe TBI patients, in terms of VAP incidence, tolerance to enteral nutrition, duration of mechanical ventilation, and mortality rate. Methods A prospective randomized 1-year study of 64 patients 19 to 78 years old allocated to receive for 10 days either an early enteral diet plus a new probiotic (Bioent; Lactobacillus bulgaricus 10 trillion CFU/cp + activated charcoal) every 6 hours (Group A) or the same formula without probiotics (Group B). The diets were isocaloric and isonitrogenous, and there were no diff erences between groups in gender, age, and nutritional status. We assessed the VAP incidence, duration of mechanical ventilation, tolerance to enteral nutrition, length of ICU stay, duration of diarrhea episodes, and mortality rate. The ANOVA test and t test were carried out; P <0.05 was considered signifi cant. Results The infection rate was higher in group B, the duration of mechanical ventilation was shorter in group A, and the patients in group A received 91.7% of total caloric needs by an enteral route versus 74.68% in group B. There is a signifi cant diff erence in the number of diarrhea episodes, and the ICU length of stay was signifi cantly lower in group A; there was no signifi cant diff erence in the mortality rate between groups. See Figure 1 . Conclusion Early administration of new probiotics to severe TBI patients could have benefi cial eff ects in terms of reduction of GI dysfunction, VAP incidence and length of ICU stay. was noted (Spearman's rho = 0.566, P <0.001). The IMPACT and the APACHE II were found to identify slightly diff erent groups of patients that eventually do not survive (Figure 1 ). Conclusion The IMPACT and the APACHE II models showed equal performance for 6-month mortality prediction. A moderately strong, positive correlation, with some major discrepancies between the models, was found. Thus, features of both the IMPACT and APACHE II models are valuable for optimal outcome prediction in patients with TBI treated in the ICU. References demographic profi les of severe TBI in the local context, with implications in the management of severe TBI, particularly the utilisation of critical care resources. Results A total of 780 patients were admitted with TBI during the study period, of which 365 patients (46.8%) sustained severe TBI. The majority (75.3%) of the severe TBI patients were male. There was a bimodal preponderance of severe TBI cases in young adults (age 21 to 40) and older people (age >61). Motor vehicle accidents (48.8%) and falls from <2 m (35.1%) were the main mechanisms of injury. Invasive monitoring was frequently employed in these patients with severe TBI: arterial blood pressure monitoring in 298 patients (81.6%), central venous pressure monitoring in 219 patients (60.0%), and intracranial pressure monitoring in 173 patients (47.4%). The incidence of use of tiered therapy such as sedation, mild hyperventilation, osmotherapy with mannitol, cerebrospinal fl uid drainage, barbiturate coma and decompressive craniectomy to control ICP converged with international practices. Conclusion Young adults and older people involved mainly in motor vehicle accidents and falls respectively were among the high-risk groups for severe TBI. Management of these patients goes beyond the ICU and involves, but is not limited to, social support, emotional motivation and community reintegration of these patients [2] . TBI among the high-risk groups is largely preventable. Public awareness and prevention programmes will go some way in reducing the incidence of TBI amongst the high-risk groups. The primary analysis included 162 patients from two intervention hospitals and 892 from 27 control hospitals. Thirtytwo percent of the patients were female and the mean age was 65.3 ± 16.5 years. Almost one-half (46%) of patients had a shockable initial cardiac arrest rhythm, 41% had bystander CPR, and 5% had an AED applied. PACT did not improve use of TTM (ratio of ORs = 1.03, 95% CI = 0.89 to 1.20), angiography for patients without ST-elevation Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 S178 myocardial infarction (ratio of ORs = 1.10, 95% CI = 0.87 to 1.40), or electrophysiology assessment (ratio of ORs = 1.06, 95% CI = 0.81 to 1.38) as compared with concurrent control hospitals. Patients in the intervention group were less likely to have life support withdrawn within 72 hours on the basis of neuroprognosis compared with patients in the concurrent control group (ratio of ORs = 0.62, 95% CI = 0.39 to 0.98). Conclusion PACT was associated with reduced WLST <72 on the basis of neuroprognostication but did not improve other important post-cardiac arrest processes of care. Further work is underway to identify factors that infl uenced implementation. This will guide future consideration of the PACT model in other settings. We obtained 344 questionnaires from 16 hospitals. The population of the study was registered nurses (RN) (63.8%) and doctors Critical Care 2014, Volume 18 Suppl 1 http://ccforum.com/supplements/18/S1 S179 (36.2%). The majority of health staff (81.5%) had never implemented TH. A total 45.8% of respondents stated that the main reasons for not using TH were the lack of information and training about the method, the lack of nursing staff , the lack of available cooling methods and the required time. The most common methods of application were cold packs and intravenous fl uids. Only 30.2% of the doctors and 5.5% of the nurses (P <0.001) actually had the knowledge to implement TH, and this was demonstrated by correct answers. Of the respondents who answered that they did know the method, only 23.9% answered correctly; about the target temperature, the maintenance and rewarming phase. A total 59.1% of doctors, despite having attended the Advanced Cardiac Life Support seminar, were not able to answer correctly the knowledge questions. Continuous education of health professionals and the existence of a protocol were proposed by 65% of participants as the best way of increasing knowledge and adherence with ERC guidelines about TH. Conclusion Therapeutic hypothermia is rarely used in Greek hospitals. The level of knowledge is mainly related to the lack of education and the lack of information about new techniques. Programs for continuing education are necessary for the use of new therapeutic techniques in the fi eld of health. 