Permutation entropy as the predictability of disease time series. Permutation entropy is conceptually similar to the well-known Shannon entropy 31 . However, instead of being based on the probability of observing a system in a particular state, it utilizes the frequency of discrete motifs, i.e symbols, associated with the growth, decay, and stasis of a time series. For example, in a binary time series the permutation entropy in two dimensions would count the frequency of the set of possible ordered pairs, {[01], [10] }, and the Shannon entropy, or uniformity, of this distribution is the permutation entropy. In higher dimensions, one can define an alphabet of symbols over all factorial combinations of orderings in a given dimension, e.g., {[0, 1, 2], [2, 1, 0] , [1, 0, 2] , etc.}, over which the permutation entropy will be defined. A time series that visits all the possible symbols with equal frequency will have maximal entropy and minimal predictability, and a time series that only samples a few of the possible symbols will instead have lower entropy and hence be more predictable. More formally, for a given time series {x t } t=1,…,N indexed by positive integers, an embedding dimension d and a temporal delay τ, we consider the set of all sequences of value s of the type s = {x t , x t+τ ,…, x t+(d−1)τ }. To each s, we then associate the permutation π of order d that makes s totally ordered, that iss ¼ πðsÞ ¼ ½x t i ; ; x t N such that x t i <x t j 8t i <t j , hence generating the symbolic alphabet. Ties in neighboring values, i.e. x t i ¼ x t j , were broken both by keeping them in their original order in the time series and/or by adding a small amount of noise, the method of tie-breaking did not affect the results, see ref. 37 for more details on tie-breaking and permutation entropy. The permutation entropy of time-series {x t } is then given by the Shannon entropy on the permutation orders, that is H p d;τ ðfx t gÞ ¼ À P π p π log p π , where p π is the probability of encountering the pattern associated with permutation π (see Supplementary Figure 1 ). As described above, calculating the permutation entropy of a time series requires selecting values for the embedding dimension d, the time delay τ, and the window length N over which permutation entropy is calculated. In this study, our goal was to find conservative values of H p by searching over a wide range of possible (d,τ) pairs and setting H p ðfx t gÞ ¼ min d;τ H p d;τ ðfx t gÞ. However, the value of H p should always decline as the embedding dimension d grows, i.e. no minimum value of H p will exist for finite windows sizes N. To address this issue, we follow Brandmaier 38 and exclude all unobserved symbols when calculating H p , which acts as a penalty against higher dimensions and results in a minimum value of H p for finite length time series. To control for differences in dimension and for the effect of timeseries length on the entropy estimation, we normalize the entropy by log(d!), ensure that each window is greater in length than d!, and confirm that the estimate of H p has stabilized (specifically that the marginal change in H p as data are added is <1%). To facilitate interpretation, we present results from continuous intervals by fixing τ = 1. However, our results generalize to the case where we fix both d and τ across all diseases and where we minimize over a range of (d,τ) pairs (see Supplementary Figure 4 ). Permutation entropy does not require the a priori specification of a mechanistic nor generating model, which allows us to study the predictability of-potentially very different-systems within a unified framework. What is not explicit in the above formulation is that the permutation entropy can be accurately measured with far shorter time series than Lyapunov exponents and that it is robust to both stochasticity and monotonous transformations of the data, i.e. it is equivalent for time series with different magnitudes 31, 39 . Consider-for example-two opposite cases with respect to their known predictability, pure white noise, and a perfectly periodic signal. We expect the former, being essentially random, to display a very high entropy as compared with the latter, which instead we expect to show a rather low entropy in consideration of its simple periodic structure. In Fig. 1 , we demonstrate that this is indeed the case, even when we allow the periodic signal to be corrupted by a small amount of noise. We track the short-scale predictability of the time series by calculating the permutation entropy in moving windows (with width = 1 year, although the results are robust to variation in window size). For comparison, we calculate the same moving-window estimate of the permutation entropy for the time series of measles cases in Texas prior to the introduction of the first vaccine. The critical observation is that the moving-window entropy for the measles time series fluctuates between values comparable with that of pure random noise and, at times, values closer to the more predictable periodic signal, which suggests alternating intervals with different dynamical regimes and, thus, predictability. The magnitude of the entropy fluctuations for measles in Texas is statistically significant by permutation test, p < 0.001, as compared with simulated fluctuations obtained by building an estimated multinomial distribution over the symbols and repeatedly calculating the expected Jensen-Shannon (JS) divergence from simulations. Pathogen-dependent entropy horizons. We now turn our attention to a broader set of diseases and ask how the predictability, defined as χ = 1 − H p (where H p is the permutation entropy), scales with the amount of available data (i.e. the timeseries length). Specifically, we compute the permutation entropy across more than 25 years of weekly data at the US state-level for chlamydia, dengue, gonorrhea, hepatitis A, influenza, measles, mumps, polio, and whooping cough and plot the predictability (χ = 1 − H p ) as a function of the length of each time series. Focusing first on the predictability over short timescales (Fig. 2 ), for each time series we average H p over temporal windows of width up to 100 weeks by selecting 1000 random starting points from each state-level time series for disease and calculating H p for windows of length 10, 12, ..., 100. We find that all diseases show a clear decrease in predictability with increasing time series length , which implies that accumulating longer stretches of time-series data for a given disease does not translate into improved predictability. However, we also find strong evidence that the majority of single outbreaks-i.e. temporal horizons characteristic for each disease-are predictable. The confidence intervals in Fig. 2 show that there can be large variation in predictability across outbreaks of the same disease, providing a first indication of the presence of a changing underlying model structures and or dynamics on the scale of months. We obtained similar results, e.g., decreasing predictability with time-series length, clustering of diseases, and the emergence of barriers to forecasting, using a weighted version of the permutation entropy, which reduces the dependence of the standard unweighted permutation entropy on rare, large fluctuations and by considering estimates of the permutation entropy where the time delay, τ, is allowed to vary 32, 40 (see Supplementary Figure 4 ). By comparison, across all models with fixed structures studied to date, e.g., white noise, sine waves, and even chaotic systems, the predictability is constant in time or is expected to improve with increasing amounts of time-series data 41 . Zooming out, what is also conspicuous about the relationship between time-series length and predictability is that diseases cluster together and show disease-specific slopes, i.e. predictability vs. time-series length, which suggests that permutation entropy is indeed detecting temporal features specific to each disease (Fig. 3a) . After re-normalizing time for each disease by its corresponding R 0 (the average number of secondary infections a pathogen will generate during an outbreak epidemic when the entire population is susceptible, very large, and is seeded with a single infectious individual)-we used the mean of all reported values found in a literature review (see Supplementary Table 1 )we find that the best-fit mixed-effect slope on a log scale is 1 and that the residual effect is well predicted by the times series' embedding dimension d (see Supplementary Figures 2 and 3) . Moreover, because the embedding dimension d of a time series is the length of the basic blocks used in the calculation of the permutation entropy, it encodes the fundamental temporal unit of predictability in the form of an entropy production rate, thus implying that predictability decreases with time-series data at a disease-specific rate determined to first order by R 0 , which is further modulated by d. The result that predictability depends on temporal scale also suggests that the permutation entropy could be an approach for justifying the utility of different data sets, i.e. one could determine the optimal granularity of data by selecting the dimension that maximized predictability. Table 1) against the log of the permutation entropy Drivers of disease time-series predictability. One might assume that this phenomenon, i.e. decreasing predictability with increasing time-series length, could be driven purely by random walks on the symbolic alphabet used in the permutation entropy estimation. However, n-dimensional Markov chain models built from the timeseries embeddings (n = d the time-series embedding dimension) consistently produced stable and smaller predictability values in comparison with those obtained from data, corroborating that the predictability behavior we observe does not stem from random fluctuations but is an actual fundamental feature of spreading processes (see Supplementary Figure 6 and Methods for details on the Markov chain simulations). This observation that Markov chain models of the same embedding order do not reproduce the observed predictability indicates that either the model structure is changing in time and/or the system has a very long memory, which is consistent with our current understanding of the entanglement between mobility and disease 3, 42 . That the best-fit n-dimensional Markov chain models over-predict the amount of entropy in real systems, also supports our earlier results that predictable structure does exist across most long outbreak time series. To gain insight into what mechanisms might be driving changes in the predictability, we take advantage of the repeated, natural experiment of vaccine introduction. For diseases, such as measles, where we have data from both the pre-and post-vaccine era, we ask whether the permutation entropy changes after the start of widespread vaccination. We consistently observe that predictability decreases after vaccination, again with significance determined by permutation test (Fig. 4a) . We also find that the symbol frequency distribution changes significantly after vaccination, as measured by the Jensen-Shannon divergence, across all states in the United States (Fig. 4b) . Critically, because-as stated earlier-permutation entropy is not affected by changes in magnitude, the difference in entropy cannot simply be accounted for by a reduction in cases. Instead, it means that the temporal pattern of cases changes after vaccination. This leads us to the hypothesis that the distribution of secondary infections, its first moment or R 0 and its higher moments, drives predictable changes in the permutation entropy, a phenomenon originally discovered in synthetic directed networks by Meyers et al. 43 . To further evaluate the hypothesis that heterogeneity in the number of secondary infections produces predictable changes in permutation entropy, we simulate an SIR model with probabilistic restart at end of each outbreak (details in the Supplement) on two classes of temporal networks constructed from the Simplicial Activity Driven (SAD) model 43 , a modified activity driven (AD) model in which an activated node contacts s other nodes and induces new links between the contacted nodes (see Methods). In this model we can control the epidemic threshold and the number of secondary contacts by changing the activity and the number of contacted nodes per activation. We simulated two scenarios, one in which the number of contacted nodes per activation is fixed (regular SAD) and one in which we allow fluctuations in contact number (irregular SAD), which generates fluctuations in the number of secondary infections. For both models, we investigated the predictability from below to above the epidemic or critical transmissibility threshold (set to 1 here). From the resulting epidemic curves, we calculated the permutation entropy. Figure 5a shows that we find the same pattern of decreasing predictability observed in real data with longer time series. Figure 5b shows the predictability obtained for the two scenarios below and above the transition: we see that that the strongest difference is present below the transition, where the lack of peculiar structure (the regular contact pattern) induces lower predictabilities than for heterogeneous contact distributions. Above the transitions, we find a reduced effect of the difference in contact structure. 