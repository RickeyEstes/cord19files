Applying the methodology previously described, forty queries (8.6 terms per query approximately) were submitted and all results computed and analyzed. Considering only the results extracted directly from the index, list L1 (relevance ranking), the MAP obtained was 7.14%. Once the list was retrieved, the clustering algorithm was executed. As there is no user interaction in this experiment, the cluster selected was the biggest one. Analyzing the documents included in the results by means of the Vector Space Model, L1, the recall measure confirms that 34.5% of included references have been found using this strategy, whereas analyzing the documents available in the selected clusters for each query, the recall shows that 22.65% of included references were present. Therefore, the probability of selecting this cluster by the user may be high due to the large number of good results present in it. Once the biggest cluster has been detected, its documents have been ranked according to Algorithm 2 , generating list L2. The MAP for this list (quality ranking) was 9.42%. In this case, the relevance ranking gives better results than the quality ranking, but combining the properties of both lists, the results can be improved, as shown by the MAP, 20.26%. To obtain this result, the scores from L1 and L2 were normalized and several formulas and parameters were tuned, and finally the formula giving best performance was: F usionscore = (rele v ancescore ) α * (qualityscore ) β Where ( α: β) were (1: 0.5). As can be seen, the fusion of both aspects, quality and relevance, can improve the results. Compared to other studies, such as that of Choi [18] , our model presents better behavior. On the one hand, the relevance ranking returned similar results, 7.4% vs. 7.14%. And on the other, the quality ranking is slightly better in this study, 8.2% vs. 9.42%. However, the way it is obtained is quite different. Choi uses a Machine Learning algorithm that requires 1  1730  11  1910  21  2030  31  1590  2  1650  12  1750  22  1790  32  2010  3  2430  13  1930  23  1540  33  2240  4  1610  14  1860  24  1980  34  2530  5  1830  15  2210  25  2210  35  2270  6  2090  16  1970  26  2070  36  2230  7  1920  17  2060  27  2130  37  2470  8  1870  18  2220  28  2250  38  2120  9  1950  19  1910  29  2460  39  2000  10  2190  20  2180  30  1640  40  2190 a prior training step. This step uses a collection like the Clinical Hedges Database, which is not always available or easily accessible. For that reason, an alternative method has been followed here. In this case interaction with the user is necessary, but this is not a problem because the user is necessarily waiting for the results of the search process. Hence, a clustering algorithm allows documents to be grouped according to conceptual criteria and the best to be chosen by the user. Finally, the fusion of the lists depends on the scores assigned to each document from the ranked lists L1 and L2. These scores can vary slightly according to the different models used for relevance ranking, or the Machine Learning or clustering algorithm used for quality ranking. Moreover, the Machine Learning algorithm is supposed to have better discriminatory power with regard to Algorithm 2 , but the combination of both factors, relevance and quality, proves that this system gives slightly better results in terms of MAP, 19.6% vs. 20.26%. In Table 2 , the values of the AvP for each of them are shown. Both studies confirm that relevance and quality are two factors that can help us improve the results of searching documents according to EBM principles, but under different circumstances. 