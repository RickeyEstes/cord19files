The three years of historical data contained 23,221 cases (samples from the same herd on a given day), consisting of a total of 218,795 individual test requests from cattle (i.e. bovine, dairy or beef animals of any age). Based on an evaluation of these three years of historical data, and input from experts, the syndromic groups listed in Table 2 were defined. The table also lists the criteria for syndromic group creation and the number of test requests and cases assigned to each syndromic group following manual classification. After classifying all sample submissions, and eliminating repeated syndromic instances within the same case, the final number of ''syndromic cases'' in the historical dataset was 30,760. Given that there were 23,221 initial herd investigations, this implies an average of 1.32 recorded syndromes per case. The distribution of syndromes per case is shown in Figure 1 . Of all the samples submitted, 75.7% (165,649) could be directly mapped into syndromic groups based on the test request information alone. For the syndromic groups created based on clinical signs, nonspecific signs or specific organ systems (see Table 2 ), Figure 2 illustrates the percentage of test requests which could be allocated to a syndromic group via direct mapping versus those that fell into the unmapped subset. Around 25% (53,146) of all instances in the database could not be directly mapped into a syndromic group and these provided the material for which automated classification was explored. Although these unmapped instances contain 16 of the original 22 defined syndromic groups, the syndromic group ''Mastitis'' alone is responsible for over 70% of these instances, and three groups (''Mastitis'', ''Nonspecific'' and ''GIT'') account for over 90% of the data, as shown in Table 3 . For the groups Mastitis and GIT, 94% and 77% of the unmapped observations, respectively, refer to the test ''Bacteria culture''. Unmapped observations which are ultimately classified as ''Nonspecific'' contain a greater variety of test names, including the following which occur frequently: ''Bacterial culture'' (18%), ''Histology'' (27%) and ''Necropsy'' (18%). The results of automated classification using different algorithms are shown in Table 4 and described in detail below. The use of rule induction (RIPPER) achieved only moderate performance overall. Three groups with low frequency of test requests -''Environmental samples'', ''Skin'', and Eyes and Ears'' -were not included in the rules, but as shown in Table 3 these groups represent only 0.3% of all instances subjected to automated classification. The F 1 -macro average was 0.677, but because the unlearned groups account for such a small proportion of the submissions, when the classes' performance is averaged accounting for the weight of each class, the F 1-micro is 0.979 (Table 4 ). Upon manual review of the rules created by the algorithm, it was found that the main source of error was failure of the algorithm to establish good decision rules when multiple medically relevant words were found in the same test request. This method was easy to implement and the rules generated are transparent and easily interpreted. The rules produced by the RIPPER algorithm were manually modified to account for some of the relationships missed, producing a set of custom rules. Running the custom rule set against the entire unmapped subset resulted in an F 1-macro score of 0.997, and F 1-micro score of 0.9995 (Table 4 ). The remaining errors tended to be due to use of abbreviations not common enough to have been incorporated in the rules, misspellings or the absence of a space between two words, resulting in the tokenization process failing to identify these words. The performance of the Naïve Bayes algorithm was high (F 1macro of 0.955 and F 1-micro 0.994), as shown in Table 4 . The main performance issue associated with this algorithm was its instability. Slightly different datasets resulted in very different performances (results not shown). With unbalanced training and test datasets, for instance, rather than assigning the label ''Nonspecific'' to samples that could not be classified, the Naïve Bayes algorithm would assign these samples, as well as misclassified samples from other groups, into one of the groups with a small number of submissions. The classifier based on Decision Trees performed reasonably well in the micro score (F 1-micro score of 0.923). However the classifier failed to learn 9 classes, which are biologically relevant, despite accounting for only 2% of the unmapped instances (which explains the high micro average). Moreover, the models appeared to be unstable: slight changes in the training data could result in a completely different 'shape' of decision tree, and a similar phenomenon was observed when the initial parameters for minimal gain and confidence where varied. 