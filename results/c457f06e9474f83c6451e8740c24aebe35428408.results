The ratio c/b, the rate of removal compared with the rate of infection, represents the relative effectiveness of interventions. In the simple stochastic epidemic, the relative effectiveness of intervention is always greater than one because we assume that the outbreak is eventually controlled, i.e., the assumption b , c above. Figure 1 confirms the intuition that final outbreak size declines as the relative effectiveness of intervention is increased. The CV in the final outbreak size, our measure of the imprecision with which the final outbreak size is forecasted, also declines with control effectiveness. As a benchmark, a forecast might be deemed reliable (in principle) where the CV is less than one, which Figures 2 and 3 show plots of the final outbreak size and the CV over the interval of estimated bs for each of nine directly transmitted diseases. It is important to underscore that the intervals in Figures 2 and  3 represent uncertainty about the value of the parameter b, not variation from stochastic fluctuations. Further understanding of these diseases might allow us to reduce this source of uncertainty by obtaining more precise estimates. In contrast, the CV in Figure 3 represents the range of final outbreak sizes that can result from the stochastic infection process for a fixed set of parameters. In principle, no amount of detailed information about transmission or other ensemble epidemic parameters can reduce this uncertainty. Numerical analysis of the delayed-onset intervention model showed that (1) the average outbreak size increased with the delay between the start of the outbreak and the start of intervention ( Figure 4A ), and (2) the CV (in our examples) was everywhere greater than one and increased with the time delay between the start of the outbreak and intervention, but at a declining rate ( Figure 4B ). The first result is straightforward: The delay between initial infection and intervention increases the total number of secondary (tertiary, etc.) infections that are increasing as a multiplicative process. The explanation of the second result is that the CV in outbreak size scales as the square root of the variance in outbreak size and as the inverse of the average outbreak size. As the average outbreak gets larger the CV increases but at a declining rate ( Figure 4C ). This effect is mediated by the reproductive ratio of the outbreak, so that the outbreak with the lower R 0 had a lower average outbreak size ( Figure 4A ), but larger CV ( Figure 4B and 4C) . Thus, in the sense that the CV measures the predictability of the outbreak, we found that subcritical and controlled outbreaks (R 0 , 1 and R 0 close to 1, respectively) were less predictable (have lower CV) than supercritical (R 0 .. 1) outbreaks of comparable size. 