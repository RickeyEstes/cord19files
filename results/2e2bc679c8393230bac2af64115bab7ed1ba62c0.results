The IPF values between septic (4.6 ± 3.1) and no septic patients (3.3 ± 1.5) did not diff er (P = 0.16). No correlation was found between IPF values and the severity of septic condition (no sepsis 11.7 ± 10.1; sepsis 14.3 ± 10.5; severe sepsis 10.5 ± 9.1; septic shock 19.5 ± 12.4; P = 0.3). When we considered only subjects who did not have sepsis at the ICU admission we found that patients who developed sepsis during the recovery had IPF values higher than patients who did not develop sepsis (Table 1) . Conclusions From our results IPF cannot be considered a marker of sepsis. Conversely it could be used as predictive index of sepsis because it can identify patients who will develop sepsis. Introduction The role of matrix metalloproteinases (MMPs) and tissue inhibitors of matrix metalloproteinases (TIMPs) in sepsis remains unclear. Introduction Serum C-reactive protein (CRP) is synthesized in response to infl ammatory status. Elevated CRP levels are associated with an increased risk of multiorgan failure in ICU patients. Preoperative elevation of serum CRP is a prognostic indicator in gastric and colorectal surgery. The aim of this study was to determine serum CRP as prognostic variable in patients undergoing esophagectomy with gastric tube reconstruction in contrast to other ICU admitted patients. Methods Data were collected retrospectively for a total of 208 patients admitted to the ICU following elective surgery from October 2007 to December 2008. Patients included underwent esophagectomy with gastric tube reconstruction, liver transplantation, hemihepatectomy, neuro-and abdominal aneurysm surgery. Postoperative serum markers were measured and the relation between the course of postoperative serum CRP, development of complications and prognosis of the patients was investigated. Results Postoperative serum CRP was signifi cantly higher at T24 in patients undergoing esophagectomy with gastric tube reconstruction, compared with all other patients. Higher serum CRP levels correlated with the occurrence of complications in the heterogeneous ICU population but especially in the esophagectomy patients. Within this group, serum CRP levels at T24 and T48 were signifi cantly higher in patients with a postoperative pneumonia, which in itself was associated with increased 1-year mortality. Conclusions Postoperative serum CRP levels can easily be monitored in the ICU in order to identify patients at risk for the development of postoperative complications. Especially in the esophagectomy patients, the occurrence of postoperative complications is associated with reduced survival. Introduction To examine whether we can safely shorten the periods of antibiotic administration for septic patients with procalcitonin (PCT) measurements compared with no PCT measurements. Methods The participants were septic patients including (1) hospitalized to our ICU from February 2009 to November 2009, (2) administered antibiotics for 4 days and over, and (3) stopped the antibiotics during their ICU stays. We treated the patients from February to June without PCT measurements (Group A). On the other hand, we treated the patients from July to November referring to serum PCT level (Group B). In group B, when : sepsis secondary to hydronephrosis. This graph clearly shows the rise in procalcitonin (PCT) during early onset of sepsis and its fall during antibiotic treatment. Antibiotics were discontinued when the PCT level fell to 80% of its peak value on day 8. Example 2 (right): laparoscopic cholecystectomy complicated by bile peritonitis. Good response to antibiotics initially, but PCT started to rise on day 12 which prompted us to change antibiotics. Serial PCT is useful to assess the response to treatment. Introduction Bloodstream infection is a life-threatening condition with a high mortality rate, especially in intensive care and neutropenic patients. Standard diagnostics is based on blood culturing (BC). However, limitations of BC include relatively low sensitivities and a long time-toresult for the identifi cation of the pathogen, generally over 2 days and more. On the grounds of data from a multicentre study using a universal 16S rRNA gene PCR assay, SepsiTest™, molecular diagnosis is discussed as a rapid and sensitive tool for the detection and identifi cation of pathogens supportive of BC. A new commercial PCR test, SepsiTest™, for direct detection of bacteria in whole blood was compared with BC in terms of sensitivity, specifi city, predictive values and time to positivity (TTP) of bacterial infections of the bloodstream of critically ill patients. Methods The test, SepsiTest™ (Molzym, Bremen), comprises the extraction and 16S rRNA gene PCR detection of bacterial DNA in whole blood samples. Bacteria in positive samples were identifi ed by sequence analysis of the amplicon. In a prospective multicentre study, 342 blood samples from 187 patients with systemic infl ammatory response syndrome (SIRS), sepsis, or neutropenic fever were included. Results Compared with BC, the diagnostic sensitivity and specifi city of PCR/sequencing was 87.0% and 85.8%, respectively. The positivity rate of PCR/sequencing (25.7%) was higher than BC (15.8%). Of 31 PCR/ sequencing-positive, BC-negative patients, most of whom received antibiotics, the PCR results of 25 were judged as true or possible to bacteraemia. Using a routine testing workfl ow, time to positivity of the PCR test was on average decreased by 40 hours for anaerobe/fastidious infections and by 54 hours for yeast infections. Modest diff erences in circulating concentration of coagulation-fi brinolysis markers on ED presentation (day 1) and over the fi rst 7 days. P 1 , day 1 P value from ANOVA model adjusted for sex, race, co-morbidity, and smoking status; Tobit models used for censored data. P 2 , day 1 to 7 P value from mixed eff ects model adjusted for sex, race, comorbidity, and smoking status; Tobmit models used for censored data. Introduction Prompt initiation of appropriate antibiotic therapy improves outcome in critically ill patients [1] . In a tertiary-level ICU, we evaluated the appropriateness of, and adherence to an antibiotic guideline (based on local bacterial epidemiology) in CDC-defi ned hospital-associated infections. Methods We conducted a 6-month prospective observational study (April 2008 to October 2008) in consecutive ICU admissions of patients who satisfi ed investigator-adjudicated classifi cation of ICU-acquired infections according to CDC criteria [2] . We assessed patient characteristics including severity of illness at admission, ICU length of stay (LOS), appropriateness of initial antibiotic choice as judged by in vitro sensitivity results and appropriateness of the current guideline. Results are presented as mean (SD) or median as appropriate. Results During the study period there were 101 antibiotic starts in 65 patients with sepsis secondary to ICU-acquired infections. Medical patients formed 44% of the study cohort; whilst 23% of patients were general surgical and the remaining 33% were post cardiothoracic surgery. The age and admission APACHE II score of the study cohort was 61.8 (16.3) years and 18.4 (5.6) . The median LOS and ICU mortality of the cohort was 24 days and 27.6%. The most common CDC reportable diagnosis was clinical or microbiological confi rmed pneumonia (PNU1/PNU2/LRI) (n = 57), followed by intra-abdominal infection (SSI-GIT) (n = 10) and urinary tract infection (SUTI) (n = 8). The culture positivity rate was 71.2%. The appropriateness of the ICU antibiotic guideline is summarised in Table 1 . Monotherapy was used in 52.5% of episodes. The median length of antibiotic treatment with positive cultures was 7 days, and 5 days for culture negative episodes. In sepsis episodes with negative culture, antibiotics were stopped within 3 days in 17% of the episodes. Introduction Secondary peritonitis is the most frequent form of peritonitis, characterized by a high disease burden and a high mortality rate. Choice of adequate antibiotics is an independent factor for survival. The aim of this study was to compare treatment of secondary peritonitis with tigecycline (TG) with standard regimens (SR) from an economical standpoint. Methods After ethics committee approval, the study was performed as prospective, non-interventional cohort trial in 23 medical centers in Germany. Patients could be included if suff ering from severe secondary peritonitis treated in an ICU. Patients with pregnancy, aged below 18 years and milder forms of diseases (APACHE II score <15) were not eligible. In order to compare treatment with TG with SR, the following data were documented: demographic data, disease severity scores, causing microorganisms, laboratory parameters, and length of stay (LOS). Patients were analysed according to initial antibiotic choice except for perioperative prophylaxis. In order to balance for diff erences in co-morbidities and severity of disease, a matched-pairs analysis was performed based on logistic regression analysis. Results A total of 178 patients were enrolled (49 TG/129 SR). After logistic regression analysis and matching for gender, age ± 3 years, APACHE II score ± 1 and (non)existence of liver cirrhosis, arterial sclerosis and coronary heart disease, 15 matched pairs were built. Compared with the SR group, in the TG group was a tendency for higher creatinine, urea and glucose levels, higher number of co-morbidities (3.3 vs 3.0, NS) and higher number of pathogens isolated on initial surgery (2.2 vs 1.6, NS). There was a higher number of discharges 9/15 in the TG group (vs 7/15 SR, NS) and 6/15 patients died (vs 4/15 SR, NS). Considering these factors, there was a trend for shorter LOS in patients treated with TG (11 days vs 18 days and 8 days vs 16 days for survivors and nonsurvivors, respectively, NS) and total costs of ICU stay were signifi cantly lower in the TG group (€8,832 vs €15,482, P = 0.023). Conclusions In our non-interventional study, tigecycline tended to be used in patients with more severe co-morbidities. In spite of this, there was a trend for shorter LOS and treatment costs were signifi cantly lower, which make tigecycline an attractive treatment, also from a pharmacoeconomical standpoint. Introduction The purpose of our study was to compare the eff ectiveness of colistin monotherapy and colistin combination in the treatment of nosocomial infections with multi-resistant germs. Methods Retrospective study including 63 patients realized during 3 years from January 2006 to December 2008 in the medical ICU of University Hospital Ibn Rochd, Casablanca, Morocco. The study includes only the patients who suff er from nosocomial infection with multi-resistant germs, all of the sites are concerned. The patients have been divided into groups: group 1 including 30 patients treated by colistin only, and group 2 including 33 patients treated by the association colistin-rifampicin. The colistin has been administered intravenously and/or in nebulization and/ or in an intrathecal way according to the considered infectious site. The main criterion of judgment was the rate of mortality in the resuscitation unit, the second criteria of judgment were the ventilator weaning, the introduction of the vasoactive drugs and the supervening of side eff ects. Results Sixty-three patients judged as appropriate, have been included. The mean age of the patients was about 43.62 ± 17.34 years and APACHE 2 score at the admission was about 15 ± 5.69. The total mortality caused by infection was about 41.27%. The basic characteristics of the two groups were similar. The mortality in group 1 was about 36.66%, and about 69.69% in group 2 (P = 0.001), the rate of introduction of vasoactive drugs was about 23.33% in group 1 versus 48.48% in group 2 (P = 0.03). In group 1, 6,66% of the patients developed renal failure, against 12.12% of the patients in group 2 (P = 0,46). With the rifampicin, 27.27% of the patients of group 2 presented cytolysis. Conclusions This study suggests that colistin represents a good therapeutic alternative for the treatment of nosocomial infection with multi-resistant germs. However, our study is not without limits; it is a retrospective study, absence of randomization and the control group of patients. In 18 patients (0.64%) consisting of 12 males and six females, at least one MDR pathogen was isolated. Gram-positive pathogen was certifi ed in fi ve patients (27.8%) and Gram-negative in 13 patients (72.2%). Specifi cally, four patients were infected with vancomycin-resistant enterococcus (VRE), six with Klebsiella pneumoniae, three with Acinetobacter spp., one with VRE and Acinetobacter spp., four with Acinetobacter and K. pneumoniae. Low output syndrome (CI <2.0l/min/m 2 ) was common in all these patients and essentially contributed to the deterioration of clinical situation with dependence on inotropic support, prolonged mechanical ventilation (>10 days), acute renal failure and need for haemodilution (66.6%). Therefore, the ICU and hospital stay is prolonged (>20 days and >30 days, accordingly) and pathogenesis of MDR infection is provoked after 20 days of ICU stay. Consequently, nine patients with MDR infection (50%) died. All were critically ill patients with multiple organ dysfunction syndrome under broad-spectrum antimicrobials with hospital-acquired bloodstream MDR bacteremia. Conclusions Infection with MDR pathogens, while rare, constitutes a notable prognostic marker of increased mortality after cardiac surgery. It is worth noting that the higher mortality rate is mainly attributable to the severe co-morbidity in haemodynamically compromised patients. Management must concentrate on the implementation of eff ective preventative strategies. Introduction Prior antimicrobial therapy is one of the most important factors leading to the acquisition of MDR organisms. Formulating antibiotic policy and choice of empirical antibiotic selection will be helped by knowing the association of MDR Gram-negative organisms with a previous exposure of particular antibiotic. Methods Prospective observational study during January 2008 to June 2009 in a 50-bed ICU in a tertiary care hospital. Specimens sent after 2 days of the start of the antibiotic and no more than 90 days from the stop date have been included in the study. Analyses were based on those specimens that resulted in detection of MDR Gram-negative organism. Observed relative risk (RR) of an antibiotic class was computed with respect to an MDR infection. RR was computed as the ratio of the risk of the event (acquiring the infection) occurring in the exposed group vs in the nonexposed group. A logistic regression model was used where multiple antibiotics were applied. Results A total of 1,072 specimens from 500 patients met the criteria as specifi ed above. Of these, 423 (39.4%) specimens resulted in detection of MDR bacteria, 186 (17.4%) resulted in detection of non-MDR bacteria and no bacteria were detected in the remaining 463 (43.2%) specimens. Of the total 423 cases of MDR acquisitions, ESBL Enterobacteriaceae (151 or 35%), MDR Acineto (89 or 21%) and MDR Pseudo (58 or 14%). Risk of isolating ESBL Enterobacteriaceae was highly signifi cant with the prior exposure to third-generation cephalosporin (RR = 5.8 and P <0.001). Risk of isolating MDR Acinetobacter spp. was highly signifi cant with the exposure to piperacillin-tazobactam (RR = 2.7 and P <0.001). Risk of isolating MDR Pseudomonas spp. was signifi cant with the exposure to Group 2 carbapenem (RR = 2.2 and P >0.001). Group 1 carbapenem, aminoglycosides have not been found to have signifi cant association with any individual MDR organisms. Conclusions Previous exposure to antibiotics leads to increased acquisition of MDR organisms. There is a signifi cant association of isolating diff erent MDR organisms with previous exposure to a particular class of antibiotic. Introduction The emergence of multidrug-resistant Acinetobacter baumanii (MDRAB) poses a serious threat to patients on the ICU. The production of metallo-β-lactamase leaves colistin as the only therapeutic option. Outbreaks due to MDRAB can persist for months. Traditional decontamination methods fail to deal with this level of colonisation and contamination eff ectively. We tackled a recent outbreak of MDRAB eff ectively using gaseous ozone. To our knowledge it is the fi rst time ozone has been used to control an outbreak of MDRAB. Methods An external company (Hydrozone Environmental Ltd) was hired to perform the fumigation. The ICU was divided into three decontamination areas using heavy-duty polythene sheets. Patients were in turn relocated from contaminated to clean areas before each area was sealed and fumigated. Humidity levels within were raised to 70 to 80% using a humidifi er. An Ozone Ultra Pro 16 g/hour ozone generator with ozone destruct capability, operated remotely, delivered ozone to a target concentration of >2.0 ppm for 15 minutes. A fan was used to achieve even dispersal. For safety reasons perimeter ozone concentrations were monitored with a UV photometer and kept below 0.05 ppm. The effi cacy of the fumigation was measured by environmental microbiological sampling before and after fumigation. Results All fumigated areas received ozone concentrations of 4.62 to 5.66 ppm for 21 to 32 minutes. Ozone was not detected outside the treatment areas. Prior to fumigation, 72 (38%) of 188 environmental samples were MDRAB-positive. Following fumigation, nine (5%) of 158 samples were positive. Most of these samples were from nontouch areas, for example ceiling, above door frame with signifi cant dust collection and without daily cleaning. Considering that dust may impede ozone penetration, Introduction In the current pandemic it is likely that some patients will be admitted to hospital and require respiratory support including mechanical ventilation. These patients are likely to have a profound systemic infl ammatory response syndrome (SIRS); consequently they may have multiorgan failure (MOF) requiring renal replacement therapy (RRT) with haemofi ltration. Two questions then arise -what dose of oseltamivir (Tamifl u) should we give these patients to shorten the duration of the H1N1 infection? How should we modify the dose in response to altered renal drug clearance and in those requiring RRT? Methods A young adult female patient with H1N1 infection and MOF was given oseltamivir 75 mg BD nasogastrically. Failure to respond changed the risk: benefi t ratio and justifi ed doubling the dose despite uncertainties over an overall reduced clearance. Enteral nutrition absorption was uncertain, and thus we sampled her blood to ensure adequate oseltamivir absorption and that activation of the pro-drug was not inhibited. We undertook serial sampling for blood concentration assay to determine the pharmacokinetic parameters in this diffi cult scenario. Blood samples were collected in plain serum tubes (without sodium fl uoride). The samples were spun and refrigerated within half an hour, then batched and shipped to Bangkok for drug concentration measurement. Results We report both the parent oseltamivir phosphate (OP) and that of the active metabolite oseltamivir carboxylate (OC). OP levels were low at 10 to 77 ng/ml. but OC concentrations were high at 2,600 to 5,000 ng/ml. Conclusions The population normal parameter for half-life OP is 1 hour and for OC it is 3 to 5 hours. A single dose of 150 mg OP is expected to achieve an OP level of 50 to 150 ng/ml and an OC level of 1,000 to 1,500 ng/ml. Our slightly low OP levels are likely to be due to ex vivo hydrolysis in the collection tube due to a lack of esterase inhibitor. The high OC levels are most probably due to reduced renal elimination despite being on haemofi ltration. Our concerns were focused on the possibility of viral mutation (subsequently shown to be negative) or poor enteral absorption/activation. What we found was that 150 mg BD produces more than adequate OC levels to treat H1N1 infection. Introduction Invasive candidiasis (IC) is associated with increasing morbidity and mortality in critically ill patients. This, in conjunction with diffi culties in diagnosis, underscores the need for novel treatment strategies based on the identifi cation of signifi cant risk factors for IC. The aim of the study was to evaluate the effi cacy and safety of a protocol for pre-emptive antimycotic treatment. Methods A randomized prospective controlled trial was carried out in a general ICU for 2 years. After the implication of the inclusion and exclusion criteria, patients were submitted to block randomization and stratifi ed on the basis of their initial SAPS II expanded score. We have developed a protocol for pre-emptive antimycotic treatment. Having reviewed the current literature, we combined the most signifi cant risk factors for IC with tree major clinical criteria for persistent nonbacterial sepsis and assumed this algorithm as an indication for starting pre-emptive therapy. According to the protocol, antimycotic therapy was started on the day of inclusion in the treatment group and only with proven IC in the control group. Initial data were gathered on demographic characteristics of the patients, proven risk factors for IC-related mortality (malnutrition, non-albicans colonization, creatinine clearance) and severity of infl ammatory response and organ dysfunction. Dynamics of SIRS and SOFA, subsequent Candida isolates, ventilator-free days, length of ICU stay, outcome and eventual adverse reactions were followed. Results A total of 110 patients (equal in both groups) were enrolled. No statistically signifi cant diff erences in the basal characteristics of the patients, length of ICU stay and the number of ventilator-free days were found. The delta SOFA score was signifi cantly lower in the treatment group (P = 0.019).The in-hospital mortality was 38.2% in the treatment group vs 61.8% in the control group (P = 0.013). The associated with pre-emptive therapy relative risk was 0.62 (95% CI = 0.4 to 0.94). Signifi cant diff erences between the Kaplan-Meyer estimates of survival were found (log-rank test P = 0.007). A total of 15 (13.6%) adverse reactions were observed among treated patients in both groups which was not associated with higher mortality risk. Conclusions The implementation of the developed protocol reduced the degree of organ dysfunction severity and was associated with signifi cant survival benefi t. Fourteen studies were identifi ed, three involving patients undergoing cardiac surgery (1,841 patients) and 11 involving patients in noncardiothoracic ITUs (1,497 patients; see Table 1 ). Five studies (including two cardiac studies) found a signifi cant reduction in episodes of nosocomial RTI in the chlorhexidine-treated group versus placebo or standard care. Pooled data indicated a signifi cant reduction in nosocomial RTI in the treatment group among all patients, and among cardiac and noncardiac sub-groups (odds ratio 0.57 (95% CI 0.42 to 0.77), 0.52 (0.37 to 0.75) and 0.6 (0.4 to 0.89), respectively). However, no signifi cant diff erences in mortality, duration of mechanical ventilation or ITU stay were demonstrated. Signifi cant heterogeneity (I 2 statistic >40%) was detected for all outcomes except mortality. Conclusions Use of oral chlorhexidine is associated with reduction in nosocomial respiratory tract infection in intubated mechanically ventilated critically ill adults. Conclusions NICOM demonstrated a moderate agreement with LiDCO but showed excellent agreement with PulseCO in tracking CO changes following therapeutic interventions. Introduction Arrhythmias are common among high-risk surgical and ICU patients. The PulseCO pressure waveform algorithm is used for both LiDCO™plus and LiDCOrapid hemodynamic monitors, which are frequently used to estimate cardiac output (CO) in critically ill patients. Cardiac arrythmias could increase the variation of both the lithium dilution (LiDCO) and/or the PulseCO measurement. At set-up the algorithm is calibrated by comparing the PulseCO CO estimate, averaged over 30 seconds, with a known CO (normally LiDCO) to generate a calibration factor (CF) [1] . This study was designed to explore the eff ect of arrythmias on the accuracy of CF generation in the PulseCO monitor. Methods LiDCO™plus hemodynamic data fi les were obtained retro spectively from a university hospital medical/surgical ICU. Files were separated into those records with and without arrhythmia -defi ned as heart rate variation >5% during at least one additional CF determination after monitor set-up. Previous studies have established the coeffi cient of variation (CV) of a single LiDCO determination at 8% [2] and the PulseCO measurement at 2.4% [3] . A combined CV, refl ecting the eff ect of calibration, is estimated at 8.5%, resulting in an expected precision for the CF of 17%. Data were analysed for variation in CF against HRV using linear regression and the Student' s t test. Results Twenty-eight records were collected and analysed. Twenty-one records contained 32 post set-up calibration events. Of these 17 occurred with HRV ≤5% (median = 1%, range: 0 to 5%) and 15 occurred while HRV >5% (median: 19%, range: 7 to 26%). The average variation in CF during HRV was 5.4 ± 4.0% and for high HRV was 8.9 ± 8.1% of the initial value. The t test indicated no diff erence in the mean variation of CF (P = 0.162) or median. There was no correlation between HRV and CF variation (r 2 = 0.002). Ninety-one per cent (29/32) of the observed CF variation were less than 17% of the initial CF value. Conclusions CF determinations are not signifi cantly aff ected by HRV. We found a signifi cant (P <0.05) increase in CO reported by Vigileo/FloTrac system in the post-clamping phase, when compared with the pre-clamping and basal phases, while the blood fl ow in thoracic aorta resulted decreased, according with the theory of redistribution of fl uids in the splanchnic venous vasculature [2] . There was an important contribution of the wave refl ection to the aortic pulse pressure wave after the AoX, as expressed by a signifi cant increase in the AI. The proportional changes in NiCOM-SVi induced by PLR were correlated with the proportional changes in NICOM-SVi induced by volume expansion (r = 0.67, P = 0.02). The proportional changes in NICOM-cardiac index (CI) induced by PLR were also correlated with the proportional changes in NICOM-CI induced by volume expansion (r = 0.63, P = 0.03). A PLR-induced increase in stroke volume of 9% or more predicted an increase in stroke volume of 15% or more after volume expansion with a sensitivity of 100% and a specifi city of 80%. Conclusions The response of NICOM-stroke volume to PLR was a good predictor of volume responsiveness. In our hemodynamically unstable patients with spontaneous breathing activity, fl uid responsiveness can be assessed totally non-invasively with a bioreactance device. Introduction Transpulmonary thermodilution (TPTD)-derived volumetric parameters such as global end-diastolic volume (GEDI) and ELWI have been established as hemodynamic cornerstones for assessment of preload (GEDI) and pulmonary hydration. Normal values of GEDI have been created more than a decade ago based on studies in pre-selected patients. Therefore, it was the aim of our prospective study to investigate the correlation of GEDI to cardiac index (CI) in clinical routine. Methods Over a 6-month period all 1,574 routine TPTD measurements in 78 consecutive patients (APACHE II: 23.5 ± 8.6) of an internal ICU with a PiCCO catheter were prospectively documented and analysed: correlation (Spearman) and multiple regression analysis; SPSS 17.0. Results Including all 1,574 measurements, CI was univariately correlated to GEDI (r = 0.251; P <0.001), dP max (r = 0.221; P <0.001) and heart rate (r = 0.102; P <0.001), but not to CVP (r = 0.001; P = 0.962). The correlation of GEDI, dP max and heart rate to CI was confi rmed in multivariate analysis (P <0.001 for all three variables). Changes in CI (Delta-CI) were univariately correlated to changes in GEDI (r = 0.414), dP max (r = 0.240) and ELWI (r = 0.152; P <0.001 for all comparisons). In a multivariate analysis of all measurements, Delta-CI was independently associated with changes in GEDI (P <0.001), dP max (P <0.001) and CVP (P = 0.017). Subgroup analysis of all measurements with GEDI below the lower normal level 680 ml/kg/m 2 demonstrated an independent association of CI to GEDI (P <0.001), dP max (P <0.001) and ELWI (P = 0.041) but not to CVP. Similarly, Delta-CI was independently associated with changes in GEDI and dP max (P <0.001). Similar results were found for the measurements with GEDI within the normal range (680 to 800 ml/kg/ m 2 ): signifi cant and independent correlation of CI to GEDI (P <0.017) and dP max (P <0.001). Changes in CI were independently correlated to changes in GEDI (P <0.001), dP max (P <0.001) and CVP (P = 0.035). Interestingly, even in measurements with GEDI >800 ml/kg/m 2 , CI was independently correlated to GEDI (P = 0.009) and dP max (P <0.001). Changes in CI in this group were independently associated with changes in dP max and GEDI (P <0.001).In the subgroup of measurements with GEDI >1,000 ml/kg/m 2 there was no correlation of any parameter to CI, however changes in CI were independently correlated to changes in GEDI (P <0.001) and dP max (P = 0.003). Conclusions GEDI and dP max and their changes have an independent and positive correlation to CI and its changes even in patients with increased GEDI. In 19 patients (responders), CO increased by >15% after fl uid infusion. Fluid responsiveness was better predicted with DPP 6 ml/kg (ROC curve area 0.92 ± 0.05) than with pulmonary artery occluded pressure (0.56 ± 0.09) and right atrial pressures (0.74 ± 0.08). Increasing tidal volume to 8 ml/kg did not improved prediction as the ROC curve area with DPP 8 ml/kg was 0.94 ± 0.03. The best cut-off values defi ned by the ROC curve analysis was 6.5% and 10.5% for DPP 6 ml/kg and DPP 8 ml/kg, respectively. Conclusions The maneuver to change tidal volume to 8 ml/kg in patients ventilated with protective ventilatory strategy to better predict fl uid responsiveness is not useful. Fluid responsiveness can be correctly predicted in patients ventilated with tidal volume of 6 ml/kg. The mean SVI level was 19.4% higher at p4 to p6 in the IGDT group compared with the control group (P = 0.02), and 12.1% higher in the entire intervention period (t1 to p6) (P = 0.07). The mean DO 2 I was 18.0% higher at p4 to p6 (P = 0.01) and 12.9% higher in the entire intervention period (t1 to p6) in the IGDT group (P = 0.03). Mean arterial pressure and heart rate did not diff er signifi cantly (P = 0.12 and P = 0.21). There was no diff erence in the frequency of postoperative cardiac complications between the groups. Conclusions The results of this study demonstrate that IGDT targeting SV and DO 2 can be performed safely in patients undergoing EAAS. Whether this intervention is benefi cial is being evaluated in the current study. Introduction Studies have suggested that tissue oxygenation (StO 2 ) measured on the thenar muscle is not sensitive to track acute changes in hemodynamics due to reductions in central blood volume. We aimed to investigate the feasibility of StO 2 measurements in the mouth as a quantitative indicator comparable with StO 2 measurements obtained from the thenar eminence during changes in central blood volume (CBV). Methods We performed a head-up tilt (HUT) test in 10 healthy volunteers as an experimental model to reduce CBV. StO 2 was continuously measured using two devices (InSpectra model 650): a multiple depth optical probe was placed over the thenar eminence (15 and 25 mm) and a 1-mm probe was placed in the mouth. Subjects were placed on an electrically driven tilt table with a footboard. After 5 minute baseline measurements in the supine position, the table was tilted up to 70° and returned to the supine position after 10 minutes. StO 2 readings were analyzed at the lowest stroke volume value. Results All subjects (mean age: 23 ± 6; six males) tolerated well the supine and head-up positions. Cardiac output signifi cantly decreased in the HUT position; simultaneous decrease in StO 2 was observed in the mouth, but not in the thenar. The general results of the HUT test are shown in Tables  1 and 2 . Introduction Hemodynamic optimisation based on fl ow variables allows an early detection and correction of possible occult organ hypoperfusion in patients undergoing major surgery. Shoemaker described a markedly decreased cardiac index (CI) in nonsurvivors, which remained signifi cantly below the values compared with survivors during the surgery. The aim of this study was to evaluate the length of ICU stay, overall in-hospital stay and the postoperative outcome in a group of patients undergoing major urological surgery, while the CI is maintained within the normal range during intraoperative period. Methods Patients were randomised into groups the day before surgeryconventional management group (decision about fl uid therapy and vasoactive support was based on internal guidelines to preserve normal macrohemodynamic variables), and protocol group. Each patient in the protocol group received an oesophageal Doppler probe (TED) (Hemosonic 100; Arrow International, USA) after the start of general anaesthesia and then hemodynamic optimisation (fl uid management and vasoactive drugs), according to TED variables, was performed to keep CI between 2.6 and 3.8 l/minute/m 2 . We enrolled 230 patients. The control group: n = 115 and the protocol group: n = 115. High-risk criteria surgery was fulfi lled in 43% patients in protocol group and 45% in control group. There were no signifi cant diff erences in baseline variables between both groups (age, gender, length of surgical procedure, estimated blood loss and also in intraoperative values of MAP and CVP). In the protocol group was observed a high frequency of CI <2.6 l/minute/m 2 after induction of anesthesia 75% with fast recovery of CI. Introduction Fluid therapy in ICU patients serves to maintain tissue perfusion and is directed at increasing cardiac stroke volume (SV) through an increase in preload: fl uid responsiveness. Although there is increasing evidence that the regional blood fl ow cannot be predicted from global hemodynamic measurements, the relation between SV and parameters of peripheral perfusion is not clear. The aim of our study was to evaluate the eff ect of an increase in preload on commonly used parameters of peripheral perfusion. Methods Hemodynamically unstable patients with clinically suspected fl uid responsiveness underwent a passive leg raising (PLR) test, which consisted of 5 minutes of rest in a semirecumbent position of 30°, followed by 5 minutes PLR (lower limbs elevated at 30° and trunk in supine position). SV was measured continuously by pulse contour analysis using PiCCO (Pulsion). Peripheral perfusion was measured continuously with sidestream dark fi eld imaging (sublingual area) and laser Doppler fl owmetry (LDF) (fi nger). Results Sixteen patients (age: 63 years (55 to 72), APACHE II: 25 (20 to 28), SOFA: 10 (7 to 13)) were included in our study. Of these 16 patients, six (38%) increased their SV by >10% in response to a PLR. Flow indices (LDF and sublinguale microcirculatory fl ow) did not change. However, there was a trend in increase of the functional capillary density in the responders (see Figure 1 ). Conclusions These data suggest that increasing SV in hemodynamically unstable patients might improve peripheral perfusion, however only in the sublingual area and not in all patients. There was no relation between systemic circulation and peripheral perfusion: it remains to be investigated whether optimizing SV actually results in improved tissue perfusion. Among all patients, at baseline, median CI and dIVC% were 2.6 l/minute/ m 2 and 29%, respectively. Volume expansion signifi cantly increased the median CI from 2.6 (2 to 3.3) to 3 (2.1 to 4) l/minute/m 2 (P = 0.005) and decreased dIVC% from 29.4% to 12.6% (P = 0.003). The median dIVC% in R was higher than NR: 31.3% vs 17% (P <0.05). Fluid therapy decreased more dIVC% in R than in NR: R 31% to 12% (P = 0.03), NR 17% to 12% (P = 0.04). The dIVC% showed similar trend in both groups of septic shock (SS) and trauma shock (TS) patients before and after fl uid therapy: dIVC% 27% in SS and 24% in TS before fl uid therapy; 15% in SS and 11% in TS after therapy. Conclusions Our data suggest that dIVC% is a sensitive index of fl uid responsive ness in septic and trauma patients in shock. Limitations: few patients. One patient received off -pump coronary artery bypass surgery, and fi ve underwent thoracic aortic surgery. Hemodialysis was initiated in one of six patients before operation, while the continuous hemodiafi ltration was initiated in all patients postoperatively. After operation, high-dose catecholamines were necessary in fi ve of six patients for long periods because of severe hypotension. In four of six patients, abdominal pain was the presenting symptom. The rest of two patients had a nonspecifi c presentation because they were ventilated and sedated. All patients presented abdominal distension and their abdominal X-rays showed paralytic ileus features. The serum values of AST, LDH, CK, and lactate were slightly elevated in most patients. Five of six patients died from septic shock and multiple organ failures, and the mortality rate of patients with NOMI was 83%. Potential risk factors contributing to the occurrence of NOMI and sensitive markers might be the following: continuous hemodiafi ltration (6/6); hypotension (5/6); high-dose catecholamines (5/6); dehydration (6/6); abdominal pain (4/6); paralytic ileus patterns of abdominal X-rays (6/6). Conclusions The increase of NOMI incidence following cardiothoracic surgery might be related to continuous hemodiafi ltration, hypotension, dehydration, and uses of high-dose catecholamines. Identifi cation of patients at NOMI risk and prevention of hypovolemic hypotension and use of vasodilator may help to reduce the incidence of NOMI. increased prevalence of CHF in the elderly. The aim of present study was to investigate the major causes, comorbidities and in-hospital mortality of patients with CHF. Methods A retrospective study was performed in 6,960 patients (4,352 males, 2,608 females) with a validated primary discharge diagnosis of CHF hospitalized from 1 January 1993 through 31 December 2007, at Chinese PLA General Hospital in Beijing. The patients were divided into fi ve groups based on the number of etiologies and comorbidities from one to fi ve or more than fi ve. A comparative analysis was performed to explore the major causes, comorbidities and in-hospital mortality of patients among the groups. The mean (± SD) age of patients was 53 ± 17 years in the onecomorbidity group, 60 ± 16 years in the two-comorbidity group, 65 ± 14 years in the three-comorbidity group, 70 ± 13 years in the fourcomorbidity group and 72 ± 11 years in the fi ve-comorbidity group. Introduction Sildenafi l is a phosphodiesterase-type-5 inhibitor and selectively decreases pulmonary artery pressure. So far, the mechanism underlying sildenafi l's eff ects on pulmonary vascular remodeling and potassium channel activity in pulmonary artery smooth muscle cells (PASMCs) has not been clearly addressed in pulmonary hypertension secondary to increased pulmonary blood fl ow. Methods A total of 27 male SD rats were randomly divided into a sham group (n = 9), a shunt group (n = 9) and a shunt + sildenafi l group (n = 9). A left-to-right shunt was established by performing an abdominal aorta to inferior vena cava fi stula both in the shunt group and the shunt + sildenafi l group. Rats in the shunt + sildenafi l group received oral sildenafi l 10 mg/kg/ day, whereas the rats in the sham group and the shunt group were fed with normal saline of the same volume. Eleven weeks later, mean pulmonary artery pressure (mPAP) was measured. Meanwhile, the ratio of right ventricular mass to left ventricular plus septal mass (RV/(LV+S)) was detected as a marker of the degree of right ventricular hypertrophy. The relative medial thickness (RMT) of middle and small pulmonary muscularized arteries was calculated as a sign of pathological changes of pulmonary vasculature. The voltage-gated potassium channel Kv1.5 mRNA expression of pulmonary vasculature was detected using real-time PCR. Results Eleven weeks later, the rats in the shunt group developed pulmonary hypertension as evidenced by signifi cantly increased mPAP, RV/(LV+S), as well as higher RMT of middle and small pulmonary muscularized arteries (all P = 0.01). In addition, the rats in shunt group had decreased Kv1.5 mRNA expression in pulmonary vasculature (P = 0.01). The rats in the shunt + sildenafi l group had a signifi cantly decreased mPAP, and RV/(LV+S) ratio, and a lower RMT as well (all P = 0.01), whereas the levels of Kv1.5 mRNA expression were signifi cantly upregulated (P = 0.01). Furthermore, there were no statistically signifi cant diff erence in mPAP, in RV/(LV+S) ratio, RMT of middle and small pulmonary muscularized arteries and Kv1.5 mRNA expression between the shunt + sildenafi l group and the sham group (all P = NS). Conclusions Oral sildenafi l attenuated pulmonary vascular remodeling and upregulated Kv1.5 mRNA expression in the rats with pulmonary hypertension secondary to left-to-right shunt. Methods Eleven anesthesized pigs were instrumented for the measurement of arterial blood pressure, central venous pressure, RV and pulmonary pressure. An ultrasonic fl owprobe (MA14PAX; Transonic) was positioned on the main pulmonary artery to obtain pulmonary fl ow. Distal to the fl owprobe, a balloon-occluder was positioned facilitating gradual constriction of the pulmonary artery. To obtain a stepwise pressure diff erence increment over the banding of 10 mmHg at each measurement, we gradually infl ated the balloonoccluder. After 10 minutes, all invasive hemodynamic data were registered and an epicardial echocardiography was performed to obtain tricuspid fl ow velocities, isovolumetric and isovolumetric relaxation time. To calculate the TEI index during one heartbeat, echocardiographic measurements were synchronized with fl owprobe measurements to obtain ejection time. The E/E΄ ratio was obtained with tissue Doppler echocardiography of the lateral tricuspid annulus. All echocardiographic measurements were performed in triple and averaged. The ejection period and mean pulmonary acceleration and cardiac output were calculated from the pulmonary fl ow curve derived from the ultrasonic fl owprobe. Resistance over the pulmonary banding was calculated by pressure gradient divided by cardiac output.  In all patients, left ventricular systolic function was not signifi cantly reduced when evaluated with transthoracic echocardiography, while a diastolic dysfunction was always demonstrated. We have recorded a signifi cant (P <0.05) decrease in blood pressure values after clinical stabilization. Estimation of the central aortic pressure waveform by mathematical transformation of radial tonometry pressure was similarly reduced. Other tonometric parameters, such as the Augmentation Index, which represents the contribution of the wave refl ection to the global pulse pressure wave, and PWV, which is inversely correlated to the arterial compliance, were also signifi cantly decreased, when compared with the values at the admission time. The subendocardial viability ratio (SEVR), an indirect index of myocardial perfusion relative to cardiac workload, increased after treatment with vasodilatators. Conclusions Our data confi rm the determining role of the increased arterial stiff ness in the pathogenesis of ACPE, and how a therapeutic strategy able to ameliorate this target can be associated with a clinical improvement for the patient. The applanation tonometry could be an interesting method to evaluate these patients in the ICU setting. We found the probability distributions of the most extreme and the median data for each parameter over 1-minute intervals. The bivariate Gaussian distributions of best fi t for data from critical events (abnormal respiratory rate (RR) or heart rate (HR)) show signifi cant diff erences in covariance when compared with those calculated for data intervals from stable patients (see Figure 1 , in which covariance is indicated by ellipse orientation). Under normal conditions (black ellipses), RR is correlated with HR. During critical events, tachycardia occurs with little or no variation in RR (red ellipses); tachypnoea occurs with little or no variation in HR (blue ellipses). Conclusions Dynamics and correlations that exist in vital signs during periods of stability change signifi cantly during periods of abnormality. A statistical approach that integrates vital signs and captures correlations between them will be sensitive to cardiorespiratory deterioration. The results of NIRS dynamic and hemodynamic variables are shown in Table 1 . R dec StO 2 was signifi cantly increased in the day after the marathon. Signifi cant increases in lactate (2.7 ± 0.5 vs 1.2 ± 0.3) and WBC (14 x 10 3 ± 4 vs 5 x 10 3 ± 2) were observed after the completion of the marathon. Serum CRP, PCT and CK were signifi cantly increased on the day after the marathon (0.6 ± 0.6 vs 12 ± 8; 0.09 ± 0.2 vs 0.7 ± 0.9; 398 ± 198 vs 1,932 ± 1,620), indicating the muscle-specifi c infl ammatory response as a result of muscle damage. Conclusions Delayed increase in thenar VO 2 is associated with musclespecifi c infl ammatory response after strenuous exercise as a result of the muscle damage. Introduction Current clinical parameters on effi ciency of resuscitation are often insuffi cient, and unable to reliably assess tissue hypoxia. Considerable interest has been aimed to direct monitoring of tissue oxygen tension (ptO 2 ). However, no golden standard has yet been found. Animal experimental models have shown interesting results in hypovolemic and septic shock and have lead to some interesting clinical studies. The purpose of this review is to evaluate the current value of ptO 2 monitoring in animal experimental studies under various pathophysiological conditions. Methods An electronic literature search on Pubmed and Embase databases was conducted to fi nd relevant articles on tissue oxygenation and hemorrhage, trauma and endotoxemia. An initial amount of 7,876 articles were retrieved. After applying inclusion and exclusion criteria and critical appraisal, 48 articles were ranked on their level of evidence. Results After screening of the 48 articles, 19 articles were found to be suitable to answer our goal. Ten articles discussed tissue oxygenation in a hemorrhagic model and nine articles in a model of endotoxemia. No relevant articles concerning ptO 2 and experimental trauma were found. Eight articles compared splanchnic ptO 2 measurements in relation to ptO 2 measured in several peripheral sites, like subcutaneous and skeletal muscle tissue. A positive correlation between these locations was found in seven articles. A decrease in both peripheral and central tissue oxygen pressure (ptO 2 ) was found during hemorrhage. Importantly, peripheral ptO 2 changed earlier during hemorrhage before any hemodynamic parameter did. In contrast, only one article found an earlier change in hemodynamic parameters than in peripheral ptO 2 . Nine articles discussed ptO 2 in a model of septic shock. Only two studies compared peripheral ptO 2 measurements with splanchnic ptO 2 measurements. Both sites had decreased ptO 2 values in endotoxemia and both were found to change before any changes in hemodynamic parameters. Conclusions A signifi cant relation between intestinal ptO 2 and peripheral ptO 2 was found in studies on experimental models of hemorrhagic and septic shock. This suggests that peripheral sites like the subcutaneous tissue or skeletal muscles tissue are reliable locations for measurements of ptO 2 . In all, cardiac output (CO), stroke volume (SV), pulse pressure variation (PPV), and standard hemodynamic parameters were continuously recorded. Fluid responsiveness was defi ned as an increase in SV >15%, after a 500 ml colloid (HEA 130/0.4) infusion over 10 minutes. StO 2 measurements and VOT (sphygmomanometer infl ated until >50 mmHg above systolic pressure and kept infl ated until StO 2 decreased to 40%) were performed after anaesthesia induction (baseline), and before (PPV >13%, therefore defi ning hypovolemia) and after fl uid challenge. Main results are reported in Table 1 . Conclusions Metformin can cause hyperlactatemia by impairing platelet mitochondrial function. Results A total of 1,544 patients were included. Complete data to calculate day 1 and day 3 parameters were available in 1,070 and 656 patients, respectively. Overall 30-day mortality was 21.3%. Mean (SD) age and APACHE II score were 63.0 (16.4) and 21.6 (6.9), respectively. For survivors versus nonsurvivors there were signifi cant diff erences in baseline SIDa (P = 0.03), iSIDa, SIDe, lactate and base excess (all P <0.001), but not in SIG (P = 0.1). An increased SIDa (P = 0.015) and SIDe (P <0.001), and decreased iSIDa and lactate (all P <0.001) but not SIG (P = 0.09) were predictive of mortality. For day 1 to day 3, only Δ-SIG was weakly predictive of mortality (OR 1.03 per 1 unit increase, P = 0.04, 95% CI = 1.00 to 1.05, n = 656) but there was no diff erence between survival groups (P = 0.051). Conclusions This study confi rms recent data suggesting baseline diff erences (but not changes over 3 days) in the relationships between strong and weak ions and pCO 2 (refl ected in SIDa, lactate, and impacts on mortality). The diff erent eff ects observed for SIG and BE imply a signifi cant unmeasured acid load infl uencing mortality. Bias and confounding may aff ect these fi ndings and they should therefore be confi rmed prospectively. Introduction Intubation and mechanical ventilation impair secretion clearance and can lead to lung collapse, consolidation and ventilatorassociated pneumonia [1] . There is, however, no valid diagnosis of secretion retention in the intubated and ventilated patient. Vibration response imaging (VRI) is a commercially developed acoustic lung imaging system that displays breath sound distribution [2] . VRI should be able to identify relationships between specifi c breath sound distributions and secretion retention. This preliminary study investigated the changes in VRI measurements before and after chest physiotherapy in adult intubated and mechanically ventilated patients. Methods Intubated and ventilated adult patients who were receiving chest physiotherapy were investigated. Lung sound amplitude at peak inspiration was measured immediately before and after chest physiotherapy using two arrays of sensors attached to the patient's back in a supine position. Chest physiotherapy included combinations of closed airway suctioning, saline lavage, postural drainage, manual techniques and/or lung hyperinfl ation, dependent upon clinical indications. Means are compared with the Wilcoxon matched-pairs signed-ranks test. Results A total of 16 patients were included in the study (12 males, four females, age 65 ± 14). Patients were predominantly ventilated with continuous positive airway pressure and pressure support. Following physiotherapy, total lung sound amplitude at peak inspiration decreased twofold from 37 ± 58.10 6 to 18 ± 23.10 6 arbitrary units (AU), with signifi cant reduction in the left lung (P = 0.03). Furthermore, the diff erence in sound amplitude between right and left lungs signifi cantly decreased posttreatment compared with pretreatment (P = 0.03). The healthy lung ( Figure 1A ) parenchyma showed less deformation caused by fl uidic pressure compared with the lavaged lung ( Figure 1B) . In contrast to the lavaged lung, the deformation of the healthy lung was less when higher P aw was present. Conclusions Micromechanical properties of lung parenchyma can be analyzed in vivo at an alveolar level. The healthy lung parenchyma appears to be stiff er (less deformation) at higher P aw . The stronger deformation and less dependence on airway pressure in the lavaged lung support the hypothesis that small lung compliance in lavaged lungs might not be reasoned by stiff lung parenchyma, but rather by regional collapse. References At every step we studied the changes of FRC, C rs , PaO 2 /FIO 2 ratio and performed a transthoracic echocardiography (Agilent 5500; Hewlett and Packard) to evaluate the integral of velocity time of left ventricular outfl ow tract (LVOT VTI). All data are reported as mean ± SD. ANOVA was used to compare changes during the time. Results Table 1 presents the main results of the study. Best PEEP was set at 10 cmH 2 O, at which level the decrease of FRC and improvement of C rs indicates the start of de-recruitment and end of overdistention. Introduction Electrical impedance tomography (EIT) is a promising new tool for bedside monitoring of regional lung ventilation. Several studies have focused on the ventilation distribution and relationship with regional lung volume on a lower, caudal lung level. However, no information is available at a higher, cranial lung level. Methods EIT (EIT Evaluation Kit 2; Dräger, Lübeck, Germany) was measured at cranial and caudal lung levels in 10 patients after cardiothoracic surgery. Patients were fully sedated and mechanically ventilated and a PEEP trial was performed at four PEEP levels (15, 10, 5 and 0 cmH 2 O). The center of gravity index decreased after lowering the PEEP level at both the caudal and cranial lung levels ( Figure 1 right) . Whereas the tidal volume impedance variation divided by tidal volume increased at the cranial lung level and decreased at the caudal lung level during the step-wise reductions in PEEP (Figure 1  At Tshock there was hemodynamic compromise, statistical decrease in lung compliance (C stat ) and signifi cant increases in pulmonary vascular resistance index, mean pulmonary artery pressure and peak pressure (P peak ), with no statistical diff erence between groups. Endpoints and hemodynamic stability were achieved in the PPV group in 117 ± 28 minutes. C stat continued to deteriorate and P peak continued to rise in both groups from T1 to T3. When compared with PC, the PPV group had signifi cant impedance increases in ROIs 1 and 2 at T2 and T3 and, at T3 the increase in ROI 1 was also statistically greater than Tshock within the group. Statistical decrease in the percentage tidal distribution in ROI 3 and increase in ROI 2 of the PPV group, in relation to the PC group, were also noted. Conclusions Despite re-establishment of hemodynamic adequacy in PPV group and although ventilatory parameters were similar in both groups over time, resuscitation as performed in the study induced signifi cant changes in tidal impedance toward nondependent lung regions, implying greater lung impairment in treated animals. Introduction At the bedside, but even in most research work, the analysis of respiratory system mechanics is limited to quasi-static conditions excluding any insight into what happens during the breath. The new gliding-SLICE method helps looking into the breath. It is a further development of the SLICE method for calculating compliance and resistance of subsequent intratidal volume ranges (slices) of the pressurevolume (PV) loop by multiple linear regression analysis in a continuous way. This allows for detecting intratidal compliance and resistance nonlinearity during ongoing ventilation. Our objective was to determine whether the nonlinear intratidal compliance profi le hints at what level to set PEEP and tidal volume (VT) to make lung ventilation protective. Methods In 12 piglets, atelectasis was induced by application of negative pressure. The PV relationship and the ECG signal were recorded during mechanical ventilation at diff erent levels of end-expiratory pressure (PEEP: 0, 5, 10 and 15 cmH 2 O) and a VT of 12 ml/kg BW. Using the gliding-SLICE method [1] , intratidal compliance profi les were calculated and compared with the conventional quasi-static compliance. Results In contrast to quasi-static compliance, the gliding-SLICE method revealed pronounced intratidal nonlinearity of the compliance profi le under ongoing ventilation (Figure 1 ). At low levels of PEEP, intratidal compliance increased in the low volume range, remained at a high level while further volume was delivered, and fi nally decreased with volume >6 ml/kg BW. With higher levels of PEEP, intratidal compliance decreased from the onset of inspiration. Conclusions The gliding-SLICE method gives detailed insights into the intratidal course of compliance during uninterrupted ventilation. From Introduction Heartbeats transfer mechanical energy to the lungs causing fl ow and pressure disturbances that appear as cardiogenic oscillations (COS) at the airway opening. Here we adopt a new approach for analyzing respiratory system mechanics. We consider the beating heart as a natural intrathoracic mechanical oscillator transferring mechanical energy to the lungs that travels across the lung parenchyma. COS therefore convey information on the mechanical conditions of the lung parenchyma that they cross. Methods In 25 piglets with either healthy or atelectatic lungs, the pressurevolume relationship and the ECG signal were recorded during mechanical ventilation at diff erent levels of end-expiratory pressure (PEEP: 0, 5, 10 and 15 cmH 2 O). The heartbeat-related disturbance of the PV loop was quantifi ed as the maximal compliance following an R-wave in the ECG signal, as determined by the gliding-SLICE method. Atelectasis was assessed by CT. The intratidal pattern of heartbeat-induced C COS changed with PEEP and atelectasis in a characteristic way. With PEEP and tidal volume levels assumed to be lung protective C COS was high with little intratidal changes, compared with atelectasis and overdistension that were signaled by low C COS that either increased (atelectasis) or decreased (overdistension) intratidally. The systolic pressure variations did not parallel the C COS pattern, hinting at a negligible impact of hemodynamics on the inspiratory C COS pattern. Conclusions Heartbeats induce fl uctuations in the PV loop and, as a consequence, peaks in compliance, which show characteristic patterns depending on the presence of atelectasis or overdistension. The gliding-SLICE method has the potential to detect those intratidal nonlinearities without requiring additional technical equipment making use of the ECG signal and the pressure and fl ow signals already required for controlling the ventilator. Introduction Setting the optimal level of positive end-expiratory pressure (PEEP) in the ICU is still a matter of debate. Talmor and colleagues used the transpulmonary pressure calculated from the oesophageal balloon to set PEEP in a recent randomized controlled study. This strategy aims at preventing alveolar collapse by counterbalancing the gravitational force of the lung by an equal or higher PEEP. We evaluated the relation between ventilation distribution measured by EIT and transpulmonary pressure during a PEEP trial in porcine ALI. Methods Eight pigs (30 kg) were studied during a PEEP trial before and after the induction of acute lung injury (ALI) with oleic acid. Global lung parameters, regional compliance, and oesophageal pressure were recorded at the end of each PEEP step. Regional compliance was calculated by dividing the tidal impedance variation (EIT Evaluation Kit 2; Dräger, Lübeck, Germany) by the applied driving pressure. Results Transpulmonary pressures were negative at 0 cmH 2 O PEEP and became positive during the stepwise increase of PEEP at 5 cmH 2 O before, and 10 cmH 2 O PEEP after the induction of ALI ( Figure 1 ). Optimum regional compliance was diff erent between the ventral (nondependent) and dorsal (dependent) regions of interest (ROI). In the healthy lung, optimum PEEP was 10 in the dorsal ROI and 5 in the ventral ROI, whereas after ALI this was 15 in the dorsal ROI and 5 in the ventral ROI. Conclusions If EIT is measured at a caudal lung level, optimal EIT PEEP in the dependent lung exceeds the PEEP required for positive transpulmonary pressures as used in the Talmor study, whereas in the nondependent lung optimal EIT PEEP is equal before ALI and lower after ALI. We speculate that this is probably infl uenced by the location of the EIT slice in the cranial to caudal direction. The results demonstrated a time-course-dependent improvement in compliance and oxygenation, together with clearance of neutro philic infi ltration at 96 hours. TNFα, and IL-1β, IL-6 and IL-10 were signifi cantly elevated in BAL fl uid early post injury. Although total lung collagen remained similar at all time points, evidence of an early fi broproliferative response was present in the form of transforming growth factor-β activation and pro-collagen I and III peptide mRNA levels. Matrix metalloproteinase 3 and 9 zymography demonstrated increased levels of these matrikines. Histologic assessment of injury revealed increased alveolar tissue fraction up to and including 96 hours post injury. Myofi broblasts were present in α-smooth muscle actin stained sections in signifi cantly increased numbers post injury. Conclusions This rat model of repair of VILI demonstrates some of the mechanisms by which excessive lung stretch can contribute to fi broproliferation in ARDS and will serve to improve our knowledge of aberrant lung tissue remodelling as well as provide a useful paradigm for testing strategies to hasten recovery in ALI.  In the control group, C rs (ml/cmH 2 O) did not change during hemorrhage or re-transfusion ( Figure 1 ). In the ARDS group, C rs decreased with lung lavage (31.2 ± 5.7 (baseline) to 16.4 ± 3.0; *P <0.01). After hemorrhage C rs increased (21.5 ± 2.9; **P <0.001 compared with lavage) and then decreased again after re-transfusion (18.7 ± 2.7; ***P <0.05). In the same group PaO 2 /FiO 2 (mmHg) decreased after ARDS (469 ± 50 (baseline) to 105 ± 38; P <0.001), increased during hemorrhage (218 ± 105; P <0.05) and did not change after re-transfusion (207 ± 125; P = 0.82). The shunt fraction (%) decreased during hemorrhage in the ARDS group (26.2 ± 14.9 (lavage) to 6.4 ± 6.6; P <0.05) but did not change signifi cantly after re-transfusion (13.9 ± 17.0; P = 0.3). Conclusions Acute reduction of blood volume is associated with an increase of respiratory system compliance and oxygenation parameters. Reduction of intrapulmonary blood and interstitial fl uid volume or thoracic cage compliance could be responsible for this eff ect. Introduction Trials comparing higher versus lower levels of positive end-expiratory pressure (PEEP) in adults with acute lung injury or acute respiratory distress syndrome (ARDS) were underpowered to detect small but important eff ects on mortality, overall or in any subgroups. Methods We searched MEDLINE, Embase, and the Cochrane Central Register for trials randomly assigning adults with acute lung injury or ARDS to higher versus lower levels of PEEP (minimal diff erence of 3 cmH 2 O over fi rst 3 days), while using low tidal volume ventilation, and comparing mortality. Data from 2,299 individual patients in three trials were analyzed using uniform outcome defi nitions. We tested prespecifi ed eff ect modifi ers using multivariable hierarchical regression, adjusting for important prognostic factors and clustering eff ects. Results Overall, there were 374 hospital deaths (32.9%) in the higher PEEP group and 409 (35.2%) in the lower PEEP group (adjusted relative risk, 0.94; 95% confi dence interval (CI), 0.86 to 1.04; P = 0.25). Treatment eff ects varied with the presence or absence of ARDS, defi ned by a ratio of partial pressure of oxygen to fractional inspired oxygen concentration equal to or less than 200 mmHg (interaction P = 0.02). The relative risks of hospital mortality for patients with and without ARDS were 0.90 (95% CI, 0.81 to 1.00, P = 0.049) and 1.37 (95% CI, 0.98 to 1.92, P = 0.065), respectively. Patients with ARDS were more likely to achieve unassisted breathing earlier (hazard ratio, 1.16 (95% CI, 1.03 to 1.30, P = 0.01); whereas the hazard ratio for time to unassisted breathing was 0.79 (95% CI, 0.62 to 0.99, P = 0.04) in patients without ARDS at baseline. Rates of pneumothorax and the use of neuromuscular blockers, vasopressors and corticosteroids were similar. Conclusions Higher levels of PEEP are likely to improve survival for patients with ARDS, but not for patients with less severe acute lung injury. Introduction Experimental and clinical studies have shown benefi cial eff ects of recruitment maneuvers (RMs) (sustained infl ation (SI) or SIGH) on ventilatory and gas exchange parameters. In this study we investigated the eff ect of diff erent RMs on bacterial translocation from lung to blood. Methods Thirty-two rats were anesthetized, after tracheotomy was performed ventilation was started with 10 cmH 2 O P aw , 0 cmH 2 O PEEP, 60 breaths/minute, I/E: 1/2 on pressure-controlled ventilation (PCV) mode. After cannulation of the carotid artery was performed, a baseline blood gas sample was taken. Subsequently 0.5 ml of 10 5 cfu/ml Pseudomonas aeruginosa was inoculated through the tracheotomy tube and PEEP was increased to 3 cmH 2 O and ventilated for 30 minutes before randomization. Then rats were randomized into four groups: G1; SI was performed as 40 cmH 2 O PEEP and 0 P aw for 20 seconds, four times in an hour (15-minute intervals), G2; SI was performed as 20 cmH 2 O PEEP and 0 P aw for 40 seconds, four times in an hour (15-minute intervals), G3; SIGH was performed four times in 1 hour (15-minute intervals) as 40 cmH 2 O P aw , 3 mH 2 O PEEP for 60 seconds, G4; control group that was ventilated with P aw 10 cmH 2 O, PEEP 3cmH 2 O during the study period. Multiplication of pressure and pressure performing time for each study group were equal. Blood cultures were taken at baseline, 15 minutes after randomization, which is after each RM for the fi rst hour, and last blood culture was taken after 60 minutes from the fourth RM. Then rats were sacrifi ced with intra-arterial sodium thiopental, and the lungs were extirpated; the left lung was taken for measurement of the wet weight/dry weight ratio (WW/DW). Results There were no diff erences in baseline pH, PaO 2 , PaCO 2 , MAP, HR among groups. But PaO 2 were decreased in groups G1, G2, and G3, but only in G3 was statistically signifi cant to compared baseline values. The WW/DW ratio was found higher in G3 when compared with G1, and G2, but this diff erence was not signifi cant. The amount of positive blood culture was higher in G3 at early study periods. The alveolar pressure P alv reduced as the frequency increased (for both collapsed and open alveoli). Compared with the situation at 0.2 Hz, P alv reduced to 9.1 ± 1.0% at 4 Hz. On the other hand, P alv increased as the percentage of atelectatic area increased (for all frequencies). P alv increased to 267 ± 6% when 70% of alveoli collapsed compared with 10% of collapse. The pressure diff erences between collapsed and open alveoli increased as the frequency increased. At 0.2 Hz the diff erences were <3%, while at 4 Hz the diff erences were >20% (relative to the lower value). Introduction Low tidal volumes (Vt) are thought to protect the lung by avoiding overdistension. We recently have shown that Vt may also infl uence repeated opening and closing (O/C) [1] . The goal of this study was to determine whether decreasing Vt from 6 to 4 ml/kg was eff ective to reduce O/C, and whether it was possible to maintain alveolar ventilation at such low Vt. Methods Cross-over study at two Vt levels: 6 versus 4 ml/kg IBW. We included ALI/ARDS patients, ventilated <48 hours, and who would have a chest computed tomography (CT) scan. For the 4 ml/kg arm: we replaced the heat and moisture exchange fi lter by a heated humidifi er, and the respiratory rate was increased to keep minute ventilation constant. The protocol had two parts: one bedside and other in the CT room. Both Vts were applied in a random order. For the bedside protocol each Vt arm was applied for 30 minutes. Data on lung mechanics and gas exchange were taken at baseline and 30 minutes. For the CT scan protocol each Vt arm was applied for 5 minutes and then a dynamic CT (4 images/second for 8 seconds) was taken at each Vt at a fi xed transverse region at the lower third of the lungs. Afterwards, CT images were analyzed by software (MALUNA) and repeated O/C was determined as nonaerated tissue variation between inspiration and expiration, expressed as a percentage of lung tissue weight. We analyzed nine patients (six male), who had a median age of 39 (21 to 72) years, APACHE II score 14 (5 to 23) and SOFA score 9 (6 to 15). All patients had a pulmonary origin of their ARDS and were on their fi rst day of ventilation. At baseline patients had a PaO 2 /FiO 2 ratio of 141 (71 to 280), compliance of 32 (17 to 43) ml/cmH 2 O, and PEEP of 12 (10 to 16) cmH 2 O. In the Vt arms 4 and 6 ml/kg, Vts were 260 (210 to 300) and 350 (310 to 400) ml, respectively (P <0.01), respiratory rates were 37 (31 to 42) and 25 (21 to 28) breaths per minute (P <0.01), and PaO 2 levels were 84 (54 to 148) and 83 (61 to 162) mmHg (P = 0.3). PEEP and FiO 2 were kept constant. PaCO 2 did not signifi cantly increase with Vt 4 but repeated O/C (delta nonaerated tissue) consistently decreased ( Figure 1 ). Introduction Acute lung injury (ALI) and acute respiratory distress syndrome (ARDS) are associated with signifi cant morbidity and mortality. Mechanical ventilation is the cornerstone of supportive therapy. However, the optimal strategy of ventilation and adjunctive therapies are still evolving. There is evidence to support the use of volume-limited and pressure-limited lung-protective ventilation but practice variability in the clinical management is still a concern mainly in sicker patients. and hospital outcome have not found a consistent outcome association. We hypothesized that severity of hypoxemia on admission after optimal ventilation may predict hospital outcome if the ARDS patients are categorised based on the primary etiology: pulmonary ARDS (ARDS p ) and extra pulmonary ARDS (ARDS exp ). Our aim was therefore to ascertain the relationship between hospital outcome and the severity of hypoxemia in patients with early ARDS (days 1 to 3 following admission) after categorising based on etiology. Methods We used a prospective cohort study design and enrolled 151 consecutive patients with a primary diagnosis of ARDS on the day of admission, admitted over a 2-year period to our adult general ICU. Patients enrolled in other clinical interventional trials were excluded from the study. Protocol-based management of mechanical ventilation was used to achieve optimal ventilation. Two authors independently designated patients as ARDS p or ARDS exp and a third reviewed any confl icts (only four cases). Patients were then subcategorised by severity of hypoxaemia based on PaO 2 /FiO 2 ratio (P/F ratio) intervals. All other clinical interventions were at the discretion of treating clinician. The hospital mortality in all patients included in the study (n = 151) with ARDS was 44.1%. The patients classifi ed as ARDS p had a higher hospital mortality (50.6%) compared with ARDS exp (36.4%), but the diff erence was not statistically signifi cant (P = 0.12). Nonsurvivors with ARDS p had a signifi cantly lower P/F ratio on day 1 of ARDS diagnosis compared with survivors (12.05 ± 4.46 vs 15.39 ± 4.97 kPa; P = 0.002), a relationship not observed with ARDS exp . This association between mortality and hypoxaemia in early ARDS p persisted at even more serve levels of hypoxaemia (<15 kPa, <12.5 kPa, and <10 kPa; P = 0.01, P = 0.003 and P = 0.002, respectively), while in ARDS exp the eff ect of hypoxaemia on mortality was not observed. Conclusions Our fi ndings indicate that the hypoxaemia burden as assessed by P/F ratio intervals despite optimal ventilatory support on the day of ICU admission predicts increased risk of death in ARDS p ; but not in ARDS exp . Interventional trials need to account for the infl uence of etiology and hypoxaemia burden on outcome prior to concluding this as a negative intervention.  The P/F ratio decreased with time and was below 300 from day 4 onwards. However, the survivors maintained a P/F ratio of 300 or more. In the patients with a fatal outcome, the ratio continued to decline and the subjects developed acute lung injury on day 3. From day 5 onward, they showed a signifi cant decrease of the ratio, with values of 200 or less and symptoms of adult respiratory distress syndrome. The levels of IL-6, IL-8, ICAM-1, and GE increased after admission, but then decreased again in the survivors. In the patients who died, these levels continued to rise and there was a signifi cant increase of IL-6, IL-8, and ICAM-1 after 1 week. The correlation between the blood level of IL-6 and the level of IL-8, ICAM-1, or GE was strong, while that between the IL-8 level and the ICAM-1 or GE levels was also strong and that between the ICAM-1 and GE level was weaker. In contrast, the correlations between the P/F ratio and the blood levels of IL-6, IL-8, ICAM-1, or GE were moderate and negative. Therefore, inverse correlations were noted between the P/F ratio and all these parameters. Conclusions By determining the changes of humoral mediators, we demonstrated that vascular endothelial damage was involved in the occurrence of the pathological state of acute lung injury, which occurs in severe TBI patients with SIRS. Introduction With adaptive support ventilation (ASV), a microprocessorcontrolled mode of mechanical ventilation, the ventilator adapts tidal volume (VT) size based on the Otis least work of breathing formula. In recent studies in patients with ALI/ARDS, ASV applied VT of 7.3 (6.7 to 8.8) ml/kg ideal body weight (IBW). It is unclear whether an open-lung approach was used in these studies. Lung recruitment improves lung compliance, and as a consequence may allow the ventilator to apply too large a VT with ASV. Methods Ten consecutive patients with ALI/ARDS, ventilated in accordance with our local protocol dictating frequent recruitment maneuvers, were observed while the ventilator was switched from pressure control ventilation (PC) to ASV. Thereafter, all patients were subjected to an additional standard recruitment procedure. The primary endpoint was VT before and after switch of the ventilator, and after standard recruitment. Results Four patients suff ered from ALI, six patients from ARDS. Seven patients had an extrapulmonary cause for ALI/ARDS. VT increased from 6.5 ± 0.8 ml/kg IBW to 9.0 ± 1.6 ml/kg IBW (P <0.01) after switch from PC to ASV. Additional recruitment after switch of the ventilator did not aff ect VT size (9.3 ± 1.4 ml/kg IBW, P >0.05). In seven patients ASV applied VT >8 ml/kg IBW, in one patient VT even increased to >12 ml/kg IBW. Conclusions Patients with ALI/ARDS may be ventilated with too large a VT when subjected to ASV. Our results contrast fi ndings of previous studies on ASV in patients with ALI/ARDS, probably because we frequently use recruitment maneuvers. Conference criteria). We measured plasma RAGE levels twice on the fi rst 2 days from intubation, then every 3 days for the fi rst month and then once a week, until ICU discharge or death (n = 188). We also measured RAGE levels in BALf obtained by means of a standardized technique when clinically indicated (n = 22). At each sampling time we recorded data on ventilator settings, gas exchange, organ function and blood cell counts. Between 2004 and 2009 our crew evaluated for transfer on ECMO 15 ARDS patients (10 males), age 38 ± 15 years, BMI 28 ± 7, APACHE II score 26 ± 9, SOFA score 9 ± 4, Oxygenation Index 39 ± 17. The average distance was 133 ± 124 km. Two patients improved after NO trial and were transferred without ECMO. All of the other patients underwent venovenous ECMO: 11 with cannulation of femoral veins, one femoral-jugular veins and one with a DL cannula in the jugular vein. ECMO settings were (mean ± SD) BF 2.9 ± 0.8, GF 3.6 ± 1.6, GF FiO 2 1. Data have been recorded 30 minutes before and 1 hour after ECLS began: vv-ECMO granted a better clearance of pCO 2 (75 ± 20.5 vs 49.7 ± 7.9 mmHg, P <0.01), thus improving the pH (7.279 ± 0.10 vs 7.41 ± 0.06, P <0.01) and mean pulmonary arterial pressure (41 ± 11 vs 31 ± 5 mmHg, P <0.05) and allowing a reduction in respiratory rate (28 ± 11 vs 9 ± 4, P <0.01), minute ventilation (10.2 ± 4.6 vs 3.3 ± 1.7 l/min, P <0.01) and mean airway pressure (26 ± 6 vs 22 ± 5 cmH 2 O, P <0.01). Arterial pO 2 , mean blood pressure and heart rate did not show signifi cant variations. After ECMO began, vasoconstrictor therapy (being administered to fi ve patients) was quickly tapered. Neither clinical nor technical major complications were reported. Conclusions ECMO employment at referral centers enabled longdistance, high-risk ground transportation.  After 24 hours, diaphragm forces were signifi cantly lower in MVSAL compared with all groups. Administration of NAC completely abolished this decrease such that forces produced in the MVNAC group were comparable with those of both SB groups. Protein oxidation was signifi cantly increased in MVSAL (+53%, P <0.01) and was restored in MVNAC. Diaphragm caspase-3 activity was signifi cantly increased in MVSAL compared with SBSAL (+279%, P <0.001). Caspase-3 activity was also increased in the MVNAC group (+158.5%, P <0.01) but to a signifi cantly lesser extent compared with that of MVSAL. Calpain activity was signifi cantly increased after CMV (+137%, P <0.001 vs SBSAL), while it was similar to SB groups in the MVNAC group. Signifi cant negative correlation was found between calpain activity and diaphragm tetanic force (r = -0.48, P = 0.02). Conclusions These data show that the administration of NAC was able to preserve the diaphragm from the deleterious eff ects of CMV. NAC inhibits the increase in oxidative stress and proteolysis and reduces the decrease in force generating capacity of the diaphragm. We observed synchrony between Edi signal and ventilator breaths in pressure control or Bivent, and when the patients have started spontaneous breaths (T signal) we move to pressure support. Low Edi minimum (0 to 1) was associated with overdistending the diaphragm and required decreasing the PEEP, contrary to high Edi minimum that was associated with higher tonic activity and dictated to raise the PEEP. During NAVA, the pressure delivered was proportional to the Edi. The NAVA level was continuously readjusted in proportion to the predicted inspiratory eff ort from the Edi signal. At the highest assist level, we found lower Vt/kg (6.1 ± 3 ml/kg vs 8.1 ± 1.8, P <0.001), and higher breathing frequency (19 ± 6.9 vs 11 ± 7, P <0.001) and peak EAdi (11.8 ± 8.5 vs 8.1 ± 7.7, P <0.002) in NAVA than in PSV. The main indications for mechanical ventilation were sepsis/ shock (35%), acute respiratory failure (33%), cardiopulmonary resuscitation (16%) and neurologic disease (10%). Respectively, 35%, 28% and 24% of the patients were extubated, discharged from the ICU and from the hospital. For patients treated with NIV prior to IMV, the rates were 22%, 17% and 10%, respectively. In multivariate analysis, three variables were independently associated with a decreased probability of being discharged from the hospital: NIV use before IMV (OR = 0.30, 95% CI: 0.09 to 0.95; P = 0.04); leucopenia (OR = 0.21, 95% CI: 0.06 to 0.77; P = 0.02) and serum bilirubin >1.1 mg/dl (OR = 0.38, 95% CI: 0.16 to 0.94; P = 0.04). Conclusions NIV failure before IMV is an independent poor prognostic factor in cancer patients treated by IMV. The total number of tracheotomies during the study period was 220, of which 125 (56%) were PT and 95(44%) were OT. Both groups were similar in age, sex and ISS distribution. Of the OT group, 60 (63%) were done in patients with no cervical spine clearance or cervical spine injury. There were no immediate complications reported in the OT group. The PT group had 63 cases (50.4%) done with no preoperative cervical spine clearance or positive for cervical spine injury. The PT group underwent the procedure without bronchoscopy assistance in 95% of the cases. Two cases (1.5%) in the PT group were reported with postoperative bleeding from the tracheostomy site that did not required intervention. Both cases were PT done without bronchoscopy assistance and did not have preoperative cervical spine clearance. No other immediate complications were reported. Conclusions The results of this study suggest that PT is safe in trauma patients without preoperative cervical spine clearance or with cervical injuries as compared with the OT group. Most of the PT cases were done without bronchoscopy assistance (95%). This fi nding suggests the need for further study to clarify the role of bronchoscopy assistance in PT. Introduction Mechanically ventilated (MV) patients are prone to develop ventilator-associated pneumonia. One of the major risk factors is microaspirations of supraglottic secretions past the endotracheal tube cuff (usually in polyvinyl (PV)). A novel polyurethane (PUE) cuff was designed to minimize these leakages. We therefore compared the sealing capacities of the two tubes in MV patients. Methods Twenty-nine consecutive MV patients (mean age ± SD: 68 ± 13, 21 males), were randomly allocated to receive either a PV (HI-LO Evac, Mallinckrodt) or a PUE (SEALGUARD Evac, Mallinckrodt) cuff ed endotracheal tube (size: 9 for men; 8 or 8.5 for women, as a rule). We excluded patients with emergency intubation, unstable haemodynamics, severe respiratory failure or patients with history of tracheal/laryngeal disease. In each patient, cuff pressure was maintained at 30 cmH 2 O, and ventilator parameters were set to plateau pressure ≤30 cmH 2 O; patients were fasting and placed in a strict semirecumbent position (45°). Radioactivity of tracheal aspirates was assessed sequentially (hourly samplings from T0 to T6 hours, then T8 hours and T12 hours) after injection of 74 MBq 99m Tc-DTPA diluted in 5 ml of 0.9% NaCl just above the cuff via the aspiration channel of the tube. Additionally, kinetics of respiratory tract contamination was followed by simultaneous pulmonary images using a scintillation camera. Data were blindly analysed by nuclear physicians. The study was approved by the hospital ethics committee and informed consent was obtained from relatives. Results Sixteen PUE and 13 PV cuff ed tubes were compared. The study was performed 3.2 ± 2.8 days after intubation and 8.3 ± 9.6 days after ICU admission (mean ± SD). Ventilator parameters were the following: volume control or pressure support but one on T tube, FiO 2 was 0.43 ± 0.14, PEEP 6 ± 2 cmH 2 O. Leakages were observed in 11/29 patients (38%), with similar rate of aspiration in PUE (5/16) and PV (6/13) groups (P = NS). Leakages were more frequently observed in female (7/8) than in male patients (4/21) (P <0.001). There was a trend to decreased frequency of aspiration in patients with larger tubes (size 9 vs 8.5: P = 0.062). Of the 14 pigs, one had to be excluded due to accidental cuff defl ation. Tube sizes were evenly distributed amongst the groups. Cuff pressures were equal: TG 23.7, HL 25.2 -P <0.2. As seen in Table 1 , the incidence of microaspiration was signifi cantly less for TG in the Blue Dye and bronchitis groups. following extubation. However, the true incidence of laryngeal edema in postoperative patients is not clear. We assessed the relationship between upper airway obstruction and the values of cuff -leak pressure in postoperative patients. Methods One hundred and fi fty-eight postoperative patients (123 elective, 35 emergency) were included. After ventilator weaning was accomplished, we measured the airway pressure at which a sound of cuff leakage was audible by a cuff pressure monitor. In 28 cases, the cuff leak pressure was measured during both awake and sedated states. The cuff leak pressure value was 12.8 ± 10.1 (median 10) mmHg for elective cases and 12.6 ± 10.0 (median 10) for emergency cases (NS). Six patients (3.8%) were not extubated because of high leak pressure and the value was 40.5 ± 16.0 (24 to 60) mmHg. One hundred and fortythree patients were extubated and nine of those (6.3%) were diagnosed as laryngeal edema by laryngoscopy. Seven of those (5.0%) needed reintubation, one was for the reason of massive sputa, three were for granuloma formation, and three (2.1%) were diagnosed as severe laryngeal edema. Patients who developed severe laryngeal edema had a higher leak pressure (27.2 ± 22.7 mmHg) than those who did not, and all of such patients had pressure above 20 mmHg. The sensitivity and the specifi city of the test using a threshold value of 20 mmHg for severe laryngeal edema were 97.2% and 40.0%, respectively. The occurrence of severe laryngeal edema was not associated with age, gender, perioperative weight gain, duration of translaryngeal intubation, inner diameter of the endotracheal tube, or serum albumin concentration. However, the cuff leak pressure value >20 mmHg was associated with gender (female, P = 0.02) and inner diameter of the endotracheal tube (P = 0.0017) by multivariate regression. In 28 patients, the cuff leak pressure was 17.8 ± 10.6 during the awake state and 9.9 ± 4.6mmHg under sedative. Because tonus of larynx is considered to be related to cuff leak pressure, it is useful to measure the cuff leak pressure in a sedated state if the patient had a high value in the awake state. Conclusions Cuff leak pressure values <20 mmHg at any time are useful to rule out severe laryngeal edema. It may be useful to measure the pressure in a sedated state if the value in the awake state is high.  The variation of cardiac index (CI) was: T1 2.53 ± 0.43 ml/minute/m 2 CPE vs 3.2 ± 0.64 ml/minute/m 2 COPD; T2 2.93 ± 0.93 ml/minute/m 2 CPE vs 2.9 ± 1.33 ml/minute/m 2 COPD; T3 1.96 ± 0.61ml/minute/m 2 CPE vs 3.3 ± 1.37 ml/minute/m 2 COPD. The variation of CI depended on the variation of stroke volume (SV), while the heart rate (HR) did not change during the trial. We calculated the oxygen delivery (DO 2 ) by the correlation of CI and gas exchange: T1 767 ± 218 ml/minute CPE vs 839 ± 134.3 ml/minute COPD, T2 917 ± 417 ml/minute CPE vs 701 ± 349 ml/minute COPD, T3 760 ± 404 ml/minute CPE vs 753 ± 340 ml/minute COPD. The variation of CI and DO 2 , observed within and among the two groups was never signifi cant. Conclusions Both groups of patients were successful in achieving spontaneous ventilation. In our opinion, continuous hemodynamic monitoring may provide helpful beat-to-beat information and it might be used, combined with the gas exchange and oxygen saturation monitoring, during the weaning process as a predictor of cardiovascular instability or respiratory failure. Moreover, continuous hemodynamic monitoring enables one to be aware of the variation of systemic oxygen delivery, and these data could be used to value critically ill patients during the weaning process. Introduction Obesity rates are increasing in the general population and it is also prevalent in ICUs. Patients are sometimes admitted to ICUs for hypercapnic respiratory failure or cor pulmonale but in general they are admitted for pneumonia, excessive daytime sleepiness, heart failure, COPD or asthma attacks or pulmonary embolism; and hypercapnic respiratory failure is noticed during this period. On the other hand, optimal non-invasive mechanical ventilation strategy is not known during their ICU treatment. The aim of this study is to assess the diff erences between NIV strategies and outcomes between obese and nonobese patients with acute hypercapnic respiratory failure. Methods In this retrospective cohort study, 73 patients were studied and all of them were ventilated with a face mask. Patients divided into two groups as obese (BMI >35 kg/m 2 ) and nonobese (BMI <35 kg/m 2 ), and whether necessary pressure, volume, mode, ventilator and time to reduce PaCO 2 below 50 mmHg were signifi cantly diff erent in obese and nonobese patients was investigated. Results Mean age of the patients was 66 ± 14 years and mean admission APACHE II score was 18 ± 4; 41 (56%) of them were female. ICU admission reasons for the obese patients were signifi cantly more frequently pulmonary edema and less frequently pulmonary infections (P = 0.003 and 0.043, respectively) than the nonobese patients. While there were no signifi cant diff erences across the groups between the ventilators, modes, and inspiratory pressure levels, obese patients required higher end-expiratory pressure levels and more time to reduce the PaCO 2 level below 50 mmHg than the nonobese group. Length of NIV and ICU stay, intubation and the mortality rate were similar across the groups. Conclusions These results suggest that improvement of hypercapnia in obese patients may require higher PEEP levels and longer times than the non-obese ones during acute hypercapnic respiratory failure attack. The mean (± SD) age and SOFA scores were 65.3 ± 16.7 years and 10.0 ± 3.9, respectively, and 35% required prolonged MV. Univariate analysis indicated that the length of ICU and hospital stays, hospital mortality, the rate of transfusion, incidence of ARDS, ventilator-associated pneumonia (VAP), other nosocomial infection (NI) and drug-resistant bacteria, the ratio of steroid therapy and muscle relaxant use, and the mean PaO 2 /FiO 2 ratio during the fi rst 3 days after admission were signifi cantly diff erent between the two groups. The independent predictors for prolonged MV were ARDS (OR 5.24 (P = 0.001; 95% CI: 1.9 to 14.1)), VAP (OR 7.75 (P <0.001; 95% CI: 2.7 to 22.0)), and transfusion (OR 2.84 (P = 0.036; 95% CI: 1.0 to 7.5)) Using these results, we were able to develop a prolonged MV predictive scoring system. This simplifi ed clinical risk assessment tool was developed from Introduction Severity scoring is a powerful tool for quality control in the ICU. We introduced a new severity scoring system to our ICU. The aim of this report is to describe the database and present the results of the fi rst 12 months. Methods We analysed needs for database and severity scoring. We chose on-admission scoring with EUROSCORE for cardiac surgical patients, and Mortality Prediction Model II time zero for other patients. Data were collected via highly simplifi ed forms onto a spreadsheet. Demographic entry was by ward clerk, on-admission severity scoring was by resident medical staff , and risk of death calculation and data cleaning was by senior attending medical staff . Individual patient risk of death was presented as a logit. Combined risk of death for the whole cohort was calculated from the arithmetic mean of individual logits. We estimated the time to completion of each step in data acquisition to calculate a total time spent per patient. Results There were 1,355 admissions, mean (SD) age 69.2 (15.9) years, 57.1% male; median (range) length of stay 23.1 (1.7 to 1,882.5) hours. Fiftyfour per cent of patients were ventilated for a median (range) 7.5 (0.8 to 1,877) hours. Predicted mortality for cardiothoracic and noncardiothoracic patients combined was 8.64% and the observed mortality was 2.8%. EUROSCORE-predicted mortality for the 535 cardiothoracic patients was 5.72% and the observed mortality was 0.75% (four patients). MPMpredicted mortality for the 820 noncardiothoracic patients was 11.23% and the observed mortality was 4% (34 patients). Estimated time to complete the severity scoring form was 30 seconds, to enter a new patient on the database was 90 seconds and to calculate risk of death and check data integrity was 90 seconds. Total was 3.5 minutes per patient. Conclusions Data collection/analysis is essential for quality management in the ICU. Proprietary systems are expensive. Traditional scoring systems (APACHE II) are poorly calibrated to some case mixes. To overcome these problems, we devised a simple, inexpensive and highly valuable ICU database. The key features were well calibrated, on-admission severity scoring, highly simplifi ed forms, a basic spreadsheet and collaborative staff involvement. Senior medical staff performed the fi nal data checking. The project provided abundant high-quality data with a total input of 3.5 minutes per patient. We are trialling the database in a large ICU in China and we would welcome input from other ICUs that would like to copy our methods. In 2007, 430 patients were admitted to our ICU. One hundred and forty were older than 60 years and remained more than 5 days in the ICU. Eleven of these patients had a SOFA score higher than 9 during 5 days or more. In the SOFA(+) group, LOS (22.6 ± 13.1 vs 10.8 ± 6.9 days) and mortality (55% vs 33%) are signifi cantly higher (P <0.05). The mortality of 55% is in good agreement with the predicted IGSII mortality but far less than the 100% predicted by Cabré and colleagues. Conclusions A SOFA score higher than 9 for at least 5 days in patients older than 60 years seems useless to defi ne futility in our patient population. It is applicable to only less than 3% of the 430 patients admitted in 2007. The mortality rate of the SOFA(+) group increased but remained far from values permitting to withdraw or withhold intensive care. A weakness of the use of sequential SOFA score to predict outcome is that some therapeutic options can infl uence the value of the SOFA score. Introduction Prolonged ICU stay is associated with high morbidity, mortality and costs [1, 2] . Prediction of this prolonged stay will provide information for physician and family and help with resource allocation. Even though, available severity scoring system ie. APACHE II, APACHE III, MPM, SAPS II, MODS scores are widely accepted for evaluating outcomes in the ICU population. But these models might be inaccurate when apply to subpopulation and might not predict prolonged length of stay. Conclusions All three scores are useful prognostic factors for mortality and for ICU therapy in the ED, with usually lower patients' severity of infection than in the ICU. The ICU-validated APACHE II and SOFA scores were of similar prognostic value as the ED-specifi c MEDS score. The ultrasound images helped giving the hematoma block under direct vision and delineated the fractures as accurately as did the conventional radiographs. All parameters measured on the ultrasound images showed substantial restoration of anatomic alignment after reduction. Under direct vision, the hematoma block helped reduce the number of attempts and was less painful and achieved more patient satisfaction. Conclusions Sonography is an accurate, simple, time-saving, less painful and radiation-free tool. Reducing badly displaced or angulated forearm fractures in the emergency department can be diffi cult. Multiple attempts at reduction may be required, with repeated trips to the radiology department, before an adequate reduction is achieved [2] . Therefore ultrasound-guided hematoma block and reduction of diffi cult forearm fractures reduces the number of needle attempts and allows the physician to assess the adequacy of the reduction at the patient's bedside. Introduction Patients with acute cardiogenic pulmonary edema require rapid assessment and therapy to prevent progression to respiratory failure and cardiovascular collapse. Lisbon city has emergency medical teams that respond to situations where the life of the patient is at risk and whose goal is to begin treatment if indicated and assure transport to the hospital in the best conditions possible. We studied the intervention of one of these teams on patients with acute pulmonary edema.  Of the 105 patients included in triage scenarios, 66 (63%) were women, the mean age was 43.7 years (SD ± 26.3), and 22 were under the age of 18 years. The most frequent presentation at triage was minor trauma (30%). There were 30 hospital admissions: 27 in non-intensive wards and three in ICUs. The mean age of students was 24 ± 3 SD. Few participants attended triage training before (18%) and they declared scarce triage knowledge. Inter-rater reliability was k = 0.42 (95% CI: 0.37 to 0.46) and k = 0.61 (95% CI: 0.56 to 0.67) before the course (without triage protocol) and after the course (with TEMv2), respectively. Complete disagreement occurred in 98% of scenarios evaluated before and in 64% after the course. Complete agreement was always zero. Conclusions Our data suggest that TEM v2 improves triage reliability among nursing students. It seems to be easy to understand and to use.  We observed an increase in proapoptotic markers in patients with infectious complications from the fi rst day in the ICU, antiapoptotic markers (Cu/Zn superoxide dismutase) were lower with regard to reference points. The patients demonstrated a maximum of Fas-L, protein p53, and protein Bcl-2 (near 50% relative to fi rst day) on the second day. In patients without infection complications we identifi ed a decreased level of proapoptotic markers and an increased level of antiapoptotic markers (SOD, soluble Fas-L, soluble APO-1/Fas Introduction Inadequate or delayed imaging of the unconscious patient with traumatic brain injury or of the unconscious polytrauma patient may lead to catastrophic consequences. The need for comprehensive imaging needs to be balanced against the patient's condition, the resources available and excessive radiation exposure. We have introduced regional evidence-based guidelines in the Southwest region of the UK. The aim of these guidelines was to standardise practice, minimise delayed or missed diagnosis of serious injuries, facilitate treatment of associated injuries (such as head injuries) and to obviate the need for repeated imaging. Methods The notes of all unconscious polytrauma patients transferred from the emergency department or other hospitals and all patients transferred to our institution for ongoing neurosurgical care were retrospectively reviewed over a 1-year period. Adherence to the imaging protocol was assessed and the need for any further radiological investigations within 48 hours was also documented. Results A total of 46 patients were identifi ed who fulfi lled the criteria for the introduced guidelines. Of these patients, 21% were transferred from other hospitals while the remainder was admitted from the onsite emergency department. Eighty-three per cent of all eligible patients adhered to the protocol. Two patients (4%) required further radiological investigation in the 48 hours following admission. Both had isolated head injury and required further imaging to exclude cervical injuries following inadequate imaging which did not follow protocol. One further patient (2%) required repeated imaging to exclude mesenteric injury. Conclusions Regional adoption of imaging guidelines aimed at obtaining early comprehensive imaging of both head-injured and polytrauma patients results in high compliance and a low rate of re-imaging. The mean sodium level at admission was 140.1 ± 4.1 mmol/l. Hypernatraemia was detected in 26 patients (33.8%). The mean duration of the period of hypernatraemia in Group B was 4 days (3 to 6 days), while the mean sodium level during the period of hypernatraemia was 158.3 ± 3.3 mmol/l (max 176.8 mmol/l). The duration of the period of hypernatraemia in Group C was 4.5 days with max 181.1 mmol/l and average 161 ± 4.7 mmol/l. Polyuria was diagnosed in 15.5% of the cases. The highest diuresis in this group was 4.1 mmol/kg/hour, mean 3.7 ± 0.5 ml/kg/hour. Such changes were considered a manifestation of CDI. All 12 patients in Group C received desmopressin (DDAVP) for more than 48 hours (mean 56.8 ± 4. 5 hours). The doses were 0.025 to 0.2 mg/day. In four out of 14 children in Group B (29%), an increase hourly diuresis up to 3 ml/kg/hour was considered the onset of CDI; thus, they were also prescribed DDAVP. Unfavorable outcomes (GOS score 1 to 3) during a 30-day assessment were observed only in Groups B and C. In a comparison of unsuccessful outcomes between Groups B and C, there was an increase in the unfavorable outcome rate in patients of Group C (with hypernatraemia and polyuria) -10 children (84%) and Group Bfour children (28%). The risk factor in the comparison between patients of Groups B and C was 0.3, P <0.05. Conclusions Our results demonstrate that hypernatraemia increases the rate of unfavorable outcomes in children with TBI. Thirty-day outcomes were worse with CDI patients. Presumably, the used of DDAVP prevents dehydration and CDI advance. Introduction Secondary ischemic insult after severe traumatic brain injury (TBI) is correlated with poor outcome. Transcranial Doppler sonography (TCD) permits a non-invasive measurement of cerebral blood fl ow. The purpose of this study is to determine the usefulness of TCD in patients with severe TBI. Methods TCD was performed on 73 patients with severe TBI, defi ned as a Glasgow Coma Scale of 8 or less on admission. All patients were on mechanical ventilation. TCD was performed on hospital days 1, 2, 3 and 7. Hypoperfusion was defi ned by having two out of three of the following: mean velocity of the middle cerebral artery less than 35 cm/ second, diastolic velocity of the middle cerebral artery less than 20 cm/ second and a pulsatility index greater than 1.4. Vasospasm was defi ned by the following: mean velocity of the middle cerebral artery greater than 120 cm/second and/or Landegaard index greater than 3. Results Thirty-four patients (64%) had normal measurements. Thirteen were discharged home, 16 were discharged to a long-term care facility and fi ve died. Two of these patients were comatose and their families requested withdrawal of care. The other three died from brain death. Eighteen patients (25%) had hypoperfusion and all 18 progressed to brain death. Twenty-one patients (29%) had vasospasm. Four of these patients were discharged home, 11 to a long-term care facility and six died. The vasospasm was detected on hospital day 1 in three patients, hospital day 2 in seven patients, hospital day 3 in four patients and hospital day 7 in seven patients. Nimodipine was administered in six patients and all six were discharged to a long-term care facility. However, in one patient, nimodipine caused hemodynamic instability and was discontinued. In 15 patients, nimodipine was not given. Six of these patients expired from brain death. Twelve of 21 patients (57%) with subarachnoid hemorrhage on computed tomography had vasospasm. Conclusions Most patients with normal measurements can be expected to survive. Patients with hypoperfusion have a poor prognosis. In patients with vasospasm, the use of nimodipine should be considered; however, further studies are needed to determine safety and effi cacy. TCD may be useful in determining early prognosis. Further studies are also needed to determine whether TCD can improve outcome in patients with severe TBI.  The parameter of rSO 2 showed no signifi cant diff erence following HBO treatment in two groups (t = 0.352, P >0.05); however, there were inconceivable results in blood gas analysis. Partial pressure of oxygen (PaO 2 ) was signifi cantly decreased after HBO treatment (P <0.05), although these changes did not take eff ect on the clinical manifestation. Otherwise, the measurement of cytokines (TNF, IL-1) before and after HBO has no diff erence (P >0.05) in all groups. Conclusions HBO has no eff ect on brain oxygen saturation, after HBO treatment. But PaO 2 was signifi cantly decreased following HBO. The mechanism needs to be studied further. Introduction This study evaluated relationships between CSF levels of brain biomarkers, glial fi brillary acidic protein (GFAP), ubiquitin C-terminal hydrolase (UCH-L1) and α II -spectrin breakdown (SBDP145), partial pressure of brain tissue oxygen (ptiO 2 ) and brain temperature (Licox system) during the fi rst 24 hours and for up to 10 days following severe TBI. Methods We studied 27 severe TBI patients having CSF drainage and invasive monitoring of partial brain tissue oxygen tension (PbtO 2 ) and brain temperature using the Licox (Integra Neurosciences, Plainsboro, NJ, USA) probe. CSF SBDP145, UCH-L1 and GFAP levels were measured by quantitative ELISA assay on admission and every 6 hours thereafter for a maximum of 10 days. Using a double lumen bolt, ptiO 2 and temperature were measured with the Licox. This study focused on the recordings of the fi rst 24 hours following injury (27 patients), as well as preliminary data from four patients for 10 days. The total duration of monitoring was 1,512 hours. During the fi rst 24 hours, biomarker levels decreased while levels of PbrO 2 increased. All three biomarkers correlated with PbrO 2 (P <0.0001, P = 0.016 and P = 0.023, respectively). After the fi rst 24 hours, there were statistically signifi cant changes in levels of brain biomarkers (SBDP145, UCH-L1 and GFAP) as well as in levels of ptiO 2 (respectively, P = 0.025, P <0.0001, P = 0.033, P <0.0001). However, the correlation between biomarkers and brain tissue oxygenation was sustained, and for UCH-L1 improved (P <0.0001). No signifi cant correlations between biomarker levels and brain temperature were found. There were no complications from the monitoring. Conclusions Our fi ndings show that CFS levels of SBDP145, UCH-L1 and GFAP are related to brain tissue oxygenation in acute and possibly the subacute (≤10 days post injury) phases of severe TBI. Future studies will more directly address relationships between changes in tissue oxygenation and biochemical markers of injury following severe TBI. CSF levels of biomarkers and brain tissue oxygenation could yield insights into pathophysiological events following severe TBI and aid in clinical assessments of severe TBI patients. Introduction After traumatic brain injury (TBI), structural lesions are heterogeneous, but the spatial heterogeneity of consequences of insults as hypoxia-hypotension (HH) and/or TBI has never been studied. The objective of this study was to compare the eff ect of standardized insults (HH, TBI and both) on brain energy metabolism in two diff erent regions: frontal cortex and thalamus. Methods Twenty-eight Sprague-Dawley rats were randomized into four groups: Sham, TBI (impact acceleration alone, 450 g weight drop from 1.8 m), HH (blood depletion to mean arterial pressure 40 mmHg, FiO 2 10%, 15 minutes) and TBI-HH (TBI followed by HH, 45 minute delay). Cerebral perfusion pressure (CPP) was continuously and invasively measured. Brain microdialysis and PtiO 2 probes were both inserted stereotaxically in the right thalamus and frontal cortex. Results Except during the HH phase, CPP was always greater than 60 mmHg. During the hour following the HH period, a signifi cant increase in cerebral lactate/pyruvate ratio (Figure 1 ), glycerol and glutamate was observed. This increase was higher in the cortex than in the thalamus in all groups subjected to HH (P <0.001). In the TBI-HH groups, the increase in glycerol in the cortex was signifi cantly higher compared with the HH group (P <0.001), as well as thalamic and cortical glutamate. During 15 minutes following the HH phase (after reinjection and reoxygenation), an increase in PtiO 2 was observed in the cortex and thalamus, but with diff erent profi les (lower increase in the cortex) ( Figure 1 ). Conclusions Diff erent profi les of cerebral response to HH and TBI were observed with higher sensitivity in the cortex than in the thalamus. The post-ischemic hyperemia seems to be altered in the traumatized cortex but conserved into the thalamus and nontraumatized brain. Introduction Decompressive craniectomy is indicated for treatment of severe intracranial hypertension. However, this procedure is invasive and potentially associated with complications. We present a preliminary result of a study comparing early and late decompressive craniectomy in severe traumatic brain injury. Methods Patients studied were all admitted to the ICU of a tertiary referral center (Careggi Teaching Hospital, Florence, Italy) during 4 years (2005 to 2009). In total, data of 62 brain-injured patients, who underwent decompressive craniectomy, were retrospectively examined and included in two groups based on decompressive craniectomy execution: early decompressive craniectomy group (decompressive craniectomy performed within 24 hours after brain injury, group A; n = 41) and late decompressive craniectomy (later than 24 hours, group B; n = 21). For all patients, demographic, scores, clinical data, length of stay and fi nal outcome were collected from the institutional database. Traumatic lesions were compared at admission using the Marshall score and 24 hours after decompressive craniectomy execution with CT scan. The Glasgow Outcome Scale (GOS) at 6 months was also collected. No signifi cant diff erences have been found between groups regarding age and sex. The ONSD distribution is shown in Figure 1 . ONSD, eICP and ICP values were signifi cantly higher in Group 1 than in Group 2. Linear regression analysis identifi ed a signifi cant relationship Critical Care 2010, Volume 14 Suppl 1 http://ccforum.com/supplements/14/S1 between ONSD and ICP (r = 0.588). By calculating the receiver operating characteristic curve, an ONSD value of 5.35 mm resulted as the optimal cut-off point with a sensitivity of 95.1% and a specifi city of 96.2%. Conclusions ONSD measurements correlated with invasive and noninvasive (TCD) measurements of ICP. It is a useful, non-invasive bedside tool to diagnose IH. It is safe, easy to perform and can rapidly give reproducible information on patients' ICP. The medians of minute ventilation were 6.1 (interquartile range 2.6 to 8.1) and 3.9 (IQR 1.7 to 5.4) litres per minute in the SB and control groups, respectively (Figure 1 ). Hyperventilation >10 l/minute occurred only in the control group (P = 0.23). Conclusions Using the SB without previous hands-on training is possible for the majority of nursing and medical staff and decreases hyperventilation in comparison with the standard BVM. In the current study, therefore, we investigated the effi cacy of ECC in the dental chair in comparison with ECC on the fl oor. Methods Two dentists and two nurses with experience of ECC participated in this study; 30 ± 5 years old, 160 ± 5 cm in height, 65 ± 7 kg in weight. Before the study, they were educated about CPR and performed ECC for 5 minutes on the resuscitation manikin in two diff erent situations; on the fl oor and in the dental chair. On separate days, they repeated these ECC procedures on the fl oor and in the dental chair again. The depth of compression and the percentage of adequate compression were evaluated. In addition, each participant commented on the preferable situation in the questionnaire after each set of ECC. Results Four dental personnel performed ECC fi ve times on the fl oor and fi ve times in the dental chair and commented on the preferable setting fi ve times. The effi cacy of ECC was evaluated by the average depth and the percentage of ECC with adequate depth; 39.8 ± 8.2 mm and 46.8 ± 48.8% on the fl oor and 34.4 ± 6.9 mm and 41.7 ± 42.7% in the dental chair. The percentage of ECC with adequate depth was higher for the fl oor setting than that of the dental chair setting, although it did not reach statistical signifi cance (P = 0.079). In the 20 questionnaires, three of them preferred the dental chair setting, two of them were no diff erence between both settings and 15 of them preferred the fl oor setting. The presence of bystander CPR signifi cantly augmented the 1-month survival rate. However, there were no signifi cant diff erences among the four groups of CPR. The multivariate logistic regression analysis identifi ed three time factors including intervals of collapse-to-call, call-tofi rst CPR, call-to-arrival to patients as independent factors associated with 1-month survival. See Figures 1 and 2 . Conclusions Signifi cance of correctable time factors rather than type of CPR should be considered in the future guideline revision. Conclusions As an alternative airway device recommended by the ERC, the LT may enable airway control rapidly and eff ectively. Additionally, by using the LT, a reduced 'no-fl ow-time' and a better outcome may be possible. LT may be a good alternative airway device for providing and maintaining a patent airway during resuscitation. Introduction Besides the gold standard endotracheal tube, supraglottic airway devices are alternatives for emergency airway management [1] . The goal of the study was to identify airway devices that provide successful ventilation, even 12 months after training in manikins. Conclusions One year after training, time for successful ventilation for all devices was lower than 25 seconds. This is acceptable compared with the gold standard endotracheal tube, but due to the rising gastric infl ation rate and high amount of insuffi cient tidal volume, shorter intervals of training maybe necessary. Introduction The purpose of the study was to clarify the infl uence of aging on attitudes toward the initial three links in chains of survival. Methods We gave questionnaires to attendants of compulsory programs for basic life support (BLS) or driving technique at the beginning in authorized driving schools. The questionnaires included their backgrounds. We studied their willingness in four hypothetical scenarios related to the initial three links: early emergency call, cardiopulmonary resuscitation (CPR) under one's own initiative, telephone-assisted chest compression and use of AED. The respondents were divided into young (17 to 29 years, n = 6,122), middle-aged (29 to 59 years, n = 827), older person (>59 years, n = 15,743) groups. Results There were signifi cant diff erences in gender, occupation, residential area and experience of BLS training, and knowledge of AED use between the three groups. The proportions of respondents who are willing to perform the desirable BLS actions were lowest in the older person group (Table 1) . Multiple logistic regression analysis confi rmed that aging is one of the independent factors relating to negative attitude in all the scenarios. Gender, occupation, resident area, experience of BLS training, and knowledge for AED use were other independent factors relating to negative attitude to some of the scenarios. Conclusions The aged population is more negative to the chain of survival. More are willing to follow the telephone-assisted direction for chest compression. The BLS training should be modifi ed for them to gain confi dence and to be aware of the signifi cance and benefi t of early call.  The use of the pattern classifi cation machine which combines amplitude and spectral features of VF ECG signals shows an improved predictive power as compared with other methods. Conclusions This technique could help to determine which patients should receive shock fi rst and which should receive a period of CPR prior to shock, thereby increasing the probability of survival. The potential impact of this research is high in the direction of generating a new methodology able to increase the probability of survival after a cardiac crisis. References Introduction Recent studies report an increase of asystole and pulseless electrical activity (PEA) as the fi rst monitored cardiac arrest rhythms after arrival of the Emergency Medical Services [1] . The asystolic patients presumably undergo ischaemia for a longer time and may benefi t from treatment reducing hypoxic brain injury. Therapeutic hypothermia (TH) has expanded into prehospital care to be initiated as soon as possible. Rapid cold crystalloid infusion is the most frequent method; however, severe haemodynamic instability is its contraindication. The aim of the study was to assess the adverse eff ects of prehospital volume expansion in patients with initial nonshockable rhythms when used in a setting with only restricted cardiovascular monitoring. Methods All patients who were deemed eligible for advance cardiac life support (ACLS) were included as long as the arrest was witnessed and cardiopulmonary resuscitation (CPR) was initiated within 20 minutes of collapse. Patients were randomized to treatment or control groups. The trial was designed to determine the safety and eff ectiveness of early cooling initiated at the site of arrest. Survival and time to target temperature were documented. Results Data are presented as the mean ± SD or median (interquartile range (25, 75%)). Mean age was 67.8 ± 14 years in the intervention group and 65.4 ± 13.9 years in the control group. On average, cooling therapy was started in 33 ± 12 minutes in the RhinoChill™ group and 170 ± 97 minutes in the control group. Temperatures at hospital admission were signifi cant lower in the RhinoChill™ group. Time to target tympanic temperature, refl ecting brain temperature, were signifi cant faster in the RhinoChill™ group (211 ± 124 minutes vs 424 ± 217 minutes; P <0.05). Adverse events occurred in 12 patients. None was related to the cooling therapy. In the intervention group fi ve patients (20%) survived and three patients (12%) had a CPC of 1 to 2. In the control group only four patients (12.5%) survived and one patient (3.1%) had a CPC of 1 to 2. Conclusions Using the intranasal cooling method, cooling was much faster and earlier in treated patients. Neurologically intact survival and discharge rates were higher in treated patients. Transnasal cooling for the induction of therapeutic hypothermia during prehospital resuscitation is feasible and highly eff ective in lowering brain temperature rapidly. The method off ers the possibility for immediate introduction and realization of mild hypothermia in the fi eld. [1] . We sought to investigate whether a hospitalwide approach to TH after CA would reduce delays and result in improved outcomes. We identifi ed 82 patients treated with TH post cardiac arrest. Nineteen (29%) had proven infection either on PC or on peripheral blood cultures. PC specimens were taken in 21 (25%) patients. Of the 21 patients who had PC specimens taken, 16 (76%) had proven infection. We also found that an increase in ICU length of stay was associated with increased infection rates, 44% in patients with a length of stay of greater than 3 days and 55% in patients with a length of stay of greater than 4 days. The fi nal study population of 865 patients had the following eight diff erent coma etiologies: poisoning (n = 329), stroke (n = 213), epilepsy (n = 113), circulatory failure (n = 60), infection (n = 56), metabolic disorder (n = 44), respiratory insuffi ciency (n = 33), and intracranial malignancy (n = 17). The hospital mortality rate among the 865 patients was 26.5%, varying from 0.9% for epilepsy to 71.7% for circulatory failure. The accumulated total 2-year mortality rate was 43.0%, varying from 13.7% for poisoning to 88.2% for malignancy. The level of consciousness on admission also infl uenced the prognosis: a GCS score of 3 to 6. Conclusions The prognosis in patients presenting with nontraumatic coma is serious and depends largely both on the level of consciousness on admission and on the etiology of the coma. Adding the suspected coma etiology to the routine coma grading of these emergencies may more accurately predict their prognosis. Introduction Recently, near-infrared time-resolved spectroscopy (TRS), which is quite eff ective in quantitative monitoring tissue oxygenation, because it off ers the actual measurement of photon migration in the tissues, and the photon mean path length is easily obtained from the center of gravity of the temporal profi le, has been developed. In this study, we investigated whether the changes in the cerebral oxygen saturation (T-SO 2 ) obtained with the TRS and the jugular venous oxygen saturation (SjvO 2 ) predicted cognitive decline after cardiac surgery. Methods With institutional approval and informed consent, we studied 10 patients (68.7 ± 6.1 years) undergoing cardiac surgery under cardiopulmonary bypass (CPB). T-SO 2 was continuously monitored using a TRS-10 (Hamamatsu Photonics KK, Hamamatsu, Japan). For measurement of SjvO 2 , a 5.5 Fr oximetry catheter was inserted by retrograde cannulation of the right internal jugular vein. The values of T-SO 2 and SjvO 2 were compared with each point: before CPB, 5 minutes after the onset of CPB, before aorta clamp, after aorta clamp, rewarming, aorta declamp, end of rewarming, and end of CPB. The cognitive decline was evaluated by Mini Mental State Examination before and 7 days after the operation. The statistical analysis was performed by repeated-measures ANOVA followed by Fisher's PLSD. P <0.05 was considered statistically signifi cant. Results Four of 10 cases showed postoperative cognitive decline. The mean values of T-SO 2 and SjvO 2 during operation in patients without postoperative cognitive decline (n = 6) were 63.9 ± 4.6% and 60.5 ± 9.7%, respectively, and there were no statistical signifi cances between these values. However, the mean values of T-SO 2 and SjvO 2 during operation in patients with postoperative cognitive decline (n = 4) were 62.8 ± 5.6% (T-SO 2 ) and 70.4 ± 14.9% (SjvO 2 ), and showed signifi cant diff erences between two values (P = 0.0024), and at the rewarming period, the values of SjvO2 was signifi cantly higher than those of T-SO 2 (87.9 ± 6.3% vs 65.0 ± 5.3%, P = 0.0014). Systemic hypertension and smoking showed strong association with the rupture of intracranial aneurysms. The arteries of the previous segment were those that had higher incidence of aneurysms. More than onehalf of the patients did not have complications during the procedure. Embolization of cerebral aneurysms was revealed to be a low lethality method.  We excluded two patients with bleeding for more than 72 hours. There was no signifi cant change in the levels of CK total, renal or liver function. We included 20 patients, 11 in the SVT group and nine in the control group. Mortality was eight patients (38%), six patients in the control group and two from the SVT group. Vasospasm was confi rmed by Critical Care 2010, Volume 14 Suppl 1 http://ccforum.com/supplements/14/S1 S116 cerebral arteriography examination in four patients in the control group and one patient in the SVT group. All patients who died had Fisher scale IV. Conclusions SVT at a dose of 80 mg was eff ective in reducing the mortality (18.1% against 66%) compared with the group that did not use SVT, and also decreased the incidence of cerebral vasospasm despite higher APACHE II score in the group that used SVT (14. 3 Conclusions We can conclude that the group with patients with SAH is predominantly female (74:29). The APACHE II in Group I was 10.9, while Group II was 17.9. Regarding the criteria used to assess patients with SAH was observed that the only criterion which showed statistical signifi cance in the prediction of death was the serum sodium (P = 0.002). The other criteria evaluated did not have statistical signifi cance in predicting the prognosis of patients. Reference Introduction The objective of this study was to investigate the relationship between cardiac output response to a fl uid challenge and changes in brain tissue oxygen pressure (PbtO 2 ) in patients with severe brain injury. Methods Prospective observational study conducted in a neurological ICU in a university hospital. Seventy-eight fl uid challenges were administered to 17 consecutive comatose patients that underwent multimodality monitoring of cardiac output, intracranial pressure (ICP), and PbtO 2 . The relationship between cardiac output and PbtO 2 was analyzed with logistic regression utilizing GEE with an exchangeable correlation structure. Of the 78 fl uid boluses analyzed, 34 (44%) resulted in a ≥10% increase in cardiac output. Median absolute (+5.4 vs +0.7 mmHg) and percentage (20% vs 3%) changes in PbtO 2 were greater in cardiac output responders than in nonresponders within 30 minutes after the end of the fl uid bolus infusion. In a multivariable model, a cardiac output response was independently associated with PbtO 2 response (adjusted odds ratio 15.4, 95% CI 1.9 to 122.0, P = 0.01) after adjusting for mean arterial pressure, Critical Care 2010, Volume 14 Suppl 1 http://ccforum.com/supplements/14/S1 S117 intracranial pressure and end-tidal CO 2 . Stroke volume variation showed a good ability to predict cardiac output response with an area under the ROC curve of 0.85 with a best cutoff value of 8%. See Figures 1 and 2 . Conclusions Bolus fl uid resuscitation resulting in augmentation of cardiac output can improve cerebral oxygenation after severe brain injury. Introduction ICM + software encapsulates our 20 years' experience in brain monitoring. It collects data from a variety of bedside monitors and produces time trends of parameters defi ned using confi gurable mathematical formulae. To date it is being used in nearly 40 clinical research centres worldwide. We present its application for continuous monitoring of cerebral autoregulation using near-infrared spectroscopy (NIRS). Methods Data from multiple bedside monitors are processed by ICM + in real time using a large selection of signal processing methods. These include various time and frequency domain analysis functions as well as fully customisable digital fi lters. The fi nal results are displayed in a variety of ways including simple time trends, as well as time window based histograms, cross histograms, correlations, and so forth. All this allows complex information from bedside monitors to be summarized in a concise fashion and presented to medical and nursing staff in a simple way that alerts them to the development of various pathological processes. Conclusions ICM + software is proving to be a very useful tool for enhancing the battery of available means for monitoring cerebral vasoreactivity and potentially facilitating autoregulation guided therapy. Complexity of data analysis is also hidden inside loadable profi les, thus allowing investigators to take full advantage of validated protocols including advanced processing formulas. dysbalances and their relationship to outcome in the neurologicneurosurgical care unit (NNICU) over a period of 5 years. Methods We prospectively evaluated patients with brain diseases, who developed serum sodium below 135 mmol/l (hyponatremia) or above 150 mmol/l (hypernatremia). We compared the incidence of cerebral complications, Glasgow Outcome Scale upon discharge from the NNICU and mortality in the NNICU between these two groups. In the 5-year observation period, serum sodium dysbalances occurred in 378 (24%) patients. The majority of them had hyponatremia (245 patients, 65%); hypernatremia was less frequent, in 133 (35%) patients. Hypernatremic patients stayed in the NNICU longer (P = 0.035), onset of hypernatremia arose in patients with signifi cantly lower Glasgow Coma Scale (P = 0.001). These patients had more cerebral complications (P <0.001), worse Glasgow Outcome Scale upon discharge from the NNICU (P <0.001), higher mortality in the NNICU (P = 0.003) and higher incidence of pulmonary edema (P = 0.021). They received more antiedematic therapy (P <0.001) and diuretics (P <0.001). On the other hand, hyponatremia was more frequent upon entry to the NNICU (P <0.001) and arose later after brain damage (P <0.001) in comparison with hypernatremia. Conclusions In neurointensive care, hypernatremia was a prognostically more serious and less frequent sodium dysbalance than hyponatremia. 19%) , cerebral contusion in 17 patients (17%), brain oedema in 15 patients (15%), and subdural hematoma in two patients (2%). Glasgow Coma Scores (range 3 to 10); 60 patients were mechanically ventilated; 10% were diabetic and 22% were hypertensive. Hypernatremia was documented in 40 patients (40%) of the total TBI patients. The total inhospital mortality was 36/100 (36%), 10 of them had normal sodium levels all through their in-hospital course and 26 patients were hypernatremic. After adjustment for the baseline risk, the incidence of hypernatremia over the course of the ICU stay was signifi cantly related to increased mortality (hazard ratio 3.2 (P = 0.0001)). However, there was positive correlation between serum sodium levels and duration of the ICU stay (Spearman correlation coeffi cient 0.5 and P = 0.002). We calculated the elastic shear modulus of standard MA (Gt) and HVG MA (Gh), which refl ect the total clot strength and procoagulatory protein component, respectively. The diff erence was an estimate of the platelet component (Gp). There was a 16% perioperative increase of standard MA, corresponding to a 51% increase of Gt (P <0.05) and a 79% to 87% contribution of the calculated Gp to Gt. We conclude that serial standard thromboelastography and the HVG viscoelastic test may reveal the independent contribution of platelets and procoagulatory proteins to clot strength. Using multiple linear regression, all coagulation, TEG and HVG variabilities were used to model postoperative hypercoagulation. Results showed that some components of the TEG failed to identify hypercoagulation (r <0.2, P >0.75). All components of the HVG test refl ect postoperative coagulopathies. Conclusions Hypercoagulability is not refl ected completely by standard coagulation monitoring and TEG, and seems to be predominantly caused by increased platelet reactivity. HVG provides a fast and easy-toperform bedside test to quantify in vitro coagulation, and may be useful in determining the coagulation status of cancer patients perioperatively. Conclusions HIT incidence in the ICU is extremely rare, with no cases shown in this study. HIT was suspected in 23 of 327 patients (7.03%), but the HIT incidence was 0.0%. ICU patients show numerous potential etiologies of thrombocytopenia and in this population sepsis was the most common. References Introduction Many procedures in cardiac surgery require extracorporeal circulation (EC). During EC, blood is pumped from a venous line to an oxygenator, cooled and infused back via an arterial line. This procedure is known to compromise the coagulation system, especially under prolonged hypothermia. The aim of the present study was to investigate the eff ects of EC and hypothermia on the coagulation system in a porcine model. The effi cacy of a substitution therapy with a prothrombin complex concentrate (PCC) regarding normalization of coagulation and decrease in hemorrhage was evaluated. Methods A total of 17 male anesthetized pigs were included in the study. EC was performed in 12 animals by a hollowfi ber oxygenator with a priming solution containing saline, HES and heparin. Five animals without EC served as a control. The coagulation system was characterized by thromboelastography (TEG), thrombin generation, coagulation factor levels, platelet numbers and function (aggregation). A GoreTex patch was inserted into the carotid artery, bleeding occurred from the stitch channels. The eff ects of intravenous substitution therapy by PCC (Beriplex P/N, 30 U/kg, n = 6) compared with placebo (n = 6) on bleeding from the stitch channels was investigated. Results EC and hypothermia compromised the coagulation system. Coagulation factor levels were decreased, TEG and thrombin generation became pathologic. Platelet count and aggregation were decreased. Blood loss from the stitch channels of the GoreTex patch was doubled in the EC group when compared with animals without EC. After substitution therapy with PCC, blood loss decreased signifi cantly from 83.0 ± 48.4 ml in the EC + placebo to 27.2 ± 35 ml in the EC + PCC group (P <0.025, Wilcoxon test; Figure 1 ). PCC normalized the impaired coagulation. Conclusions Hypothermia and EC led to a compromised coagulation system, resulting in increased hemorrhage. The defi cit in coagulation could be overcome by substitution therapy with PCC. Elevated blood loss from stitch channels was decreased signifi cantly after treatment with PCC. It was concluded that PCC should be benefi cial in patients undergoing cardiovascular surgery with an EC-induced coagulopathy. Introduction Dilutional coagulopathy following massive bleeding is the result of clotting factor dilution and impaired fi brin polymerization after infusion of colloidal plasma expanders. Although fi brinogen has been clinically used to treat dilutional coagulopathy [1, 2] , the eff ects of fi brinogen dosages on normalizing coagulation function is unclear. This study investigated the eff ect of six diff erent fi brinogen dosages (range: 37.5 to 600 mg/kg) on ROTEM® parameters and overall blood loss in a pig model for dilutional coagulopathy. Methods Forty-two pigs underwent a 60% hemodilution with Voluven® (HES130/0.4). A standardized bone injury was performed after the completion of hemodilution. Animals were then randomized to receive 37.5, 75, 150, 300, 450 or 600 mg/kg fi brinogen (FGTW, LFB, France) or 500 ml saline. Four hours after fi brinogen administration a standardized liver injury was performed. Animals were then observed for 2 hours or until death. Blood loss was measured and tissue samples were collected at the end of the study. Hemodynamic and coagulation parameters were measured at baseline (BL), after hemodilution, 15 minutes, 1, 2 and 4 hours after drug infusion and 2 hours after liver injury or right before the animal's death. Statistical signifi cance was set at P <0.05. Results Fibrinogen dosages of 150 mg/kg and higher completely reversed dilutional coagulopathy: the maximum clot fi rmness (MCF) which was decreased after hemodilution (36 ± 3 vs 65 ± 4 mm at BL, P <0.05), returned to BL levels after fi brinogen administration (69 ± 5 mm). Blood loss from bone and liver injury signifi cantly decreased with increased fi brinogen dosages: 42 ± 19 (sham), 34 ± 14 (75 mg/kg), 29 ± 13 (150 mg/kg) and 28 ± 10 ml/kg BW (600 mg/kg). No thrombotic events occurred. Conclusions In a swine model of 60% hemodilution with bone and liver injury, fi brinogen administration (150 mg/kg and above) normalized MCF and decreased blood loss. Infusions of 12 times the dosage recommended in humans did not induce hypercoagulability. We calculated the elastic shear modulus of standard MA (Gt) and HVG MA (GH), which refl ect the total clot strength and procoagulatory protein component, respectively. The diff erence was an estimate of the platelet component (Gp). There was a 14% perioperative increase of standard MA, corresponding to a 48% increase of Gt (P <0.05) and an 80 to 86% contribution of the calculated Gp to Gt. We conclude that serial standard thromboelastography and the HVG viscoelastic test may reveal the independent contribution of platelets and procoagulatory proteins to clot strength. Using multiple linear regressions, all coagulation, TEG and HVG variables were used to model postoperative hypercoagulation. However, three components of the routine coagulation assay, including bleeding time, prothrombin time and platelet count, could be modeled to show prolonged postoperative hypercoagulability (P <0.01). We conclude that all components of the HVG test refl ect postoperative coagulopathies; these results suggest that it may be useful in determining the coagulation status of cancer patients perioperatively. Introduction A number of studies have observed decreased survival associated with transfusion. Leukocyte-mediated immunosuppression may contribute to postoperative infectious complications. There is evidence for a benefi t with the use of leukocyte reduced transfusion, but it is unclear among septic shock patients. In Japan, leukocyte-depleted blood products have been used since 2007. We assessed the eff ect of transfusion and the effi cacy of the use of leukocyte-depleted blood products for new onset of septic shock and mortality among patients with septic shock. Methods A total of 101 septic shock patients at a single university general ICU were enrolled in the study. A target hemoglobin (Hb) concentration of 10 g/dl (Ht 30%) was considered for allogeneic red cell transfusion (to maintain central venous saturation >70%) during the early phase of severe sepsis, and Hb was kept in the range of 7 to 9 g/dl in the stable phase except for the patients with particular disease (acute coronary syndrome, and so forth). Fresh frozen plasma was used to keep PT-INR <1.67, and platelet transfusion was done due to the patient's condition (for example, postoperative, and so forth). Results Eighty-six patients (85%) received transfusion (group T: age 68 ± 12, APACHE II 25.9 ± 9.5) and 15 received no blood product (group NT: age 63 ± 14, APACHE II 27.4 ± 3.2). Frequent sites of infection were the lung (46%), peritoneum (17%), mediastinum (12%), and gastrointestinal tract (7%) (NS between groups). Overall mortality for group T vs group NT was 27/86 (31%) vs 3/15 (20%) (P = 0.54). Onset of new septic shock for group T vs group NT was 27/86 (31%) vs 4/15 (27%) (P = 0.71). Survivors received 26.6 ± 31.2 Japanese units of total blood product and 44.6 ± 35.6 units for patients who died, respectively (1 American unit is almost equivalent to 2 Japanese units) (P = 0.013). For group T, 56 patients received leukocytereduced blood product (LRB) and 29 received ordinary product (Cont.). Overall mortality for LRB vs Cont. was 40/56 (71%) vs 18/29 (62%) (P = 0.38). Conclusions The total amount of blood products received was highly associated with increased mortality, but there was no obvious adverse eff ect of transfusion to the onset of new septic shock. No association between overall mortality and the use of leukocyte depleted blood was identifi ed in patients with septic shock.  In the period of study, were used 208 catheters in femoral access and 52 catheters in central internal jugular access with tracheostomy. There were no signifi cant diff erences between patients with central jugular with tracheostomy and femoral access in age, sex, APACHE II score, diagnosis group, use of mechanical ventilation, use of antimicrobials, use of total parenteral nutrition, use of pulmonary artery catheter, and duration of the catheter. We diagnosed 16 CRB in 208 femoral catheters during 1,679 days of catheterization and 10 CRB in 52 central internal jugular catheters with tracheostomy during 462 days of catheterization. The incidence of CRB was higher in the central internal jugular with tracheostomy than in the femoral site (21.64 vs 9.52 per 1,000 catheter-days; risk ratio = 2.27; 95% confi dence interval = 1.04 to 4.97; P = 0.04). Conclusions The femoral site could be considered a more safe venous access than the central internal jugular in patients with tracheostomy to minimize the risk of CRB. The patients submitted to vacuum-assisted closure therapy were similar to patients in whom the abdominal wall was closed, regarding the risk factors for peritoneal infection (corticotherapy, oncologic disease, renal insuffi ciency, hepatic insuffi ciency, desnutrition, hypoalbuminemia, a high APACHE II score). The patients submitted to vacuum-assisted closure therapy had a higher ICU stay and time of mechanical ventilation but lower hospital mortality. Conclusions Vacuum-assisted closure therapy is superior to primary abdominal wall closure in patients with tertiary peritonitis.  The mean heart rate was 90 ± 17 bpm, mean LV volumes, mean LVEF and mean Dct were within normal limits (LVEDVI; 44.6 ± 15.4 ml/m 2 , LVESVI; 18.7 ± 11.5 ml/m 2 , LVEF; 58.9 ± 15.7%, Dct; 175.8 ± 50.2 ms). The mean peak systolic strain measurements were -16.0 ± 5.0% (basal; -18.1 ± 7.0%, mid; -15.7 ± 5.6%, apical; -14.1 ± 6.6%). The mean SOFA score was 10.8 ± 3.9. Linear regression analysis showed correlation between the SOFA score and LVEF (r = -0.40, P = 0.02), Dct (r = -0.34, P = 0.007), mean septal strain (r = 0.40, P = 0.002), mid-septal strain (r = 0.43, P = 0.0007) and apical septal strain (r = 0.36, P = 0.006). Conclusions These results suggest that the higher SOFA score had lower LVEF, lower LV systolic strain and higher LV fi lling pressure in the patients with severe sepsis or septic shock. An assessment of the left ventricular longitudinal contraction by tissue Doppler strain imaging may serve as a useful tool to evaluate multiple organ failure in this patient population. Introduction Myocardial dysfunction is reportedly a common complication of sepsis that requires specifi c management. Although frequently reported in the literature, the clinical spectrum and frequency of this organ failure have not been fully appreciated. We sought to determine the frequency of myocardial dysfunction in severe sepsis and septic shock and to describe the clinical spectrum of this entity with transthoracic echocardiography. Methods Prospective single-center study capturing all patients admitted to the ICU with severe sepsis or septic shock from May 2007 to January 2009. All patients enrolled underwent comprehensive transthoracic echocardiography on admission. The exclusion criteria included <18 years of age, pregnancy, documented ischemic, valvular or congenital heart disease. All patients with LV systolic dysfunction defi ned as LVEF <50% received a repeat echocardiographic study at 5 days or upon dismissal from ICU. Results One hundred and six patients were enrolled, mean age was 65, and 50% were female. The mean SOFA score was 11. Central venous oxygen saturation was less than 70% in 37% of patients. Twenty-nine patients (27%) had global LV systolic dysfunction (14 = mild, nine = moderate, six = severe), 33 patients (31%) had RV dysfunction (18 = mild, nine = moderate, six = severe), 14 patients had biventricular involvement (13%) and 39 patients (37%) had diastolic fi lling abnormalities. Of the 29 patients with LV systolic dysfunction on initial examination, 28 received a follow-up echocardiogram and 96% of these patients (n = 27) improved in all parameters. Thirty-day mortality was 39%, 6-month mortality 52%. Myocardial dysfunction did not predict mortality. Conclusions These results confi rm that myocardial dysfunction is frequent and broad in patients with severe sepsis and septic shock. Right ventricular involvement and diastolic abnormalities should be considered as part of the clinical spectrum of this entity. There was poor correlation with myocardial dysfunction and mortality and it was reversible regardless of presentation. We should not focus only on left ventricular ejection fraction to diagnose myocardial dysfunction in sepsis, since only a small portion of these patients had isolated LV systolic dysfunction. In 65 of 208 patients an echocardiographic examination was performed. Age (61.0 ± 18.4 vs 63.5 ± 16.9 years, P = NS), male sex (64.5% vs 58.8%, P = NS) and mean artery pressure (86.2 ± 15.7 vs 85.7 ± 21.2 mmHg, P = NS) were comparable, whereas patients with HAS had a higher heart rate (96.5 ± 24.5 vs 108.2 ± 17.1/minute, P <0.05). At admission, patients with HAS had a signifi cantly lower EF than patients with LAS (55.1 ± 7.0 vs 50.5 ± 10.1, P <0.05). Diff erence could not be observed after 24 hours (53.6 ± 7.2 vs 52.4 ± 7.%, P = NS) and 72 hours (53.8 ± 7.3 vs 54.3 ± 7.3, P = NS); see Figure 1 . Patients with HAS showed a lower initial E' than patients with LAS (6.2 ± 2.1 vs 5.1 ± 2.1, P <0.05) and after 24 hours (6.1 ± 2.2 vs 5.1 ± 1.3, P <0.05). Three days after admission, E' values in patients with LAS and HAS were comparable (5.6 ± 2.0 vs 5.7 ± 2.1, P = NS). Conclusions In patients with community-acquired sepsis and APACHE II score ≥15 points, a signifi cantly depression of both systolic and diastolic function could be observed. After 24 hours systolic function and after 72 hours diastolic function of patients with an APACHE II score <15 and ≥15 points were similar. This could be possibly due to alterations in afterload. Following a 10 mmHg drop in mean arterial pressure (MAP) from baseline, sheep were randomly assigned to receive intravenous infusions of either POV, AVP or the vehicle (0.9% NaCl; n = 6 each). POV and AVP were titrated to keep MAP above baseline -10 mmHg. All sheep were awake, mechanically ventilated and fl uid resuscitated to maintain hematocrit at baseline ±3%. Data are expressed as mean ± SEM at 24 hours. Fluids are represented as the average over time to account for individual survival times. The 498 severe sepsis and septic shock patients were enrolled and examined at what time period would the RB still be eff ective. Using a time cut-off , the Compliers at 18 hours and NonCompliers at 18 hours were then compared. There were 202 patients who had the RB completed in less than or equal to 18 hours (Compliers at 18 hours). There were 296 patients who never completed the RB within 18 hours (NonCompliers at 18 hours). The Compliers at 18 hours had a signifi cant 10.2% lower hospital mortality at 37.1% (22% relative reduction) compared with the NonCompliers at 18 hours hospital mortality of 47.3% (P <0.03). Adjusting for diff erences in baseline illness severity, the Compliers at 18 hours had a greater reduction in predicted mortality of 26.8% vs 9.4%, P <0.01. Compliance started at 30.4% and fi nished at 63%. Conclusions A CQI initiative for severe sepsis and septic shock with particular emphasis on the RB signifi cantly improved bundle compliance and decreased hospital mortality. It has been previously shown that compliance to the RB within 6 hours improves outcome; however, when the time of bundle completion is extended to 18 hours, the mortality benefi ts are still signifi cant. Introduction To investigate the eff ect of myocardial preservation of early goal-directed therapy (EGDT) on severe sepsis/septic shock patients in the ICU. Methods This is a prospective and randomized controlled study, in which the total 158 severe sepsis/septic shock patients from the ICU were randomly assigned into two groups (EGDT group, n = 81 and control group, n = 77). Then the concentration of serum cardiac troponin I (cTnI), high sensitivity C-reactive protein (hs-CRP) and APACHE II score of patients were obtained on the 0, 3rd, 7th, and 14th day after fl uid resuscitation therapy. The levels of cTnI were the same between the EGDT and control groups on day 0 (exceeded normal cTnI patients in two groups 0.41 vs 0.42, normal cTnI patients in two groups 0.05 vs 0.04, P >0.05), there was a dramatic decrease in exceeded normal cTnI patients of the EGDT group, in which cTnI returned to normal after EGDT (14th day: exceeded normal cTnI patients in two groups 0.08 vs 0.16, P <0.05); however, only a little diff erence in normal cTnI patients of the two groups (P >0.05). The level of hs-CRP changed like cTnI (P <0.05), and there was positive correlation between cTnI and hs-CRP on each time (P <0.05); APACHE II scores obviously decrease in the EGDT group (P <0.05). Meanwhile, the EGDT group have an obviously higher 28-day survival rate and longer survival time than that of the control group (74.1% vs 55.8%; 23.5 vs 19.6, P <0.05). Conclusions EGDT has an eff ect of myocardial preservation and improves the survival rate for severe sepsis/septic shock patients in the ICU. Results In all cases we saw reliable reduction of the LPS level in serum (1.8 to 2 times). After the investigation of some cytokine levels we obtained similar results: IL-6, TNFβ and IL-8 levels had been reduced 30 to 40%, 87% and 62 to 76% accordingly. At the same time, IL-2, INFγ, IL-12, TNFα, IL-4 and IL-5 levels in serum had not changed practically. However, in approximately 1/3 cases an IL-1β and IL-10 concentration increase in serum was observed (33 and 40% accordingly). Also our research has shown that, in the most of the cases after application of this sorbent, LBP decreased insignifi cantly (10 to 32%), but the sIL-1 RII level slightly increased (10 to 18%) in the majority of patients and considerably decreased (two times) in 1/3 investigated patients. The CD14 concentration in patient serum reliably did not change before and after adsorption. The somatic status of patients was stabilized after LPS adsorption. Introduction It was assumed that pathologic activation of neutrophils and monocytes is associated with sepsis, acute lung injury/ARDS, and multiple organ failure, and that removal of these cells from the circulation could reduce leukocyte-dependent tissue injury. Cartridges containing polymyxin B (PMX) immobilized to fi bers (Toraymyxin; Toray Industries, Tokyo, Japan) have been developed for selective adsorption of circulating endotoxin in patients with Gram-negative bacterial infection, and this treatment has proven to be highly eff ective. This study examined the eff ect of direct hemoperfusion through fi lters with immobilized PMX direct-hemoperfusion (DHP) on leukocyte function and plasma levels of cytokines in patients with septic shock. Methods We evaluated the eff ect of PMX-DHP on circulating leukocytes in patients with septic shock by assessing the changes of neutrophil and monocyte surface antigen expression after PMX-DHP. In another experiment, heparinized blood from patients with sepsis was passed through PMX fi lters in a laboratory circuit and then changes of the cell count and surface antigen expression by neutrophils and monocytes were assessed. After perfusion, neutrophils were isolated and the capacity of these cells to damage cultured endothelial monolayers was also determined. We found that PMX-DHP led to an increased CXCR1 and CXCR2 expression along with a decrease of CD64 and CD11b expression by circulating neutrophils from septic patients. Plasma levels of cytokines, including IL-6, IL-8, IL-10, and high-mobility group box-1, were elevated in patients with septic shock compared with healthy controls, but cytokine levels were not altered by PMX-DHP. Ex vivo perfusion of heparinized blood from patients with sepsis through PMX fi lters in a laboratory circuit caused a signifi cant decrease of the neutrophil and monocyte counts. Activated neutrophils with high CD11b/CD64 expression and low CXCR1/CXCR2 expression showed preferential adhesion to PMX fi lters. Neutrophils isolated from the blood after ex vivo PMX perfusion caused less damage on the endothelial cell monolayer than cells from sham-treated blood. Introduction Direct and rapid removal of pathogens or noxious metabolites from a patient is the most straightforward cure imaginable. Dialysis and plasma fi ltration/exchange are the current broadly applicable methods to perform a direct removal of disease-causing factors from a patient. We describe the use of stable nanomagnets to rapidly and selectively remove heavy metal ions, overdosed steroid drugs and proteins from human blood. This nanomagnet-based purifi cation method avoids fouling of fi lter membranes and benefi ts from a high external surface area, and a correspondingly fast diff usion. Methods Nanomagnets equipped with heavy metal complexants, digoxin antibody fragments and entire human IL-6 antibodies were added to a series of blood samples. The nanomagnets scan blood by Brownian motion and capture their target. Afterwards, a small magnet was placed at the sample tube wall accumulating the nanomagnets in the pole region of the external magnet. The purifi ed supernatant can then easily be decanted. The concentrations of lead, digoxin and IL-6 in blood samples were determined by standard clinical methods. Blood integrity was observed by rotation thromboelastography and monitoring of serum potassium, lactate dehydrogenase, bilirubin and haptoglobin levels. To measure the biological relevance of the IL-6 removal, the eff ect on caspase-3 activation was assessed in camptothecin-stimulated neutrophils. Results A signifi cant decrease of lead, digoxin and IL-6 levels was measured after the blood purifi cation procedure. The extraction using nanomagnets was in clear dose-eff ect dependence and could be accurately titrated. Treatment with nanomagnets did not signifi cantly aff ect the integrity of blood and all levels remained in the clinical norm range. Caspase-3 assays showed a reduced anti-apoptotic eff ect after IL-6 removal, underlining the biological relevance of the achieved removal effi ciency. Conclusions We demonstrate the extraction of lead, digoxin and IL-6 from whole blood as an example for the rapid treatment of heavy metal poisoning, drug overdosing and severe infl ammation. The presented direct blood extraction could be combined with existing therapeutic strategies and may have major implications on the treatment of severe intoxications, sepsis (specifi c fi ltering of cytokines or toxins) [1] , metabolic disorders (thyreotoxicosis) and autoimmune diseases. References Methods Fifty-fi ve septic patients were enrolled in this study. Every patient had four CPFA treatments (LINDA; Bellco-Mirandola, Italy) for 8 hours with Q b = 200 ml/minute, Q ultrafi ltration = 30 ml/kg/hour and Q plasma = 20% of Q b . At T0 (basal), T1 (after fi rst cycle), T2 (after second cycle), T3 (after third cycle) and T4 (after fourth cycle) we evaluated haemodynamic parameters, norepinephrine dosage, PaO 2 /FiO 2 ratio, plasma IL-6, and procalcitonin (PCT). The ANOVA test was used to compare changes during times study. P <0.05 was considered statistically signifi cant. Results Patients enrolled in the study have been submitted to 256 CPFA treatments for 2,650 hours. Table 1 presents the main results of the study. IV quartile of IL-6 is shown in Table 1 . was not diff erent between groups. Group 1 patients had a trend towards higher number of organ dysfunction (P = 0.07) and more septic shock episodes (P = 0.09). Regarding treatment adequacy, compliance with any indicator or the entire SSC-6h bundle was not related to survival. However, achievement of the CPV target was associated with a higher early mortality (Group 1: 69.6%, Group 2: 40.6%, P = 0.013; OR -3.3.4 (1.33 to 8.40)). Conclusions In this observational study, factors related to disease severity rather than to the evidence-based treatment compliance were associated with short-term death among severe septic patients.  Results Out of all 717 ICU admissions, 87 (12%) had AKI and 36% of them received dialysis. Kidney injury developed more frequently in patients with hematological malignancies than in patients with solid tumors (26% vs 11%, P = 0.003). Ischemia/shock (76%) and sepsis (67%) were the main contributing factors, and kidney injury was multifactorial in 79% of the patients. The ICU mortality was 61% (53/87) and hospital mortality was 71% (62/87). Despite the lack of statistical signifi cance, hospital mortality was higher in patients who received RRT later on during the ICU stay (92%) in comparison with those who received RRT on the fi rst day in the ICU (78%) and those who were not dialyzed (64%) (P = 0.105). End-of-life decisions (to withhold or to withstand therapies) were taken in 18 (23%) patients. General and renal-specifi c severity-of-illness scores were inaccurate in predicting outcomes for these patients. In a multivariate analysis, length of hospital stay prior to ICU, acute organ dysfunctions, need for mechanical ventilation and a poor performance status were associated with increased mortality. Moreover, cancer-related characteristics were not associated with outcomes. Conclusions The present multicenter study confi rmed that AKI in critically ill patients with cancer is frequent, usually multifactorial and still associated with high mortality rates. On the other hand, the current study also suggests that ICU admission and RRT should be considered in selected patients. Mortality in these patients is mostly dependent on the severity of acute illness and the performance status, rather than cancerrelated characteristics. Conclusions These results suggest signifi cant increased ICU mortality in the following groups: over 50 years old, smokers, patients with poor exercise tolerance, patients with IHD or PV, patients with COPD and patients who drink alcohol to excess or those with ALD. While these results should not be used as a basis upon which to permit or refuse intensive care admission, they should be used to inform staff and patients of likely outcome from intensive care. Introduction C-reactive protein (CRP) is an acute-phase protein, the blood levels of which increase rapidly in response to infection, trauma, ischemia, burns, and other infl ammatory conditions. Serum albumin decreases in critically ill patients with similar conditions. The use of these blood tests as risk markers or as predictors of organ failure and death has been studied previously [1, 2] and we wished to investigate their applicability to our population. Admission CRP correlated positively with length of stay (r = 0.14, P = 0.017) and APACHE II score (r = 0.13, P = 0.03) but did not signifi cantly correlate with duration of mechanical ventilation (r = 0.10, P = 0.103). Admission albumin correlated negatively with length of stay (r = -0.15, P = 0.01), duration of mechanical ventilation (r = -0.15, P = 0.014) and APACHE II score (r = -0.17, P = 0.004). The review of 1,009 nursing days in 79 patients revealed 230 ADEs, which occurred in a total of 175 nursing days. The most commonly identifi ed ADE was a hypoglycemia of <50 mg/dl (n = 75), followed by hypokalemia (n = 67). Ninety-six percent of the ADEs were classifi ed as category E, whereas only 4% of ADEs were classifi ed as category F. The mean severity of illness and nursing workload scores were signifi cant higher on nursing days when an ADE occurred (P <0.001 and P = 0.002, respectively). Conclusions ADEs are common in the ICU. The lack of a golden standard for reporting and collecting ADEs makes it diffi cult to compare with other studies and to assess the real value of this study. However, these date strongly and clearly indicate the infl uence of severity of illness and nursing workload on the prevalence of ADEs. Introduction Appropriate use of ICU resources is mandatory. When a patient is admitted to a unit able to provide a higher (lower) level of care than required, a waste (overuse) of resources can be advocated. StART is an approach to identify possible mismatches between the level of care actually delivered, assumed to correspond to what is clinically required, and the level of care deliverable by the unit. Methods ICU beds are classifi ed by levels of care deliverable as High (ventilator, monitor, and 720 minutes nurse time) and Low (monitor and 360 minutes nurse time) [1] . The level of care actually delivered is classifi ed as High (invasive or non-invasive ventilation, or two vasoactive drugs, or at least two of the following: one vasoactive drug, dialysis, respiratory support), Low (single vasoactive drug, or dialysis, or respiratory support) and Ordinary (none of the above) [2] . Mismatches between the level of beds available and the level of care delivered were evaluated both on admission and for each ICU-day of 4,237 patients in 28 ICUs. An ICU-day was judged as inappropriate, even without mismatch, if an Ordinary patient was present. [1] . Patients in critical care typically receive highrisk medications. Error reporting is integral to identifying common errors and medication risk reduction. Methods A medication reporting form was developed to run alongside the offi cial hospital incident reporting system for 2 weeks. Forms were distributed throughout the critical care facility. All members of the multidisciplinary team were asked to anonymously complete a form every time a medication error, or near miss, occurred. After 2 weeks, the submitted forms were analysed by the project team. In total 112 reports were submitted. The largest numbers of incidents reported were due to prescribing (67%) errors followed by administration (15%), documentation (7%), electronic prescribing problems (6%), storage (3%) and monitoring (2% Introduction Communication between healthcare professionals is a key step for patient safety, its failure accounting for over 60% of root causes in sentinel events [1] . Bedside rounds are important for teamwork communication and can be improved by an explicit approach [2] and by process-oriented information tools to organize and direct interprofessional rounds [3] . Methods As part of a quality improvement project, we conducted an observation of the documentation of daily goals (DG) and best practices (BP) in a step-down unit (both tools have been previously added to patient fl owsheets), before and after the introduction of a structured rounds process and team education. Our hypothesis was that these important tools were used before rounds, without input from all team members. Rounds were observed on two separate periods and the observer would take notes of whether DG and BP were documented or not and whether discussion took place before documentation. Diff erences in proportions between the two periods were analyzed with Fisher's exact test. P <0.05 was considered signifi cant. We observed 100 bedside interactions on each period. Documentation remained unchanged for DG (pre 55% vs post 53%, P >0.05) and BP (pre 57% vs post 48%); however, the second period had an improved documentation after team discussion (DG: pre 2% vs post 31%, P <0.001; BP pre 0% vs post 33%, P <0.001). Conclusions The intervention aided in increasing documentation after discussion, implying an increased communication among the interprofessional team. About 50% of patients still will not have documentation after bedside rounds. Patient information was not collected, therefore our study is limited in providing information on clinical outcomes. Further research should focus on how to best implement these tools, how to qualitatively assess the content of daily goals and to demonstrate eff ects on patient-centered outcomes. Introduction The use of a high-fi delity simulator can lead to very realistic clinical situations sometimes diffi cult to manage psychologically. The aim of this study was to evaluate the psychological stress induced by simulationbased training and the associated skills in anesthesiologist residents. Methods A cohort of 27 residents was studied. The psychological stress just before and after the simulation session was quantifi ed by autoevaluation scale (numeric scale 0 to 10) and by salivary amylase sampling [1] . Nontechnical skills were quantifi ed by analysing videotapes and scoring the Anaesthetist NonTechnical Skills [2] . The median stress numeric scale before the simulation session was 5 (ranging 2 to 8), and after was 7 (2 to 10) (P = 0.0004) (Figure 1 ). The stress scale before the session was signifi cantly lower in residents who already underwent simulation-based training (P = 0.04). In 48% of residents, stress scales after the simulation session were above 8/10. Salivary amylase after the session was signifi cantly higher than before (P = 0.008), corresponding to a 2.2-fold increase. They were no signifi cant relationships between psychological stress parameters and nontechnical skills. Conclusions Psychological stress before the simulation session, but especially after simulation, appears to be high in anesthesiologist residents, and particularly in those who performed a simulation session for the fi rst time. This fact should be considered when organising such simulation-based teaching. References In the 6-month analysis period, there were 15,320 hours of delayed discharge. This resulted in the postponement/delay of 45 operations for patients requiring level 3 intensive care and 177 operations for patients requiring level 2 intensive care. One hundred and fourteen tertiary referrals were refused because of bed blockage. Seventy-two patients were discharged between the hours of 22:00 and 06:00. There was a mean delay of 5.3 hours between identifying a patient requiring unplanned admission to critical care and their actual admission due to delayed discharge. Only 21 patients acquired a hospital-acquired organism while awaiting discharge (seven infections). Family/patient interviews suggested that the negative eff ects of noise and exposure to the resuscitation and/or death of ICU patients were off set by a perceived benefi t of receiving critical care nursing for a prolonged period. This potential benefi t was refl ected by a 72% reduction in the readmission of critically ill patients to critical care. The estimated cost of delayed discharge was £342,000. Conclusions Delayed discharge of patients to the ward following an episode of critical illness is a common and increasing problem. This delay is not benign and should be considered when prioritising bed utilisation in acute hospitals. Introduction Pain is the most relevant factor for prolonged hospital stay after thoracic surgery and is associated with stress known to alter the Th1/Th2 ratio (Th = T helper cells) in the immediate postoperative period. Thoracic epidural block (TEB), central α 2 -receptor stimulation via intravenous clonidine application and stimulation of opioid receptors can decrease either pain and/or stress and might therefore infl uence this immune imbalance. The primary endpoint of the current study was the perioperative Th1/Th2 balance in lung surgery. The secondary endpoints aimed to the incidence of pain and pneumonia. Methods After approval by the ethics committee and informed consent a total of 60 patients was randomized to receive double-blinded either remifentanil intravenously, or remifentanil + clonidine intravenously, or ropivacaine epidurally. Pain intensity was assessed by the numeric rating scale (NRS). The Th1/Th2 ratio was measured using a cytometric bead array. Pneumonia was diagnosed according to the hospital-acquired pneumonia criteria of the American Thoracic Society. The Th1/Th2 ratio adjusted for baseline diff ered between groups over time (P = 0.012). At the end of surgery there was no signifi cant diff erence between the remifentanil and the remifentanil + clonidine groups (P = 0.679) but a signifi cantly lower ratio in the ropivacaine group compared with the remifentanil (P = 0.004) and the remifentanil + clonidine groups (P = 0.019). NRS scores immediately after surgery were lower in the ropivacaine group compared with the remifentanil group and the remifentanil + clonidine group but achieved only borderline statistical signifi cance. None of the patients developed pneumonia. Conclusions Intraoperative TEB decreases the Th1/Th2 ratio and provides better pain therapy immediately after surgery. Introduction Protocol for perioperative usage of beta blockers was used in only 9% of major clinics in the USA. It is a very common practice to discontinue beta blockers during the perioperative period or to maximally reduce their doses. The main purpose of this paper is to investigate the incidence of adverse eff ects of beta blockers during large-scale trauma and orthopedic surgeries in spinal anesthesia. Methods In this open prospective study we evaluated the adverse eff ects of beta blockers in ASA II and III research subjects who received beta blockers 1 to 12 hours preoperatively, n = 37, mean age 67 ± 12, with adjacent metabolic syndrome. Results were compared with the same number of patients and ASA group, mean age 57 ± 18, who have not received beta blockers. Patients from both groups received midazolam premedication 30 minutes before arriving in the operating room and spinal anesthesia with 0.5% bupivacaine. Patient surveillance was conducted by continued EKG, blood pressure, pulse (all non-invasive) and SpO 2 monitoring. Systolic and diastolic pressure and pulse were recorded every 10 minutes. Results were statistically tested (M ± SD, t test). The results are shown in Figure 1 . The group receiving beta blockers shows a higher percentage of patients with hypertension on arrival in the operating room for 11% and hypotension after the start of spinal anesthesia. Occurrence of bradycardia (HR <60/minute) was increased with statistical signifi cance for 24% (P <0.05), with use of atropine for 27% (P <0.05). The occurrence of arrhythmias shows a statistical increase in the same group, for 27% (P <0.05), as well as nausea for 29% (P <0.05). Conclusions Due to their adverse eff ects, beta blockers should be discontinued before spinal anesthesia and surgical procedures with signifi cant circulating volume loss. Introduction Surgical-related neuroendocrine stress response extends over the postoperative period, inducing insulin resistance and hyperglycaemia [1] . Remifentanil continuous infusion, reducing pain and anxiety, may reduce postoperative insulin resistance and result in better glycaemic control. The aim of the study was to assess trends in glycaemia and HOMA scores in postoperative patients during remifentanil-driven, postoperative analgo-sedation. Methods Enrolled patients were those consecutively admitted to a surgical ICU after major abdominal interventions during a 1-month period (July 2009). Group R patients underwent analgo-sedation with continuous remifentanil infusion at the recommended doses [2] . Glycaemia and HOMA scores were evaluated at admission time (T0) and after 12 and 24 hours. Control patients were those who underwent morphine-based continuous analgo-sedation (M Group Conclusions Remifentanil-based analgo-sedation was associated with lower glycaemia and HOMA score in patients following major abdominal surgery, particularly at 24 hours from ICU admission. HOMA scores showed that morphine-treated patients were insulin resistant, although normoglycaemic. A better control of the neuroendocrine stress response in patients analgo-sedated with remifentanil might explain these results. Introduction Ultralow-dose opioid antagonists can enhance opioid analgesia and prevent tolerance in rodent nociceptive pain assays. Methods A randomized, double-blind controlled trial is designed to investigate whether the addition of 5 ml ultralow-dose naltrexone (1 μg in a liter of sterile water) to morphine (0.05 mg/kg) changes the total opioid requirement and side eff ects. Results Two hundred and sixty-seven patients (18 to 45 years old) with moderate extremities trauma entered the study, Pain control measurements were evaluated every 15 minutes for the fi rst hour, then every 30 minutes for the second and third hours and fi nally at the fourth hour. Effi cacy was measured by the 11-point numerical pain rating scale. The following side eff ects were evaluated: sedation, nausea, vomiting and pruritus. We found that opioid requirements did not diff er signifi cantly between groups. The morphine + naltrexone group on average required 0.04 mg more morphine during the 4 hours than the morphine group. Conclusions The morphine + naltrexone group had a lower incidence of nausea than the morphine + placebo group. However, the incidence of vomiting, pruritus and sedation between the two groups were similar. The combination of ultralow-dose naltrexone and morphine in moderate Introduction Pulmonary hypertension (PH) is a life-threatening disease commonly seen in ICU septic patients and associated with poor outcome. New and better therapies are required, since the response to various agents such as NO, prostaglandins and phosphodiesterase inhibitors is usually partial and the mortality rate remains high. Inhaled drugs seem to be an attractive treatment option, since they are delivered directly to pulmonary resistance vessels. A new device (anesthetic conserving device -AnaConDa (ACD)) permitting direct administration of volatile anaesthetics -such as sevofl urane -to the breathing circuit of a conventional ICU ventilator in a safe and eff ective way has recently been introduced. The aim of the present study was to evaluate the effi cacy and safety of sevofl urane administration via the ACD on a porcine model of acute PH during sepsis. Methods PH was induced in 16 anaesthetized, mechanically ventilated swine (25 kg) by intravenous infusion of 0.5 mg/kg LPS (Escherichia coli, 111:B4) in a period of 30 minutes. After LPS, animals were randomized into two groups. In group A sevofl urane was infused via the ACD, according to the manufacturer's recommendations in order to obtain an end-tidal concentration of 0.5%, whereas group B did not receive sevofl urane and served as control. Haemodynamic parameters (systemic and pulmonary) were recorded before (phase 0) and after the LPS (phase 1) and every 20 minutes for the next 2 hours (phases 2 to 7). Results LPS was found to produce PH (phase 1) and to reduce arterial blood pressure in both groups. After sevofl urane infusion, both systolic (PAPs) and diastolic (PAPd) pulmonary pressures exhibited step-by-step reduction, which became statistically signifi cant in phase 3 and thereafter, in relation to phase 1 values. On the contrary, in group B animals, not sevofl urane-treated, pulmonary pressures remained at high levels throughout the study period. Systemic arterial pressure exhibited an endotoxin-related reduction in both groups, which was not found to be aff ected by sevofl urane treatment. Conclusions Sevofl urane administration via the ACD was found to reduce the pulmonary pressure in a porcine model of endotoxin-induced acute PH without any detrimental eff ects on the systemic circulation. This may represent a signifi cant advance in the treatment of acute PH. However, the potential clinical implications of the method merit further study. Introduction Methyl-naltrexone (Relistor®), a peripheral μ-opioid receptor antagonist shown to induce laxation in patients receiving opioids, is licenced for use in palliative care; a similar benefi t is proposed in critical care patients. Impaired gut motility and constipation are common issues in the critical care setting with contributing factors including trauma, surgery and use of opiate analgesics. The eff ects of methyl-naltrexone were studied on seven patients in whom conventional treatment for constipation with stimulant and osmotic laxatives, faecal softeners and suppositories failed to produce eff ects. Methods Over a 6-week period, 15 critically ill patients with opiateinduced constipation (OIC) of more than 4 days duration were pros pectively studied. All patients were treated with conventional methods from admission to critical care, which included regular senna and sodium docusate, supplemented with glycerin suppositories and picolax for resistant constipation. Methyl-naltrexone was administered to seven patients at 0.15 μg/kg subcutaneously on alternate days until laxation occurred. The remainder of patients in the study continued with conventional therapy. Results Six out of seven patients responded to methyl-naltrexone with laxation occurring between 12 and 24 hours. One patient did not respond due to faecal impaction but subsequently experienced laxation after manual disimpaction. Side eff ects were few (nausea 14%, vomiting 14%) and there was no increase in opiate requirement. Those patients treated by conventional means eventually produced laxation after a further 3 to 5 days. Conclusions Even though our patients with DT are frequently admitted to the ICU, this is a pathology that most of the time has a benign course with low hospital mortality and high rate of survival at 2 years. These patients are regular users of the health system. The occurrence of complications and the need for MV in our patients were low, and were present in the group of DT with other diagnoses. Introduction Delirium in ICU patients increases time on mechanical venti lation, is an independent risk factor for death and can cause cognitive impairment in survivors [1, 2] . The two most commonly used validated assessment methods for diagnosing delirium in intensive care are the Confusion Assessment Method for ICU (CAM) and the Intensive Care Delirium Checklist Score (ISDCS) [3] . We wished to determine the prevalence of delirium in our unit, whether there was a diff erence in results between these methods of assessment and whether good agreement could be achieved between two trained assessors. Methods We performed a prospective prevalence study in a single tertiary ICU. Patients were assessed between days 3 and 10 of their stay and tested provided their Richmond Agitation Sedation Score (RASS) was ≥-3 [1] . Each patient was independently assessed by the two trained assessors with the CAM and ISDCS, with randomisation of both the order of interview and the score used. Both sets of assessments were carried out within an hour of each other. Exclusions included readmissions, those who did not speak English, the deaf and registered psychiatric patients. The CAM was completed by the assessors and the ISDCS required observational answers from the bedside nurse. We performed 104 assessments (208 tests) on 52 patients. Mean (SD) age, APACHE II and total SOFA on the day of admission were 67.4 (14.0), 14.0 (5.3) and 5.7 (2.7), respectively. Delirium was found in 37.5% of patients using the CAM, but only 14.4% using ISDCS. Inter-rater agreement for CAM and ISDCS was 86.5% (κ 71.4%, SE 13.8%, P <0.001) and 94.2% (κ 76.7%, SE 13.8%, P <0.0001), respectively. There was signifi cant underscoring of the RASS by the bedside nurses. When the trained assessors found a diff erence, it was always lower than the score documented by the bedside nurse. Conclusions There was good inter-rater agreement in the diagnosis of delirium between the two trained assessors, but the prevalence found was lower than previously reported [1] and varied considerably with the method used. The diff erence in results between the two scores may be due to a lack of discernment on the part of the bedside nurse of the ISDCS assessment process. Introduction Delirium aff ects up to 33% of acutely hospitalized patients. In ICU patients it can prolong the length of stay and increase mortality [1] . The optimal management of delirium requires a calm environment, sleep hygiene and correction of underlying factors (for example, infection). This can be a challenge in the ICU, and drug therapy, commonly haloperidol, clonidine, benzodiazepines or propofol, is often used [1] . Quetiapine, an atypical antipsychotic, has been used in acute delirium outside the ICU [2] . It has few extrapyramidal side eff ects, a short half-life and is mildly sedating. Our impression was that it had potential in the treatment of delirium. Methods In a 30-bed tertiary ICU with more than 1,200 admissions annually we reviewed the notes of patients admitted from February 2008 to November 2009 who had a delirium treated with quetiapine. Patients were excluded if they were taking quetiapine prior to admission. The following data were recorded: Richmond Agitation and Sedation Score Critical Care 2010, Volume 14 Suppl 1 http://ccforum.com/supplements/14/S1 S166 (RASS), duration of delirium, agents used, patient demographics and adverse events. Results Five patients met the inclusion criteria (Table 1) . All were males, aged 37 to 85 years. All had a prolonged delirium prior to the commencement of quetiapine and all were on a combination of four drugs (clonidine, haloperidol, lorazepam and propofol) that were not controlling their delirium. At 3 to 7 days following commencement of quetiapine the RASS scores were 0 and other drugs were ceased. No adverse eff ects were noted. Conclusions Quetiapine was successful in controlling prolonged ICU delirium and allowed weaning of other medications in these patients. It may be a useful delirium therapy. Further studies are required to demonstrate effi cacy and safety. References vs 36.9%, respectively). The ICU length of stay tended to be longer in the SynColl-Group (14 versus 10 days, P = 0.055). Conclusions In patients with severe sepsis, fl uid therapy with synthetic colloids -gelatin or the third-generation 6% HES 130/0.4 -considerably increases the incidence of ARF and need for RRT compared with crystalloids. These results confi rm recent meta-analysis and RCTs, which demonstrated an increased incidence of ARF by synthetic colloids [2, 3] . Introduction Dehydration is an important problem among patients admitted to the emergency department (ED). However, there has not yet been a consensus on the ideal fl uid for these patients. This study was planned to evaluate the ideal crystalloid for the patients admitted to the ED who have symptoms of dehydration. Methods We conducted a randomized controlled trial that included 90 dehydrated patients. Three groups of patients each included 30 randomized for one of the solutions Lactated Ringer's, 0.9% NaCl or Isolyte® (Eczacıbaşı-Baxter, Turkey). Solutions were infused at a rate of 20 ml/kg/ hour for 2 hours. Venous blood sample pH, Na + , K + , Cl -, and HCO 3 levels were evaluated at 0, 1, and 2 hours. Results We detected a decrease in serum pH (7.406 to 7.365) and HCO 3 -(23.1 to 21.5) levels at the second hour in the 0.9% NaCl group. However, in the Isolyte® group an increase in both serum pH (7.410 to 7.434) and HCO 3 -(23.4 to 24.4) levels were observed. In the Lactated Ringer's group, Introduction To evaluate early continuous venovenous hemodiafi ltration (CVVHDF) in patients with refractory septic shock and multiorgan failure upon mortality and morbidity in the ICU. Methods Forty patients were prospectively studied and randomly treated with either conventional treatment (20 patients; group II) or with early CVVHDF (less than 6 hours of maximal hemodynamic support) in addition to the conventional treatment (20 patients; group I). Metabolic acidosis, serum lactate and serum procalcitonin level (PCT) before and 5 days after CVVHDF were monitored to evaluate the outcome. APACHE II and ΔSOFA scoring systems were used before and 5 days after CVVHDF. Results Compared with group II, patients of group I had lower mortality (55% vs 70%) with an insignifi cant P value (P = 0.54). Group I patients showed a nonsignifi cant ΔSOFA (5.95 ± 4.39 vs 6.2 ± 3.3 in groups I and II, respectively, P = 0.66); regarding APACHE II scores, group I also showed statistically nonsignifi cant lower fi gures than group II (on admission APACHE II scores were 39.35 ± 10.65 vs 41.85 ± 10 in groups I and II, respectively, P = 0.45, while on day 5 APACHE II scores were 34.8 ± 10.6 vs 36.1 ± 10.9 in groups I and II, respectively, P = 0.41). Group I patients showed lower PCT on admission and day 5 than group II patients (on admission PCT level was 0.64 ± 0.18 vs 0.68 ± 0.17 in groups I and II, respectively, P = 0.5) while the day 5 PCT level was (0.51 ± 0.15 vs 0.52 ± 0.17 in groups I and II, respectively, P = 0.83). Indicators of improvement showed a statistically signifi cant diff erence between survivors and nonsurvivors in group I regarding serum lactate level at day 5 (P <0.001), while other indicators as fever, renal profi le, WBC count, metabolic acidosis, serum lactate level on admission and platelet count were statistically insignifi cant (on admission P = 0.2, 0.55, 0.45, 0.41, 0.65, 0.55, respectively, and on day 5 P = 0.37, 0.94, 0.71, 0.5, <0.001, 0.88, respectively). There was a signifi cant statistical diff erence between survivors and nonsurvivors in group I considering the number of organ failures as less than or equal to three organs involved in comparison with more than three organs involved (P = 0.008). Introduction Both inhaled hydrogen sulfi de (H 2 S) [1] and intravenous H 2 S donors protected against kidney ischemia/reperfusion (I/R) injury [2] [3] [4] , but all these data originate from unresuscitated rodent models. Therefore, we investigated the eff ect of the H 2 S donor Na 2 S in a clinically relevant porcine model of aortic occlusion-induced renal I/R injury. Methods Anesthetised and ventilated pigs received Na 2 S (n = 9) or vehicle (n = 10) for 2 hours before and 8 hours after 90 minutes of intra-aortic balloon occlusion-induced kidney ischemia. During reperfusion noradrenaline was titrated to keep blood pressure at baseline levels. Before Na 2 S, prior to aortic occlusion and at 1, 2, 4 and 8 hours of reperfusion, we measured renal blood fl ow and function (creatinine clearance and blood levels, fractional Na + excretion), blood cytokines (TNFα, IL-6, IL-1β) and nitrates, renal tissue DNA damage (comet assay), HO-1 and caspase-3 expression (western blotting), and NF-κB activation (EMSA). Histological damage (glomerular tubularisation [5] ) was assessed immediately post mortem. Results Na 2 S pretreatment was associated with a progressive fall in core temperature and signifi cantly lower noradrenaline infusion rates needed to achieve the hemodynamic targets. While renal blood fl ow and fractional Na + excretion were comparable, Na 2 S attenuated the fall in creatinine clearance and the rise in creatinine blood levels, respectively, which coincided with signifi cantly lower IL-6, IL-1β, and nitrate blood levels. Kidney glomerular and tissue DNA damage were markedly attenuated, whereas NF-κB activation was signifi cantly higher in the Na 2 S-treated animals. Conclusions In a clinically relevant porcine model mimicking aortic cross-clamping-induced kidney I/R injury, Na 2 S attenuated tissue injury and organ dysfunction as a result of reduced systemic infl ammation and oxidative stress. The higher NF-κB activation and the unchanged fractional Na + excretion were most probably due to the drop in temperature [6] and the direct eff ect of H 2 S on tubular Na + absorption [7] , respectively. Introduction Sepsis has been identifi ed as the most common cause of renal injury in ICUs although the pathophysiology is not well understood. No large clinical studies are available that show an improvement of renal function in patients with sepsis and this may be related to the lack of early diagnostic tests that indicate the onset of renal injury. The aim of the current study was to search for potential new early markers of renal injury during acute endotoxemia and to investigate whether renal injury can be ameliorated by the induction of lipopolysaccharide (LPS) tolerance. Methods Five healthy males received intravenous bolus injections of 2 ng/kg/day Escherichia coli LPS for 5 consecutive days. We used surfaceenhanced laser desorption/ionization time-of-fl ight mass spectrometry (Seldi-TOF MS). This approach allows for rapid high-throughput profi ling of multiple urine samples and detects low molecular weight biomarkers. Results Repeated LPS administrations induced a diminished glomerular fi ltration rate of 33 ± 7% (P = 0.02) on day 2 and an increase in serum creatinine of 11 ± 3% (P = 0.002) on day 3, which was associated with the appearance of 15 peak intensities in the urinary protein profi le including an increase in β-microglobulin levels (P = 0.04) 6 hours after the fi rst LPS administration. Four of the 15 peak intensities on day 1 correlated with serum creatinine levels on day 3; 3,950, 4,445, 6,723 and 7,735 m/z (P = 0.03; 0.01; 0.02 and 0.05, respectively). With the development of LPS tolerance, renal function was restored, refl ected by a decrease in serum creatinine and β-microglobulin levels to baseline (P = 0.2 and 0.4, respectively, between days 1 and 5), and by attenuated peak intensities in the urinary protein profi le (P <0.0001 for all 15 peak intensities). The introduction of CVVH in all 15 patients dramatically helped to overcome the severe part of VOD until the defi brotide helped to restore Critical Care 2010, Volume 14 Suppl 1 http://ccforum.com/supplements/14/S1 the right function of vascular endothelium. None of the 15 patients died from VOD as a cause of death. Surprisingly, the median patency of the CVVH tubing set was 46 hours despite no anticoagulation in the tubing set. Defi brotide is effi cient in preventing coagulation in the tubing set. No serious hemorrhagic events were observed. The fl uent maintenance of fl uid balance, the removal of waste products of metabolism and the maintenance of acid-base balance were very important for the patient in the critical point of advanced VOD. Conclusions CVVH is an integral part of intensive care in hemato-oncology and the use of this therapeutic modality in patients with VOD with acute renal failure can signifi cantly improve the results of our therapeutic eff orts. The incidence of metabolic alkalosis in Group B from treatment day 2 onwards was signifi cantly lower than Group A. The Group A median pH was 7.42 (6.81 to 7.62) compared with Group B pH 7.35 (7.2 to 7.49), P = 0.001. The control of electrolytes and azotemia was not signifi cantly diff erent. The mean fi lter duration was 58.8 hours (95% CI 38.0 to 79.6) for Group A and 61.8 hours (95% CI 45.8 to 77.8) for Group B (P = 0.678). Longitudinal analysis revealed a statistically signifi cant result for reduced metabolic alkalosis for Group B. (standard bicarbonate P <0.001, base excess P <0.001). Repeat treatment sessions also showed a statistically signifi cant reduction of metabolic alkalosis using Prismocitrate 10/2 rather than ACD-A (P = 0.029). Conclusions RCA with Prismocitrate 10/2 reduces the incidence of metabolic alkalosis associated with ACD-A. This regime is safe, feasible and improved patient safety, with no increase in complication rates. Our unit has now converted to Prismocitrate 10/2 for RCA. injury (TBI) is common. We hypothesised that the use of such therapies would signifi cantly augment creatinine clearances (CrCl) in this population. Methods Head-injured patients requiring hyperosmolar therapy using 3% or 20% saline solutions and/or norepinephrine infusion for the maintenance of a CPP >60 mmHg were recruited into the study. Additional management was consistent with local practice and in line with the Brain Trauma Foundation guidelines [1] . An 8-hour CrCl, physiological variables, fl uid balance, and medications were recorded daily during active management of CPP. A further CrCl was collected just prior to discharge (off CPP therapy), and if this was elevated, was repeated on the ward. Augmented renal clearance (ARC) was defi ned as a CrCl >160 ml/minute/1.73m 2 for males and >150 ml/minute/1.73m 2 for females [2] . Results Twenty consecutive patients were enrolled. The average ICU length of stay was 15 days (CI 95% 11 to 18), and time to study entry averaged 2.3 days (CI 95% 1.7 to 2.8). All patients received norepinephrine (n = 20), 85% (n = 17) received hypertonic saline, and therapy lasted on average 7.6 days (CI 95% 5.6 to 9.5). ARC was demonstrated in 17 (85%) patients at any point during active management of CPP. The mean maximum CrCl was 179 ml/ minute/1.73 m 2 while on CPP therapy (CI 95% 159 to 198) returning to a mean CrCl of 111 ml/minute/1.73 m 2 (CI 95% 91 to 131, P <0.001) when measured in the ward. The mean CrCl in the ICU while not receiving CPP therapy was 150 ml/minute/1.73 m 2 (CI 95% 134 to 167, P = 0.03). The mean time to reach peak CrCl while on active treatment was 4.7 days (CI 95% 3.0 to 6.4). Norepinephrine use, saline loading, mean arterial pressure, and central venous pressure, predicted CrCl on the day of measurement. Conclusions ARC is common in head-injured patients receiving active management of CPP and persists even after ceasing such therapy. This has signifi cant implications for appropriate dosing of renally excreted drugs in this setting. Results One hundred and fi fty-six patients were analyzed. CT scan showed a NGT in 30 patients (one malposition in the medium esophagus). Gastric gas, liquid/solid and total volume were not diff erent in patients with and without NGT (Z test; gas: 54 ± 147 vs 95 ± 168 ml, P = 0.179; liquid/solid: 226 ± 282 vs 199 ± 237, P = 0.626; Total: 280 ± 311 vs 295 ± 320, P = 0.824). Twenty out of 153 patients developed a pneumonia in the fi rst 7 days, fi ve of which with NGT and 15 without (chi-squared, P = 0.459). There was no diff erence in gastric residuals between these patients and the 133 others (Mann-Whitney; Q1-median-Q3; Gas: 11-25-126 vs 14-43-90 ml, P = 1; liquid/solid: 16-63-432 vs 22-115-290 ml, P = 0.799; Total: 29-252-661 vs 68-186-371 ml, P = 0.873). Seventy-three patients were admitted to the ICU, of which 23 had a NGT and 20 developed pneumonia. There was no diff erence between gastric residuals of the 23 who had a NGT and the 50 others. A pneumonia developed in the fi rst 3 days in 1/23 patients with NGT vs 9/50 without NGT (chi-squared-Yates, P = 0.226); in the fi rst 7 days in 5/23 patients with NGT vs 15/50 without NGT (chi-squared, P = 0.462). There was no diff erence in gastric residuals between patients who developed early pneumonia (at 3 and 7 fi rst days) and those who did not. There was no diff erence in ICU length of stay (10.8 vs 12.7 days, P = 0.825) and onset time of pneumonia (4.2 vs 3.3 days, P = 0.234). Conclusions Our results suggest that gastric volume is high at admission of trauma patients, irrespective of NGT presence. In this study, pneumonia incidence was related neither with high gastric volume, nor with NGT usage.  The probability of survival on day 28 was 73% in the standard therapy group, in the group with glutamine intravenous -78%, in the group with glutamine intravenous and enteral -84%. We did not fi x the decrease of the duration of respiratory support in all of the groups. The duration of acute intestinal injury was signifi cantly diff erent (standard group 49 hours vs 38 hours in group with glutamine intravenous supplementation -35 hours in group with intravenous and enteral glutamine supplementation). We investigated the prevalence of the concentration of proinfl ammatory cytokines in the peritoneal cavity and in blood serum according to the molar coeffi cient in the control group. The molar coeffi cient had a positive correlation with the SOFA scale. In group 2 (glutamine intravenous) the molar coeffi cients were decreased to the prevalence of anti-infl ammatory cytokines (in serum on day 3, in peritoneal on day 2). In group 3 (glutamine intravenous and enteral) we investigated the signifi cance diff erence and decrease of all cytokine levels in blood serum and in the peritoneal cavity. Conclusions Intravenous and enteral supplementation of glutamine improved the cytokine balance in blood and the peritoneal compartment. Introduction Glycemic control is mandatory in the critically ill, because hypoglycemia and hyperglycemia are associated with increased mortality [1] . We compared in a prospective observational study three new pointof-care devices with the hexokinase reference method and we evaluated whether their results would modify insulin titration. Methods Arterial blood glucose was simultaneously measured by the blood gas analyser RapidLab 1265, by three glucose meters (Accu-Chek performa, Precision XceedPro, Nova StatStrip), and with the hexokinase reference method. All values were duplicated and the average value of each was computed. Bland-Altman, Passing Bablok, Kanji [2] and modifi ed Kanji approaches were performed. Biases were expressed as the glucose result of the point-of-care method minus the reference method. We evaluated the theoretical impact on insulin titration by comparing glucose meter results with the hexokinase reference method on a dynamic sliding scale targeting a glycemia of 80 to 130 mg/dl. Results A total of 156 matched analyses were done in 80 patients. The mean fl ash SOFA score was 4.5. The range of the reference glucose was 25 to 327 mg/dl. The numbers of discrepancies of dosing insulin were respectively 8-5-6-14 at 0.1 U/hour, 0-6-3-6 at 0.2 U/hour and 2-3-0-0 at 0.3 U/hour. None was greater than 0.3 U/hour. Regarding the point-of-care results, total theoretical insulin dose changes were respectively: -11.7 U, -19.2 U, -22.2 U, +4.9 U for all these measurements (devices order as in Methods). Table 1 presents the standard comparisons. Introduction Chronic critical illness is characterized by a severe metabolic disorder, caused by the loss of the physiologic hypothalamic and pituitary functions and the onset of the so-called wasting syndrome, as many studies have just confi rmed [1] . In this study we assessed the correlation between the neuroendocrine pattern in chronic ICU patients and mortality. Methods The patients enrolled were 25 (18 male and fi ve female) with a mean age of 67 years and APACHE II score of 12 ± 5. We excluded females in a premenopausal state, patients with previous endocrine problems or in therapy with dopamine, high dose of cortisone or amiodarone. In these patients, we evaluate, on the seventh day of stay in the ICU (considered the chronic phase of critical illness), the mean value of four nocturnal hormonal measurements (LH, FSH, estrogen, testosterone, DHEAS, androstenedione, androstenediol, progesterone, TSH, FT3, FT4, RT3, GH, IGF1, prolactin, cortisol). Furthermore, we observed hormonal patterns in people who died in the ICU. We found statistical evidence in the correlation of high levels of estrogens, related to aromatase increased function [2] , and the percentage of death. In the group of patients with estrogens more than 50 pg/ml, six of them died, while if estrogens were normal or low, none died ( Figure 1 ). Survivors and nonsurvivors did not diff er by mechanism of injury or APACHE score (11.6 vs 11.8). Medium serum estradiol levels were 44.96 pg/ ml in survivors and 115.08 pg/ml in nonsurvivors. Conclusions To date, the role of these hormones in critical illness pathophysiology and the increase of estrogen levels are still uncertain. Further studies are required. Introduction Patients with severe burn injury experience a rapid elevation in multiple circulating proinfl ammatory cytokines, with the levels correlating with both injury severity and outcome. Accumulations of these cytokines in animal models have been observed in remote organs, however data are lacking regarding early serial heart cytokine levels following burn injury, and the therapeutic eff ects of estrogen on these levels. Using an animal model, we studied the acute eff ects of a fullthickness third-degree burn on cardiac levels of IL-1β, IL-6, IL-10, and TNFα. In addition, we analyzed the eff ect of estrogen on levels of cytochrome C, signifying apoptosis, and levels of NF-κB, signifying infl ammation. Here, we hypothesized that acute estrogen treatment decreases infl ammation in the heart and blocks the induction of apoptosis. Methods In this study, 144 male rats received third-degree 40% total body surface area burns. Fifteen minutes following burn injury, the animals received a subcutaneous injection of either placebo or 17β-estradiol (0.5 mg/kg). The hearts were harvested at 0.5, 1, 2, 4, 6, 8, 12, 18, and 24 hours after injury, and the heart cytokine (IL-1β, IL-6, IL-10, TNFα) levels were measured using the ELISA method. In addition, we assessed the cytosolic levels of cytochrome C and NF-κB at the 24-hour time point using western blot analysis. In the burned rats, 17β-estradiol signifi cantly decreased the cardiac levels of TNFα (~95%), IL-6 (~50%), IL-1β (~25%), and IL-10 (~20%), when compared with the placebo group. In addition, we determined that estradiol treatment restored cytosolic levels of NF-κB (65% increase) at the 24-hour time point. Also, estrogen decreased cytosolic accumulation of cytochrome C (50% reduction) at the 24-hour time point. Conclusions Following severe burn injury, estrogens decrease cardiac infl ammation and levels of the pro-apoptotic cytochrome C. In addition, estrogen signaling promotes cell survival, as indicated by an increase in NF-κB levels. Importantly, estrogen treatment following burn injury nearly ablated the deleterious burn-induced increase of TNFα in the heart to levels similar to unburned control animals. Elevated cardiac levels of TNFα have previously been linked to a poor outcome. Introduction Glucocorticoids are known to have an anti-insulin action on glucose metabolism, leading to increased lactate production [1] . Alternatively, glucocorticoid-induced apoptosis is a well-recognized phenomenon initiated by mitochondrial dysfunction [2] . Increased lactate production follows loss of mitochondrial membrane potential during apoptosis. Both mechanisms may lead to clinically relevant hyperlactatemia following glucocorticoid administration during cardiac surgery requiring cardiopulmonary bypass. Methods All adult patients undergoing cardiac surgery and cardiopulmonary bypass from 5 October through 21 November 2005 in a large academic teaching hospital in Rotterdam, the Netherlands were included in this study. Dexamethasone (60 to 80 mg) was given perioperatively at the discretion of the anesthetist. Lactate levels were measured within 1 hour postoperatively. The association between dexamethasone treatment, serum lactate levels and possible confounders was evaluated using ANOVA and linear regression. Results A total of 82 patients 18 years and older underwent cardiopulmonary bypass for cardiac surgery in the above-mentioned period. Three patients undergoing heart or lung transplantation, who thus received methylprednisolone, were excluded; a further two patients who received hydrocortisone for allergic reactions were also excluded. Data were incomplete for one patient, leaving a total of 76 patients for analysis. The mean lactate level was 1.2 mmol/l in the 47 patients who did not receive steroids and 2.6 mmol/l in the 29 patients who received dexamethasone (P <0.0001, Figure 1 ). When adjusting for potential confounders such as age, glucose level, duration of cardiopulmonary bypass and preoperative NYHA heart failure classifi cation, this diff erence remained signifi cant (P <0.0001). 