In TEM construction, there is a parameter α which is used to prune the ''weak'' event dependency relationships. First, we test different values of α to evaluate the effect of α on Precision, Recall and F-measure in order to come up with the best value of α for the following experiment. In our testing, we use two query events: one is ''SARS in 2003'', which contains the largest number of related news articles and the other is ''Christchurch Earthquake in 2010 in New Zealand'', which contains the fewest number of related news articles. Fig. 3 shows the effect of α on Precision, Recall and F-measure. According to Fig. 3 , we find that as α increases, the Precision and F-measure increase while Recall decreases. The reason for this is that when the value α is small, there are many event dependence relationships with a dependence degree higher than α (but the dependence relationship is still actually ''weak''), so the Recall is high and the Precision is low. We tune the parameter α based on two queries which are not in the test queries. For these two queries, we obtain the highest value of F-measure when α = 0.65. Therefore, we set α = 0.65 for all test queries. After setting the value of α, we conduct all test queries and average the results of them on different metrics. The best value of α for each method is also identified. Fig. 4 and Table 2 show the comparison of our method with one baseline method (i.e., EEG) and two variations (i.e., ERM and CDM) on Precision, Recall and F − measure. According to Fig. 4 , it is clear that our method outperforms other three methods on Precision, Recall and F −measure. The Precision and Recall values of our method are approximately 0.8, meaning that most event dependence relationships discovered by our method are correct, and also that our method can discover more event dependence relationships than other three methods. Our method's F − measure score is also approximately 0.8, since it is a combination of Precision and Recall. Note that CDM outperforms EEG slightly on all metrics, and the reason is that using mutual information to measure feature dependence is better than only matching keyword similarity (as was done in most previous research). ERM outperforms EEG and CDM on all three metrics. This indicates that using event reference analysis (i.e., ERM) to identify event dependence relationships is more effective than using content dependence relationship analysis (i.e., CDM) and content similarity analysis (i.e., EEG). Also, it is quite interesting to see that the Recall of CDM is greater than that of ERM, while both of them are smaller than those of our method. This means that the event dependence relationships identified by CDM and ERM are indeed different and complementary. Our method is a combination of CDM and ERM, and it has the strength of both methods. In other words, taking both content dependence and event reference analysis into consideration in identifying event dependence relationships produces better results than taking just one of these. For event ranking, Table 3 shows that our method obtains a higher value of NDCG than the ERM and CDM methods. In particular, the NDCG of our method is 0.9662 while that of ERM and CDM is 0.8923 and 0.8687, respectively. This means that combining both content dependence relationships and event reference relationships to rank events can also achieve a better result. SARS has a great impact on transportation, especially airlines 5 Experts find disease and SARS outbreaks 6 SARS has great impact on economy 7 Other countries donate and offer help to China for SARS 8 Scientists find coronavirus and conduct animal test for vaccine 9 China informs and cooperates with WHO on fighting SARS 10 China makes effort to prevent disease spread 11 Beijing has SARS under control 12 Probable cases are quarantined and schools are closed for disinfecting Table 5 Comparison of event relationships discovered by our method and EEG for the query Q SARS , Correct is the number of correctly identified relationships by comparing to human annotated results, Incorrect is the number of incorrectly identified relationships by comparing to human annotated results, New is the number of identified relationships which are not in human annotated results and confirmed as correct by human annotators, Missed is the number of missing relationships by comparing to human annotated results, and Total is the number of identified relationships by this method (i.e., Total = Correct + Incorrect + New) [1] .  


Section:experiment results