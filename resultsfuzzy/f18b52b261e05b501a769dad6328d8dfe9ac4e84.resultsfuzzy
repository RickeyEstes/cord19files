In this section, we are going to conduct two experiments in order to compare the proposed algorithm with the other famous algorithms. First, we scrutinize the TFBS problem. Then we examine the proposed algorithm's performance on standard [24] and present the obtained results in tables and diagrams. A web site is also available, "lbbwcc.ir". You can find some information about implemented algorithms and applied datasets on the web. The abovementioned algorithms run 100 times for each dataset with a different size of the motif length. The best, the worst, and the average score values, the elapsed time (based on second), the score values standard deviation, and the elapsed time standard deviation (based on second) are determined. The standard deviation (SD) that is commonly used for algorithms' confidence, is a criterion that shows how much data scattered around the mean. The fewer SD is, the more reliable algorithms are. The algorithms' parameters for the randomly created datasets are shown in Table 2 and the algorithms' parameters for the real datasets in Table 3 . Moreover, the obtained results from both the randomly created datasets and the real datasets are shown in Tables 4 and 5 , respectively. The figures are shown in Tables 4 and 5 indicate that dataset1 size is 10 Â 100, dataset2 size is 20 Â 200, dataset3 size is 30 Â 300, and dataset4 size is 40 Â 400. According to figures, all the real datasets are the same size (in a size 40 Â 60). As we mentioned earlier, the conditions for all algorithms are so similar that they are compared to the same terms and conditions. The end of the predefined numbers of iterations or seasons will bring an end to the algorithms. Figs. 9 and 8 depict Tables 4 and 5 as diagrams with over 100 times of independent execution. The horizontal pivot represents amounts of execution and the vertical pivot represents the fitness values. The best, the worst, the average and the standard deviation which is inserted in Tables 3 and 4, are obtained from over 100 times of independent execution for each algorithm with different motif sizes. Bold values for every group in Tables 3 and 4 indicate the best results compared with the other algorithms. Fig. 8 shows the average stability of algorithms for all randomly created and all real datasets by implementing over 100 times of individual execution. Algorithms' stability which indicates a fluctuation in the current outcome compared with the previous and subsequent outcomes, is one of the main criteria for a comparison between meta-heuristic algorithms. From this point of view, an algorithm whose results are in a straight forward line with maximum fitness values and a minimum undulation is considered as a proper algorithm [25] . The error bars which indicate the validity of the algorithms are represented in Fig. 8 . The above-mentioned criterion determines how similar or different measurements are. The error bars with a minimum height show the suitability of an algorithm as the benchmark that can be used in scientific papers with experimental results shown on graphs [26] . An algorithm's convergence is considered another criterion for a comparison between evolutionary algorithms. When the number of seasons, iterations or generations are augmented or when the allocated time to algorithms is augmented, the heuristic algorithms have to approach an optimal answer. An examination of the above-mentioned algorithms' convergence proved that these algorithms have a proper convergence. Since the number of generations or the number of iterations is not identical for the algorithms, we have to employ the allocated time (or the number of fitness functions callings) to compare different algorithms. The convergence of algorithms for all randomly created datasets as well as all real datasets is depicted in Fig. 9 . In Fig. 9 , the horizontal pivot represents the number of fitness functions callings and the vertical pivot represents the fitness values. Dash lines in Fig. 9 relate to the mean values of algorithms. The algorithms which are at the top of the dash lines are definitely better than the algorithms which are at the bottom of the dash lines. In conclusion, we'd like to refer to some practical results of the present paper: Table 6 which depict the mean values of all randomly created and all real datasets provide a solid base for the proposed algorithm's good performance. Two bold values in every column indicate the best state. 2. The GA algorithm is of great research value because it provides a solid base for the other evolutionary algorithms' good performance. However, its results are worse than the others'. 3. Although LA has good performance for small datasets and lies at the top of the dash lines, it is not suitable for big datasets because it lies at the bottom of the dash lines. 


Section:experimental results